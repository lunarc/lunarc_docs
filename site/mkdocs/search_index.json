{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to the Lunarc Documentation pages\n\n\nHere you will find all of the documentation for the Lunarc resources in a easy to find place.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-the-lunarc-documentation-pages", 
            "text": "Here you will find all of the documentation for the Lunarc resources in a easy to find place.", 
            "title": "Welcome to the Lunarc Documentation pages"
        }, 
        {
            "location": "/login_howto/", 
            "text": "Introduction\n\n\nThe main way of accessing the Lunarc systems are using a terminal and command line tools. To get access to a terminal the user has to login in to Lunarc using a Secure Shell (SSH) terminal client, for example:\n\n\nssh alarik.lunarc.lu.se -l username\n\n\n\nor\n\n\nssh username@alarik.lunarc.lu.se\n\n\n\nOn Linux this client is built-in to the system and no installation is neccesary. Windows does not have a standard SSH terminal so an external application is needed such as PuTTY (http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) is needed.\n\n\nTo authenticate to the Lunarc system a two-factor authentication solution is used. Two-factor authentication uses two factors for authentication instead of one which is commonly found in normal username and password systems. In the Lunarc case the two factors are:\n\n\nUsername and password.\nOne-time password sent by SMS to a mobile phone.\n\n\n\nWhen you apply for an account the mobile number is registered in our user database and will be used in the login process described in the following sections.\n\n\nLogging in using One Time Passwords (OTP)\n\n\nLogging in to the Lunarc system with OTP passwords is not very different from a normal SSH login, except for the additional extra password prompt. A typical session is shown in the following example:\n\n\nlogin as: joeuser \nPassword: \nPlease enter your onetime password: 123456\n\n\n\nIf the otp and password are correct you will be logged in to the system.\n\n\nLinux\n\n\nTo be prompted for the OTP password the ssh client must be configured for so called \"keyboard-interactive\" login. These settings can be modified in either /etc/ssh/ssh_config (Redhat systems) or in the home-directory ~/.ssh/config. An example configuration is shown below:\n\n\nHost alarik.lunarc.lu.se \nPreferredAuthentications keyboard-interactive\n\nHost * \nPreferredAuthentications hostbased,publickey,keyboard-interactive,password\n\n\n\nIn the above example Platon is configured for keyboard-interactive login, but all other hosts are configured with default login options.\n\n\nTo reduce the number of logins to the system the ServerAlive option can also be added:\n\n\nHost alarik.lunarc.lu.se \nPreferredAuthentications keyboard-interactive \nServerAliveInterval 10\n\nHost * \nPreferredAuthentications hostbased,publickey,keyboard-interactive,password\n\n\n\nMac OS X\n\n\nMac OS X is already configured to handle the login to Platon with one time passwords (keyboard-interactive). \n\n\nTo reduce the number of logins to the system the ServerAlive option can also be added:\n\n\nHost alarik.lunarc.lu.se \nPreferredAuthentications keyboard-interactive \nServerAliceInterval 10\n\nHost * \n    PreferredAuthentications hostbased,publickey,keyboard-interactive,password\n\n\n\nMac OS X 10.7 Lion and 10.8 Mountain Lion\n\n\nTo login to a Lunarc system from a Mac system running Mac OS X 10.7 and 10.8, you need to unset the box \"Set locale environment variables on startup\" in the settings window of the terminal application, press \"cmd ,\" to get there. \n\n\nEnglish example:\n\n\nSetting locale in lion (english)\n\n\nSwedish example:\n\n\nSetting locale in lion (english)\n\n\nNote: This works on a per-theme basis.  In the above examples you will need to choose the theme \"Homebrew\" to connect to the Lunarc servers.\n\n\nWindows\n\n\nTo be prompted for the OTP password the PuTTY client must be configured for so called \"keyboard-interactive\" login. Open PuTTY from the start-menu. Load the session options for Platon. Open the Connection/SSH/Auth item in the tree-view. Make sure the \"Attempt \"keyboard-interactive\" auth (SSH-2) is checked in the settings, see the following image:\n\n\nTo reduce the number of logins to the system the \"Seconds between keepalives\" can be changed to a value greater than 0. See the following figure:\n\n\nFile transfers\n\n\nTo reduce the number of otp passwords needed when transferring files the SFTP protocoll should be used instead of SCP, as each SCP connection will require a new OTP password.\n\n\nFile transfers with WinSCP\n\n\nThe default filetransfer method in WinSCP is SFTP, so no special settings is needed for this. Just make sure that the \"File protocol\" setting is set to SFTP as shown in the following figure:\n\n\nTo reduce the number of logins to the system the \"Keepalive\" options can be set as shown in the following figure:\n\n\nPlease note:\n\n\nthat the \"Advanced options\" checkbox must be checked to access these settings", 
            "title": "How to login"
        }, 
        {
            "location": "/login_howto/#introduction", 
            "text": "The main way of accessing the Lunarc systems are using a terminal and command line tools. To get access to a terminal the user has to login in to Lunarc using a Secure Shell (SSH) terminal client, for example:  ssh alarik.lunarc.lu.se -l username  or  ssh username@alarik.lunarc.lu.se  On Linux this client is built-in to the system and no installation is neccesary. Windows does not have a standard SSH terminal so an external application is needed such as PuTTY (http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html) is needed.  To authenticate to the Lunarc system a two-factor authentication solution is used. Two-factor authentication uses two factors for authentication instead of one which is commonly found in normal username and password systems. In the Lunarc case the two factors are:  Username and password.\nOne-time password sent by SMS to a mobile phone.  When you apply for an account the mobile number is registered in our user database and will be used in the login process described in the following sections.", 
            "title": "Introduction"
        }, 
        {
            "location": "/login_howto/#logging-in-using-one-time-passwords-otp", 
            "text": "Logging in to the Lunarc system with OTP passwords is not very different from a normal SSH login, except for the additional extra password prompt. A typical session is shown in the following example:  login as: joeuser \nPassword: \nPlease enter your onetime password: 123456  If the otp and password are correct you will be logged in to the system.", 
            "title": "Logging in using One Time Passwords (OTP)"
        }, 
        {
            "location": "/login_howto/#linux", 
            "text": "To be prompted for the OTP password the ssh client must be configured for so called \"keyboard-interactive\" login. These settings can be modified in either /etc/ssh/ssh_config (Redhat systems) or in the home-directory ~/.ssh/config. An example configuration is shown below:  Host alarik.lunarc.lu.se \nPreferredAuthentications keyboard-interactive\n\nHost * \nPreferredAuthentications hostbased,publickey,keyboard-interactive,password  In the above example Platon is configured for keyboard-interactive login, but all other hosts are configured with default login options.  To reduce the number of logins to the system the ServerAlive option can also be added:  Host alarik.lunarc.lu.se \nPreferredAuthentications keyboard-interactive \nServerAliveInterval 10\n\nHost * \nPreferredAuthentications hostbased,publickey,keyboard-interactive,password", 
            "title": "Linux"
        }, 
        {
            "location": "/login_howto/#mac-os-x", 
            "text": "Mac OS X is already configured to handle the login to Platon with one time passwords (keyboard-interactive).   To reduce the number of logins to the system the ServerAlive option can also be added:  Host alarik.lunarc.lu.se \nPreferredAuthentications keyboard-interactive \nServerAliceInterval 10\n\nHost * \n    PreferredAuthentications hostbased,publickey,keyboard-interactive,password", 
            "title": "Mac OS X"
        }, 
        {
            "location": "/login_howto/#mac-os-x-107-lion-and-108-mountain-lion", 
            "text": "To login to a Lunarc system from a Mac system running Mac OS X 10.7 and 10.8, you need to unset the box \"Set locale environment variables on startup\" in the settings window of the terminal application, press \"cmd ,\" to get there.   English example:  Setting locale in lion (english)  Swedish example:  Setting locale in lion (english)  Note: This works on a per-theme basis.  In the above examples you will need to choose the theme \"Homebrew\" to connect to the Lunarc servers.", 
            "title": "Mac OS X 10.7 Lion and 10.8 Mountain Lion"
        }, 
        {
            "location": "/login_howto/#windows", 
            "text": "To be prompted for the OTP password the PuTTY client must be configured for so called \"keyboard-interactive\" login. Open PuTTY from the start-menu. Load the session options for Platon. Open the Connection/SSH/Auth item in the tree-view. Make sure the \"Attempt \"keyboard-interactive\" auth (SSH-2) is checked in the settings, see the following image:  To reduce the number of logins to the system the \"Seconds between keepalives\" can be changed to a value greater than 0. See the following figure:", 
            "title": "Windows"
        }, 
        {
            "location": "/login_howto/#file-transfers", 
            "text": "To reduce the number of otp passwords needed when transferring files the SFTP protocoll should be used instead of SCP, as each SCP connection will require a new OTP password.", 
            "title": "File transfers"
        }, 
        {
            "location": "/login_howto/#file-transfers-with-winscp", 
            "text": "The default filetransfer method in WinSCP is SFTP, so no special settings is needed for this. Just make sure that the \"File protocol\" setting is set to SFTP as shown in the following figure:  To reduce the number of logins to the system the \"Keepalive\" options can be set as shown in the following figure:  Please note:  that the \"Advanced options\" checkbox must be checked to access these settings", 
            "title": "File transfers with WinSCP"
        }, 
        {
            "location": "/quick_reference/", 
            "text": "Useful hints and short information on issues that may vary between the different systems\n\n\nInstalled softwared software\n\n\nTo see the installed software available through the modules system, issue the command\n\n\nmodule avail\n\n\n\nTo see the currently loaded modules\n\n\nmodule list\n\n\n\nTo load a module\n\n\nmodule add \nmodule_name\n\n\n\n\nTo unload a module\n\n\nmodule del \nmodule_name\n\n\n\n\nResource allocation\n\n\nNumber of cores\n\n\nThe number of cores for a job is specified in the batch script in the format\n\n\n#SBATCH -N \nnumber_of_nodes\n\n#SBATCH --tasks-per-node=\nnumber_of_cores_per_node\n\n\n\n\nAlarik has 16 cores per node. On this system, 64 cores would be allocated through\n\n\n# 64 cores on Alarik\n#SBATCH -N 4 #SBACTH --tasks-per-node=16\n\n\n\nMemory per core\n\n\nThe amount of memory per core is specified in the format\n\n\n#SBATCH --mem-per-cpu=\namount_of_memory_per_core_in_MB\n\n\n\n\nAlarik has nodes with 32 GB and 64 GB memory. The default allocation per core is therefore 2000 MB to match the smaller memory. To fully utilise the memory on the 64 GB nodes, the nodes have to requested specifically with \n-C mem64GB\n and the memory per core should be set to 4000 MB .\n\n\n# Twice the default amount of memory per core on Alarik nodes with 64 GB memory \n#SBATCH -C mem64GB #SBATCH --mem-per-cpu=4000\n\n\n\nFile systems\n\n\nHome directory\n\n\nCurrently all Lunarc systems have a home directory that is different for each system, i.e., the login directory for user xxxx is\n\n\n/home/xxxx\n\n\n\nThis directory can be referenced as \n$HOME\n.\n\n\nAs a rule, the home directory should not be used for job submission. It is intended for storing important files, such as the source code of user programs, and, of course, environment files, such as .bashrc.\n\n\nGlobal working directory\n\n\nFor job submission, there is a centre file system common to all Lunarc systems:\n\n\n/lunarc/nobackup/users/xxxx\n\n\n\nHere the xxxx has to be replaced with your userid.\n\n\nLocal working directory\n\n\nWhen a job is running, it has access to a temporary directory on the local disk of each allocated node. The directory can be referenced as \n$SNIC_TMP\n (or \n$TMPDIR\n). It will be deleted when the job finishes.\n\n\nIf a job is terminated prematurely, for example, if it exceeds the requested walltime, the files on the local disk will be lost. Files that would still be useful can be listed in a special file \n$SNIC_TMP/slurm_save_files\n. Filenames are assumed to be relative to \n$SNIC_TMP\n and should be separated by spaces or listed on separate lines. These files will be copied to the submission directory regardless whether the job ends as planned or is deleted, unless there is a problem with the disk or node itself.\n\n\nQuotas\n\n\nTo limit the disk usage, quotas are set for each user and filesystem. The status can be seen at login. A quota report can also be obtained by issuing the command\n\n\nsnicquota\n\n\n\nThe quota can be increased on request.\n\n\nTest queues\n\n\nOn Alarik, it is possible to request extra high priority to run short tests (maximum 1h) using at most 2 nodes using\n\n\n#SBATCH --qos=test\n\n\n\nFloating reservations are used to free two nodes every second hour between 8.00 and 20.00 to reduce the queue time for test jobs, which means that a shorter walltime increases likelihood of an earlier start. Only two such test jobs are allowed to run at the same time.\n\n\nOn Erik there is one two-GPU node reserved for tests (maximum 1 h) in a partition of its own, which is specified with\n\n\n#SBATCH -p test\n\n\n\nIt is not allowed to submit long series of jobs to a test queue.", 
            "title": "Quick reference"
        }, 
        {
            "location": "/quick_reference/#installed-softwared-software", 
            "text": "To see the installed software available through the modules system, issue the command  module avail  To see the currently loaded modules  module list  To load a module  module add  module_name   To unload a module  module del  module_name", 
            "title": "Installed softwared software"
        }, 
        {
            "location": "/quick_reference/#resource-allocation", 
            "text": "", 
            "title": "Resource allocation"
        }, 
        {
            "location": "/quick_reference/#number-of-cores", 
            "text": "The number of cores for a job is specified in the batch script in the format  #SBATCH -N  number_of_nodes \n#SBATCH --tasks-per-node= number_of_cores_per_node   Alarik has 16 cores per node. On this system, 64 cores would be allocated through  # 64 cores on Alarik\n#SBATCH -N 4 #SBACTH --tasks-per-node=16", 
            "title": "Number of cores"
        }, 
        {
            "location": "/quick_reference/#memory-per-core", 
            "text": "The amount of memory per core is specified in the format  #SBATCH --mem-per-cpu= amount_of_memory_per_core_in_MB   Alarik has nodes with 32 GB and 64 GB memory. The default allocation per core is therefore 2000 MB to match the smaller memory. To fully utilise the memory on the 64 GB nodes, the nodes have to requested specifically with  -C mem64GB  and the memory per core should be set to 4000 MB .  # Twice the default amount of memory per core on Alarik nodes with 64 GB memory \n#SBATCH -C mem64GB #SBATCH --mem-per-cpu=4000", 
            "title": "Memory per core"
        }, 
        {
            "location": "/quick_reference/#file-systems", 
            "text": "", 
            "title": "File systems"
        }, 
        {
            "location": "/quick_reference/#home-directory", 
            "text": "Currently all Lunarc systems have a home directory that is different for each system, i.e., the login directory for user xxxx is  /home/xxxx  This directory can be referenced as  $HOME .  As a rule, the home directory should not be used for job submission. It is intended for storing important files, such as the source code of user programs, and, of course, environment files, such as .bashrc.", 
            "title": "Home directory"
        }, 
        {
            "location": "/quick_reference/#global-working-directory", 
            "text": "For job submission, there is a centre file system common to all Lunarc systems:  /lunarc/nobackup/users/xxxx  Here the xxxx has to be replaced with your userid.", 
            "title": "Global working directory"
        }, 
        {
            "location": "/quick_reference/#local-working-directory", 
            "text": "When a job is running, it has access to a temporary directory on the local disk of each allocated node. The directory can be referenced as  $SNIC_TMP  (or  $TMPDIR ). It will be deleted when the job finishes.  If a job is terminated prematurely, for example, if it exceeds the requested walltime, the files on the local disk will be lost. Files that would still be useful can be listed in a special file  $SNIC_TMP/slurm_save_files . Filenames are assumed to be relative to  $SNIC_TMP  and should be separated by spaces or listed on separate lines. These files will be copied to the submission directory regardless whether the job ends as planned or is deleted, unless there is a problem with the disk or node itself.", 
            "title": "Local working directory"
        }, 
        {
            "location": "/quick_reference/#quotas", 
            "text": "To limit the disk usage, quotas are set for each user and filesystem. The status can be seen at login. A quota report can also be obtained by issuing the command  snicquota  The quota can be increased on request.", 
            "title": "Quotas"
        }, 
        {
            "location": "/quick_reference/#test-queues", 
            "text": "On Alarik, it is possible to request extra high priority to run short tests (maximum 1h) using at most 2 nodes using  #SBATCH --qos=test  Floating reservations are used to free two nodes every second hour between 8.00 and 20.00 to reduce the queue time for test jobs, which means that a shorter walltime increases likelihood of an earlier start. Only two such test jobs are allowed to run at the same time.  On Erik there is one two-GPU node reserved for tests (maximum 1 h) in a partition of its own, which is specified with  #SBATCH -p test  It is not allowed to submit long series of jobs to a test queue.", 
            "title": "Test queues"
        }, 
        {
            "location": "/batch_system/", 
            "text": "Using the job submission system on Alarik and Erik\n\n\nJoachim Hein, Jonas Lindemann, Anders Sj\u00f6str\u00f6m, Magnus Ullner\n\n\nDocument under active development - Check back frequently!\n\n\nA more in-depth guide to the job submission system on Alarik and Erik.\n\n\nContents\n\n\n\n\nContents\n\n\nSLURM - the batch system on Alarik and\nErik\n\n\nJob submission\n\n\nFirst example for a job\nsubmission\n\n\nThe job script and sbatch\n\n\nThe three parts of a job script\n\n\nResource statements for all\njobs\n\n\nWalltime\n\n\nJob naming\n\n\nSpecifying a project for users with multiple\nprojects\n\n\nSpecifying memory requirements\n\n\nControlling job output\n\n\nNotification\n\n\nJob dependencies\n\n\nTest queue\n\n\nExtra fat nodes on Alarik\n\n\nFat, extra fat and MIC nodes on\nErik\n\n\nResource statements for\nmultiprocessor\n\n\nTerminology around nodes, processors, cores,\ntasks\n\n\nOutline: Resource requests for multiprocessor\njobs\n\n\nExclusive node access\n\n\nSpecifying the number of nodes required for the\njob\n\n\nSpecifying the number of tasks per\nnode\n\n\nSpecifying the number of threads for a shared-memory\njob\n\n\nResource statements for hybrid programs using distributed and shared\nmemory\n\n\nSpecifying the number of cores to be required by the\njob\n\n\nProgram execution environment\n\n\nJob execution environment\n\n\nCompiler modules\n\n\nSLURM variables\n\n\nSNIC variables\n\n\nUsing the node local disks to improve I/O\nperformance\n\n\nLaunching MPI jobs in OpenMPI\n\n\nSubmitting, monitoring and manipulating jobs in\nSLURM\n\n\nSubmitting with sbatch\n\n\nStarting executables within SLURM with\nsrun\n\n\nMonitoring with squeue\n\n\nTerminating jobs with scancel\n\n\nExample job scripts\n\n\nJob scripts using the node local disk\n\n\nBasic run script\n\n\nVersion for codes requiring more memory than 2000\nMB\n\n\nRunning multiple serial jobs within a single job\nsubmission\n\n\nThe master script\n\n\nThe worker script\n\n\nMonitoring the progress of your multi-job\nsubmission\n\n\nMPI job using 16 tasks per node\n\n\nModifications required for file I/O on all nodes\n\n\nMPI jobs using fewer than 16 tasks per\nnode\n\n\nOpenMP jobs using shared memory\n\n\nThread binding for OpenMP codes\n\n\nThread binding with the GNU\ncompilers\n\n\nThread binding with the open64\ncompiler\n\n\nThread binding with the Intel\ncompiler\n\n\nHybrid-jobs using threads within an MPI\nframework\n\n\nThings to try for MPI-OpenMP hybrids with 16 threads per\ntask\n\n\nInteractive access to compute\nnodes\n\n\nStarting an interactive session\n\n\nModules and environment\nvariables\n\n\nKnown issues with the interactive\ncommand\n\n\n\n\nSLURM - the batch system on Alarik and Erik\n\n\nOn a modern HPC system efficient management of the compute resources is\nabsolutely crucial for the system to perform. Alarik and Erik are the\nfirst Lunarc systems to deploy SLURM (\nS\nimple \nL\ninux \nU\ntility\nfor \nR\nesource \nM\nanagement) as resource manager. For your program\nto be executed you have to describe to SLURM the resources required by\nyour program, the name of your program and the command line arguments\nyour program may require. SLURM also allows monitoring and manipulation\nof the progress of your programs execution.\n\n\nThis document contains two key parts. The \nfirst\npart\n describes in-depth the job submission system\nand its options. The \nsecond part\n gives example\nscripts for the most common use cases. They hopefully serve as a good\nstarting point when creating submission scripts fitting your needs and\nrequirements.\n\n\nJob submission\n\n\nFirst example for a job submission\n\n\nThe job script and sbatch\n\n\nYou register your program with SLURM for execution using the \nsbatch\n\ncommand. This is done easiest by using a \njob description file\n. The job\ndescription file is also know as a \njob script\n.\n\n\nA very simple job script, looks as follows:\n\n\n\n\n#!/bin/sh\n\n\n#SBATCH -t 00:05:00\n\n\necho \"hello\"\n\n\n\n\nWrite this into a file. In the following we assume the file is named\necho_script.sh, but in principle any name will do. You can now send the\nscript for execution using the sbatch command. This will execute the\n\u201cprogram\u201d echo on the backend.\n\n\nsbatch echo_script.sh\n\n\nThis should deliver a screen output similar to\n\n\n[fred@alarik Serial]\\$ sbatch echo_script.sh\n\n\nSubmitted batch job 7185\n\n\nWhere 7185 is the job number assigned by SLURM. Once your job has\nexecuted you will find a file slurm-7185.out in your directory which\ncontains the output and error messages from your program.\n\n\nThe three parts of a job script\n\n\nThe example echo_script.sh shows the three parts every job script\nrequires\n\n\n\n\n\n\nShell specification\n\n\n\n\n\n\nResource statement\n\n\n\n\n\n\nBody containing a UNIX script\n\n\n\n\n\n\nIn our example each part consists of a single line. The first line of\nour example contains the shell specification, in most cases the sh-shell\nas used here is just fine. The second line starting with #SBATCH\nspecifies the resources needed. In our case it asks for 10 minutes of\ncomputer time. If the jobs hasn\u2019t finished after that time, SLURM will\nterminate it. Job scripts typically contain more than one of these\nstatements, specifying e.g. more than one processor or more memory. The\nmost commonly used resource statements at Lunarc will be explained\nbelow. The resource statements are followed by a list of programs and\nUNIX commands to be executed on the system. This is actually a normal\nUNIX script and everything you can do in a UNIX script can be done here\nas well. In our example the script consists out of the UNIX echo\ncommand.\n\n\nResource statements for all jobs\n\n\nWe now describe a number of statements which are most commonly used to\nspecify resource requirements for all kind of jobs. Refer to \u201cman\nsbatch\u201d for more information.\n\n\nWalltime\n\n\nThe walltime attribute specifies the time requested for completing the\njob. The time is \nnot\n cpu-time but the total time, as measured by a\nnormal clock. In the previous example the time requested was 0 hours 5\nminutes and 0 seconds. Walltime is specified in seconds or using the\nfollowing notation:\n\n\n\n\nHours:Minutes:Seconds\n\n\n\n\nIf your calculation hasn\u2019t finished once the specified time has elapsed,\nSLURM will terminate your job. It is therefore \ngood practise\n to\nspecify a bit more time than you anticipate your job to take. This makes\nsure that you still get your results, even the jobs is slowed by some\ninterference, e.g. waiting for a write to a shared file system to\nfinish. However don\u2019t specify excessive extra time. Due to scheduling\nconstraints, jobs asking for less time will typically spend less time in\nthe queue, waiting for their execution. This also provides safety\nagainst depletion of your allocation. If, e.g., your job hangs, SLURM\nwill terminate your job and the project will be charged less time if the\nwalltime margin is not excessive.\n\n\nTo specify your walltime requirements write a statement like\n\n\n#SBATCH -t 00:10:00\n\n\ninto your job script.\n\n\nThe maximum walltime for any job on Alarik is 168h, which is the same as\n7 days. On Erik the maximum walltime for any job is 48h.\n\n\nJob naming\n\n\nAll jobs are given both a job identifier and a name, for easier\nidentification in the batch-system. The default name given to a job is\nthe file name of the submit script, which can make it difficult to\nidentify your job, if you use a standard name for your submit scripts.\nYou can give your job a name from inside the script by using the -J\noption:\n\n\n#SBATCH -J parameterTest\n\n\nThis will name your job \u201cparameterTest\u201d.\n\n\nSpecifying a project for users with multiple projects\n\n\nMost users are members of only one project. These users do not need to\nspecify a project in their in their submission script. The Lunarc set-up\nwill automatically use that project for accounting.\n\n\nA few users are members of more than project. In this case the system\nwould not know which project to charge for the run, so you need to\nspecify the project using the -A option:\n\n\n#SBATCH -A snic2015-x-xxx\n\n\nReplace the \u201csnic2015-x-xxx\u201d with the string naming your project. You\ncan inquire the correct string using the projinfo command or the SUPR\nsystem.\n\n\nSpecifying memory requirements\n\n\nAlarik has 32 GB of memory installed on the small memory nodes and 64 GB\nof memory on the large memory nodes. The default memory request per core\non the system is 2000 MB (a sixteenth of 32GB). If more then 2000 MB per\ncore is needed it has to be requested explictly with the\n\n--mem-per-cpu\n option of sbatch. In this case you also have to\nrequest allocation on a large memory node using the \n-C mem64GB\n\noption of sbatch. The following show an example how to request 4000 MB\nor main memory per compute core used:\n\n\n#SBATCH -C mem64GB\n\n\n#SBATCH --mem-per-cpu=4000\n\n\nWhen requesting more than 2000 MB of memory, your jobs may spend a\nlonger time in the queue, waiting for execution, since it needs to wait\nfor run-slot(s) on the large memory nodes to become available. When\nrequesting more then 4000 MB per processing core, your jobs will be\ncharged at a higher rate. In this case some processing cores have to\nremain idle since you are using more than your fair share of memory.\n\n\nErik has 64 Gb of memory on the standard nodes. Each node has two CPUs\nwith eight cores each. The default memory request per core is therefore\n4000 MB of memory. As in the case of Alarik, if more than 4000MB of\nmemory per core is needed it has to be described as above.\n\n\nControlling job output\n\n\nBy default, the output which your job writes to stdout and stderr is\nwritten to a file named\n\n\nslurm_%j.out\n\n\nThe %j in the file name will be replaced by the jobnumber SLURM assigns\nto your job. This ensures that the output file from your job is unique\nand different jobs do not interfere with each other's output file.\n\n\nIn many cases the default file name is not convenient. You might want to\nhave a file name which is more descriptive of the job that is actually\nrunning - you might even want to include important meta-data, such as\nphysical parameters, into the output filename(s). This can be achieved\nby using the -o and -e options of sbatch. The -o option specifies the\nfile containing the stdout and the -e option the file containing the\nstderr. It is good practise to include the %j string into the filenames.\nThat will prevent jobs from overwriting each other's output files. The\nfollowing gives an example:\n\n\n#SBATCH -o calcflow_m1_%j.out\n\n\n#SBATCH -e calcflow_m1_%j.err\n\n\nYou can give the same filename for both options to get stdout and stderr\nwritten to the same file.\n\n\nNotification\n\n\nSLURM on the systems can send you email if the status of your job\nchanges as it progresses through the job queue. To use this feature you\nneed to specify the email address using the --mail-user option and\nspecify the event you want to get notified about using the --mail-type\noption. The following\n\n\n#SBATCH --mail-user=fred@institute.se\n\n\n#SBATCH --mail-type=END\n\n\nWill send an email to the address fred@institute.se once the job has\nended. Valid type values, selecting the event you can get notified\nabout, are BEGIN, END, FAIL, REQUEUE, and ALL (any state change).\n\n\nJob dependencies\n\n\nTo describe job dependencies, use the -d option of sbatch. This is\nparticularly useful for job dependencies, in workflows.\n\n\nTo illustrate this consider the following example. You require a serial\njob to create a mesh for your simulation. Once this has finished, you\nwant to start a parallel job, which uses the mesh. You first submit the\nmesh creation job using sbatch\n\n\n[fred@alarik Simcode]\\$ sbatch run_mesh.sh\n\n\nSubmitted batch job 8042\n\n\nAs discussed, sbatch returns you a jobid, 8042 in this example. You use\nthis to declare your dependency when submitting the simulation job to\nthe queue\n\n\n[fred@alarik Simcode]\\$ sbatch -d afterok:8042 run_sim.sh\n\n\nSubmitted batch job 8043\n\n\nWhen using squeue to monitor job 8043, this should now be in status\npending (PD) with the reason of dependency. Another common use case for\nthis functionality is a simulation requiring many days of computer times\nbeing split into a number of submissions.\n\n\nTest queue\n\n\nTo run short tests, it is possible to request extra high priority on\nAlarik with the help of\n\n\n#SBATCH --qos=test\n\n\nFor one such job, the maximum walltime is 1 h and the maximum number of\nnodes is two and a user is only allowed to run two such jobs\nsimultaneously. A system of floating reservations is used to free two\nnodes every second hour between 8.00 and 20.00 to reduce the queue time\nfor test jobs. The way it works also means that the shorter the test\njob, the more likely it is to start sooner rather than later. It is not\nallowed to use qos=test for series of production runs.\n\n\nOn Erik there is one two-GPU node reserved for tests in a partition of\nits own, which is specified with\n\n\n#SBATCH -p test\n\n\nLike on Alarik, the maximum walltime is 1 h.\n\n\nExtra fat nodes on Alarik\n\n\nAlarik has four nodes with 48 cores and 128 GB memory. To access them,\nthe partition extra has to be specified\n\n\n#SBATCH -p extra\n\n\nFurthermore, the amount of memory per requested core also needs to be\ngiven.\n\n\n#SBATCH --mem-per-cpu=\\\nmemory in MB>\n\n\nOtherwise, the default value of 2 000 MB will be set at and if more is\nused, slurm can kill the job. For example, to run on the 48 cores of a\nsingle node and use a total of 128 000 MB of memory (for more on\nmultiprocessor statements, see the next section):\n\n\n#SBATCH -N 1\n\n\n#SBATCH --tasks-per-node=48\n\n\n#SBATCH --mem-per-cpu=3000\n\n\n#SBATCH -p extra\n\n\nFat, extra fat and MIC nodes on Erik\n\n\nErik has 7 nodes with 4 GPUs (and 96 GB of memory). To access them, the\npartition fat has to be specified\n\n\n#SBATCH -p fat\n\n\nOne node is equipped with 8 GPUs (and 96 GB of memory), which is in\npartition extra\n\n\n#SBATCH -p extra\n\n\nThere is also one node with two Xeon Phi (MIC) cards in the partition\nmic\n\n\n#SBATCH -p mic\n\n\nThere is also one node with two Nvidia K80 cards in the partition new\n\n\n#SBATCH -p new\n\n\nIf no -p option is specified, normal nodes with two Nvidia K20 cards\nwill be allocated to the job.\n\n\nResource statements for multiprocessor\n\n\nIn HPC it is very common to have many processing elements working on a\njob. The extra processing power can be utilised to process large\nproblems beyond the capabilities of a single processing element. It can\nalso be used to swiftly perform a number of calculations within a single\njob submission.\n\n\nTerminology around nodes, processors, cores, tasks\n\n\nThere is a a lot of structure within modern HPC equipment. For the\npurposes of this user guide we will stick to the following terminology:\n\n\n\n\nTerm\n    \nExplanation\n                                                                                                                                                                                                                                                             \nNumber on Alarik\n          \nNumber on Erik\n\n\n\n\nNode        A physical computer                                                                                                                                                                                                                                                         \nStandard\n:                 \nStandard\n:\n\n\n                                                                                                                                                                                                                                                                                      200                           16\n\n                                                                                                                                                                                                                                                                                      **Extra**:                    **Fat**:\n\n                                                                                                                                                                                                                                                                                      4                             7\n\n                                                                                                                                                                                                                                                                                                                    **Extra**:\n\n                                                                                                                                                                                                                                                                                                                    1\n\n                                                                                                                                                                                                                                                                                                                    **Mic:**\n\n                                                                                                                                                                                                                                                                                                                    1\n\n                                                                                                                                                                                                                                                                                                                    **New:**\n\n                                                                                                                                                                                                                                                                                                                    1\n\n\n\nProcessor   This denotes a the multi-core processor, housing many processing elements                                                                                                                                                                                                   \nStandard\n:                 2 per node\n\n\n                                                                                                                                                                                                                                                                                      2 per node\n\n                                                                                                                                                                                                                                                                                      **Extra**:\n\n                                                                                                                                                                                                                                                                                      4 per node\n\n\n\nGPU         This denotes a nvidia co-processor                                                                                                                                                                                                                                          0                             \nStandard\n:\n\n\n                                                                                                                                                                                                                                                                                                                    2 per node\n\n                                                                                                                                                                                                                                                                                                                    **Fat**:\n\n                                                                                                                                                                                                                                                                                                                    4 per node\n\n                                                                                                                                                                                                                                                                                                                    **Extra**:\n\n                                                                                                                                                                                                                                                                                                                    8 per node\n\n                                                                                                                                                                                                                                                                                                                    **Mic:**\n\n                                                                                                                                                                                                                                                                                                                    0 per node\n\n                                                                                                                                                                                                                                                                                                                    **New:**\n\n                                                                                                                                                                                                                                                                                                                    2 cards per node\n\n                                                                                                                                                                                                                                                                                                                    2 logical per card\n\n                                                                                                                                                                                                                                                                                                                    4 logical per node\n\n\n\nSocket      This is the \u201cplug\u201d the processor gets plugged into. Used as a synonym for the processor                                                                                                                                                                                     \nStandard\n:                 2 per node\n\n\n                                                                                                                                                                                                                                                                                      2 per node\n\n                                                                                                                                                                                                                                                                                      **Extra**:\n\n                                                                                                                                                                                                                                                                                      4 per node\n\n\n\nCore        Individual processing element                                                                                                                                                                                                                                               \nStandard\n:                 16 per node\n\n\n                                                                                                                                                                                                                                                                                      16 per node                   8 per processor\n\n                                                                                                                                                                                                                                                                                      8 per processor\n\n                                                                                                                                                                                                                                                                                      **Extra**:\n\n                                                                                                                                                                                                                                                                                      48 per node\n\n                                                                                                                                                                                                                                                                                      12 per processor\n\n\n\nTask        This is a software concept. It denotes a process, which is an instance of a running program. It has its own data and instruction stream(s). It can fork multiple threads to increase the computational speed. Serial programs and pure MPI programs do not spawn threads.   User controls in job script   User controls in job script\n\n\nThread      This is also a software concept. A thread is a stream of instructions executed on the hardware. It is part of a task and shares resources such as data with other threads within the same task.                                                                             User controls in job script   User controls in job script\n\n\n\n\nOutline: Resource requests for multiprocessor jobs\n\n\nWhen running multi processor jobs on the Lunarc clusters, one should\nspecify:\n\n\n\n\n\n\nThe number of nodes required by the jobs\n\n\n\n\n\n\nThe number of computational tasks per node\n\n\n\n\n\n\nThe number of threads spawned by each task\n\n\n\n\n\n\nFor a pure MPI job or when processing a large number of serial jobs in a\nso called task farm, one will typically only specify the items 1 and 2,\nwhile for a threaded job, using e.g. OpenMP or Java, one will typically\nonly specify items 1 and 3.\n\n\nIt is typically not advisable to have the product of items 2 and 3\nexceeding the number of cores per node, which is 16 for standard Alarik\nand Erik compute nodes. On the Alarik extra compute nodes this number is\n48. In most cases users requesting multiple nodes will want the product\nto equal the number of cores per node. The syntax how to control nodes,\ntasks per node and threads per task is explaned below.\n\n\nExclusive node access\n\n\nFor parallel codes using MPI or OpenMP it is typically best to keep\ninterference on the nodes at a minimum, that is to have exclusive access\nto the nodes you are using. This also applies to specialist and\nexperimental work, which would interfere very badly with other user\u2019s\ncodes on the nodes. Adding\n\n\n\n\n#SBATCH --exclusive\n\n\n\n\nto your job script will ensure that SLURM will allocate dedicated nodes\nto your job. Obviously your project gets charged for the full costs of\nthe nodes you are using, that is in case of Alarik and Erik 16 cores per\nnode.\n\n\nSpecifying the number of nodes required for the job\n\n\nIn SLURM one requests the number of nodes for a job with the \n-N\n\noption. The following statement requests four nodes for your job:\n\n\n#SBATCH -N 4\n\n\nImportant:\n without using either the --tasks-per-node or\nthe --cpus-per-task options of sbatch, this will reserve a single core\nper node, so four in total, which is most likely not what you want.\n\n\nSpecifying the number of tasks per node\n\n\nUse the --tasks-per-node of sbatch to specify the number of tasks you\nrequire per node. Most multinode job will set this equal to the number\nof cores availble per node. The following example asks for 16 task per\nnode:\n\n\n#SBATCH --tasks-per-node=16\n\n\nThis should be used together with the -N option, specifying the number\nof nodes to be used. The default value for the number of tasks per node\nis 1. For example to specify the requirements for an MPI job with 64\ntasks or multiprocessor job using 64 processors to process a larger\nnumber of serial jobs one would specify\n\n\n#SBATCH -N 4\n\n\n#SBATCH --tasks-per-node=16\n\n\nWhen using fewer than 16 tasks per node and you want to prevent other\nuser\u2019s jobs sharing your node, you need to consider using\nthe --exclusive option. If --exclusive is not specified, SLURM might\nplace other tasks onto your node.\n\n\nSpecifying the number of threads for a shared-memory job\n\n\nIf you want to run shared-memory applications using threads, e.g. OpenMP\nparallised code or Java applications, you need to specify the number of\nthreads you require per task. This can be done with the --tasks-per-node\noption of sbatch.\n\n\nFor a standard shared-memory program, which doesn\u2019t also use distributed\nmemory programming models such as MPI, one is restricted to a single\nnode. On that node, one can request as many threads as there are cores\non the node. On the standard Alarik compute nodes one can efficiently\nuse up to 16 threads. Use the following resource statement:\n\n\n#SBATCH -N 1\n\n\n#SBATCH --tasks-per-node=16\n\n\nIf your program is only efficient at a lower thread count, you may want\nto use e.g.:\n\n\n#SBATCH -N 1\n\n\n#SBATCH --tasks-per-node=4\n\n\nif you only want to use four threads. The Alarik extra nodes with 48\ncores allow for very wide shared-memory jobs:\n\n\n#SBATCH -N 1\n\n\n#SBATCH --tasks-per-node=48\n\n\n#SBATCH --mem-per-cpu=3000\n\n\n#SBATCH -p extra\n\n\nResource statements for hybrid programs using distributed and shared memory\n\n\nSo-called hybrid programs, using both distributed and shared-memory\ntechniques have recently become popular. For example: for a program\nusing 32 MPI tasks, each task spawning 2 OpenMP threads one would\nrequire 4 nodes and place eight tasks on each node. The number of\nthreads per task is given by --cpus-per-task. The resource statement\nwould look as follows:\n\n\n#SBATCH -N 4\n\n\n#SBATCH --tasks-per-node=8\n\n\n#SBATCH --cpus-per-task=2\n\n\nSpecifying the number of cores to be required by the job\n\n\nIn special cases, such as using very unusal numbers of tasks, the \n-n\n\noption of sbatch to specify the number of cores might become useful.\nWhen running a pure MPI program this option corresponds to the \nnumber\nof tasks\n required for your program. The following statement in a job\nscript would reserve 63 cores for your job\n\n\n#SBATCH -N 4\n\n\n#SBATCH --tasks-per-node=16\n\n\n#SBATCH -n 63\n\n\nPlease consider using the --exclusive option of sbatch to avoid SLURM\nspreading your job on more nodes than necessary and placing other user\u2019s\njobs on nodes utilising fewer than 16 cores for your job. Other user\u2019s\njobs could via shared node resources (memory bus, cache, FPU, \u2026)\ninterfere with your job and introduce undue operational noise. Such\nnoise is something parallel program execution can be extremely sensitive\nto.\n\n\nProgram execution environment\n\n\nJob execution environment\n\n\nWhen submitting your job to SLURM using sbatch, your entire environment\nincluding the currently loaded modules gets copied. This behaviour is\ndifferent from earlier Lunarc machines, including Platon. On Alarik,\nwhen hitting sbatch:\n\n\n\n\nMake sure that the loaded modules and any environment variable you\n    \n may have set will not be in conflict with the environment expected\n    \n by the job script\n\n\n\n\nCompiler modules\n\n\nOn Alarik we automatically load a modern version of the GCC compiler,\nwhich supports the deployed AMD Opteron processors. At the time of\nwriting this is version 4.6.2 of GCC. If you prefer using a different\ncompiler, you can add the desired module, e.g., version 12.1 of the\nIntel compiler\n\n\nmodule add intel/12.1\n\n\nIf different modules have files with the same names in the search path,\nthose of the module added last will be picked. Generally this is not a\nproblem, but the compiler wrappers in the openmpi modules have the same\nnames and it safest to only have one loaded at a time.\n\n\nOn Erik the same compilers as on Alarik are present. Note that the\nprocessors on Erik are of the Intel Xeon type and thus utilize the mkl\nas supplied.\n\n\nSLURM variables\n\n\nTo come\n\n\nSNIC variables\n\n\nThe SNIC meta-centres have agreed on a set of environment variables\nwhich should improve the portability of (parts of) job-scripts between\nSNIC sites. On Alarik the following variables are set by the system:\n\n\n\n\nEnvironment variable\n   \nExplanation\n                                                                               \nValue on Alarik\n   \nValue on Erik\n\n\n\n\nSNIC_SITE                 Identifying the SNIC site you are using                                                       lunarc                lunarc\n\n\nSNIC_RESOURCE             Identifying the compute resource you are using                                                alarik                erik\n\n\nSNIC_BACKUP               User directory which is:                                                                      /home/\\\nuser>        /home/\\\nuser>\n\n\n                         \n Regularly backed up against accidental deletion\n\n                         \n Typically extremely limited space\n\n                         \n Use for e.g. precious source code\n\n\n\nSNIC_NOBACKUP             User directory which is:                                                                      /lunarc               /lunarc\n\n\n                         \n Accessible on all Lunarc systems                                                            /nobackup             /nobackup\n\n                         \n Outliving individual systems                                                                /users/\\\nuser\\\n       /users/\\\nuser\\\n\n\n                         \n For storing larger amounts of data\n\n                         \n Not backed up against accidental deletion\n\n                         \n Protected against disk failure (RAID configuration)\n\n                         \n On Alarik: the primary root directory for job management (job scripts, input/output data)\n\n\n\nSNIC_TMP                  Directory for best performance during a job                                                   \njobid dependent\n     \njobid dependent\n\n\n                         At Lunarc:\n\n                         \n Local disk on nodes\n\n                         \n Storing temporary data during job execution\n\n                         \n High bandwidth\n\n                         \n Automatically deleted\n\n                         \n Transfer data with long-term value to SNIC\\_NOBACKUP before job has finished\n\n\n\n\n\nUsing the node local disks to improve I/O performance\n\n\nOn Alarik and Erik, all nodes have a local disk. This disk offers\nsuperior bandwidth when compared to accessing your home space or the\n/lunarc/nobackup centre storage. In particular when files are read or\nwritten repeatedly during execution it is advisable to copy the input\ndata onto the local disk prior to job execution and copy the result\nfiles back to the submission directory once your program has finished.\nDuring its execution, your program would then read and write to local\ndisk.\n\n\nIn case of Alarik and Erik, the submission directory typically resides\non the /lunarc/nobackup centre storage. All data left on the node local\ndisks \nwill be deleted\n when your job has finished. You need to copy\neverything of interest to a more permanent storage space such as\n/lunarc/nobackup. If a job is terminated prematurely, for example, if it\nexceeds the requested walltime, the files on the local disk (in\n\\$SNIC_TMP) will be lost.\n\n\nFiles that would still be useful can be listed in a special file\n\n\\$SNIC_TMP/slurm_save_files\n. Filenames are assumed to be relative\nto \\$SNIC_TMP and should be separated by spaces or listed on separate\nlines. Wildcards are allowed. These files will be copied from the local\ndisk where \\$SNIC_TMP/slurm_save_files exists to the submission\ndirectory regardless whether the job ends as planned or is deleted,\nunless there is a problem with the disk or node itself. Note that the\nslurm_save_files feature is unique to Lunarc.\n\n\nFor the required UNIX scripting you should use the following environment\nvariables. Example scripts using this technique are provided in the\nexample section of this document. Contact the help desk if you have\nspecific requirements and require consultation.\n\n\n\n\nVariable\n         \nAddressed Volume\n\n\n\n\nSNIC_TMP            node local disk\n\n\n                   copy your input data here and start your program from here\n\n\n\nTMPDIR               node local disk\n\n\n                   Many applications use this environment variable to locate a disk volume for temporary scratch space. If your application follows that convention nothing needs to be done.\n\n\n\nSLURM_SUBMIT_DIR   submission directory\n\n\n                   where you ran sbatch\n\n\n\n\n\nLaunching MPI jobs in OpenMPI\n\n\nTo execute message passing parallel jobs these should be built against\none of the MPI libraries provided by the support team as a module. To\nexecute an MPI job, your job script should do the following\n\n\n\n\n\n\nLoad MPI module relevant for the compiler you are using\n\n\n\n\n\n\nStart the program with mpirun\n\n\n\n\n\n\nOn Alarik the correct binding is crucial to achieve good\n    \n performance. When using 16 task per node, we recommend using\n    \n the -bind-to-core option of mpirun. When using fewer than 16 tasks\n    \n we recommend experimenting whether not using binding helps or\n    \n hinders performance.\n\n\n\n\n\n\nSubmitting, monitoring and manipulating jobs in SLURM\n\n\nSubmitting with sbatch\n\n\nOne uses the command sbatch to submit a job script to the batch system\nfor execution. SLURM will reply with the jobid number. The job will then\nbe held in the queue until the requested resources become available. A\ntypical use case looks as follows:\n\n\n\n\n[fred@alarik MPItest]\\$ sbatch runjob.sh\n\n\nSubmitted batch job 7197\n\n\n\n\nUser fred submitted the script runjob.sh to the job queue and got the\njobid 7197 assigned to it.\n\n\nStarting executables within SLURM with srun\n\n\nThe command srun allows to start executables in a way managed by SLURM.\nThis is particularly effective if you want to process a large number of\njobs within a single submission to the batch system. A use case of srun\nto start many serial jobs in a single multicore submission scipt is\ndiscussed in the \nexample section\n.\n\n\nMonitoring with squeue\n\n\nThe command squeue will show you the current state of the job queue. The\nstandard output, created by calling squeue without any options looks as\nfollows:\n\n\nJOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n\n\n7303 snic hybrid_n fred PD 0:00 32 (Priority)\n\n\n7302 snic hybrid_n fred PD 0:00 32 (Priority)\n\n\n7301 snic hybrid_n fred PD 0:00 32 (Resources)\n\n\n7304 snic preproce karl PD 0:00 6 (Priority)\n\n\n7300 snic hybrid_n fred R 0:24 32 an[001-032]\n\n\n7305 snic preproce karl R 0:37 6 an[081-086]\n\n\n7306 snic hybrid_n fred R 0:37 6 an[081-086]\n\n\n7307 snic testsimu sven R 0:07 1 an081\n\n\nThe first column gives the jobid, the third the job names, followed by\nthe userid. The column labeled \u201cST\u201d gives the job state. The most\nimportant states are:\n\n\n\n\nSymbol\n   \nMeaning\n\n\n\n\n\n\nR          running\n\n\n\n\nPD           pending, awaiting resources\n\n\nCG           completing\n\n\n\n\nThe state column is followed by the time used by the job and number of\nnodes utilised by the job. For running jobs the last column gives the\nnames of the nodes utilised or if the job is waiting a reason why it is\nnot executing.\n\n\nThe squeue command is highly configurable. Useful options include -u\nmyid, which lists all jobs of the user myid and also the --start option.\nThe latter gives the current estimate of when SLURM expects the job to\nstart. Note, that this can shift in either direction, depending on e.g.\njobs finishing earlier than specified or jobs with higher priority\ngetting added to the job queue.\n\n\nThe command jobinfo is a script that sorts the output of squeue into\nrunning and waiting jobs. It also shows additional information, such as\nhow long running jobs have left and in some cases when waiting jobs are\nexpected to start.\n\n\nTerminating jobs with scancel\n\n\nIt is frequently required to remove jobs from the queue. This might be\nthat you discover a problem in your job specification or intermediate\nresults of running job indicating that something went wrong. Use scancel\nto remove a job from the job queue. To do so you need the jobid, which\nis best queried with the squeue command. To remove the job with the\njobid 7103 from the job queue type\n\n\nscancel 7103\n\n\nExample job scripts\n\n\nIn this section we provide sample scripts for typical use cases.\n\n\nJob scripts using the node local disk\n\n\nBasic run script\n\n\nAs discussed the node local disk provides better I/O-bandwidth than the\nother file systems available on Alarik. The following script assumes the\nprogram processor reads the file \ninput.dat\n and produces a file\n\nresult.dat\n.\n\n\nThis example executes a single serial program and is suitable for the\noccasional serial job. If you need to process a large number of serial\njobs, we request you bundle them into a single submission. Refer to the\nsection \u201c\nR\nunning multiple serial jobs within a\nsingle job submission\n\u201d for a scripting example.\n\n\nThe script copies the input data and the program executable from the\nsubmission directory to the node local disk, executes the program on the\nnode local disk and copies the result file back to the submission\ndirectory for safe keeping. The individual steps are highlighted by\ncomments starting with a \u201c#\u201d. These comment lines can be kept in the\nfile.\n\n\nThis is the Lunarc standard example and represents \nrecommended\npractise\n for a basic serial job. You need to customise the file to\nsuit your specific needs. The script is suitable for jobs consuming no\nmore than 2000 MB of main memory.\n\n\n#!/bin/bash\n\n\n#\n\n\n# job time, change for what your job requires\n\n\n#SBATCH -t 00:10:00\n\n\n#\n\n\n# job name\n\n\n#SBATCH -J data_process\n\n\n#\n\n\n# filenames stdout and stderr - customise, include %j\n\n\n#SBATCH -o process_%j.out\n\n\n#SBATCH -e process_%j.err\n\n\n# write this script to stdout-file - useful for scripting errors\n\n\ncat \\$0\n\n\n# copy the input data and program to node local disk\n\n\n# customise for your input file(s) and program name\n\n\ncp -p input.dat processor \\$SNIC_TMP\n\n\n# change to the execution directory\n\n\ncd \\$SNIC_TMP\n\n\n# run the program\n\n\n# customise for your program name and add arguments if required\n\n\n./processor\n\n\n# rescue the results to the submission directory\n\n\n# customise for your result file(s)\n\n\ncp -p result.dat \\$SLURM_SUBMIT_DIR\n\n\nWe recommend to be selective about the files you copy between the\nsubmission directory and the local node disk. If you have multiple input\nand result files you need to modify the copy statements accordingly. The\nabove example assumes your program has been compiled with the GCC\ncompiler loaded by default. If it has been compiled with a different\ncompiler you need to load the compiler module by adding a line similar\nto\n\n\nmodule add intel/12.1\n\n\nIf you are running on Erik, it is necessary to add the support for the\nGPU with the line\n\n\nmodule add cuda\n\n\nprior to the line ./processor. You need to consult with the person who\nbuild the executable for you. Lunarc provided modules typically complain\nif the wrong compiler is loaded and are hence self-documenting.\n\n\nVersion for codes requiring more memory than 2000 MB\n\n\nIf your program requires more memory than 2000 MB, use the following\nscript. This example is set up to use 4000 MB. If you need even more you\ncan request this, but your runs will be charged to your project at a\nhigher rate, since other cores have to remain idle. The comments on the\nprevious example also apply here\n\n\n#!/bin/bash\n\n\n#\n\n\n# job time, change for what your job requires\n\n\n#SBATCH -t 00:10:00\n\n\n#\n\n\n# job name\n\n\n#SBATCH -J data_process\n\n\n#\n\n\n# filenames stdout and stderr - customise, include %j\n\n\n#SBATCH -o process_%j.out\n\n\n#SBATCH -e process_%j.err\n\n\n#\n\n\n# requesting a large memory node and 4000 MB or main memory\n\n\n#SBATCH -C mem64GB\n\n\n#SBATCH --mem-per-cpu=4000\n\n\n# write this script to stdout-file - useful for scripting errors\n\n\ncat \\$0\n\n\n# copy the input data and program to node local disk\n\n\n# customise for your input file(s) and program name\n\n\ncp -p input.dat processor \\$SNIC_TMP\n\n\n# change to the execution directory\n\n\ncd \\$SNIC_TMP\n\n\n# run the program\n\n\n# customise for your program name and add arguments if required\n\n\n./processor\n\n\n# rescue the results to the submission directory\n\n\n# customise for your result file(s)\n\n\ncp -p result.dat \\$SLURM_SUBMIT_DIR\n\n\nSince fewer nodes are equipped with 64 GB of memory, you have to allow\nfor longer queueing times until resource become available.\n\n\nRunning multiple serial jobs within a single job submission\n\n\nWhen you need to run many serial jobs, similar to the ones \ndescribed\nabove\n, these should be bundled together and\nsubmitted to the job queue in a small number of submissions or even a\nsingle submission. With SLURM is perfectly reasonable to run several\nhundred individual jobs in a single submission. To speed up the\nprocessing of your jobs, you can ask for the cores from a number of\nnodes. The concept is known as a \ntask-farm\n. The individual job are\nknown as \njob-steps\n.\n\n\nThe following is an example processing 200 such jobs using 16 cores from\na single node. The scripting use two scripts, the master script and the\nworker script. The Master script requests the resources (number of\ncores, job time, ...) and then registers 200 copies of the worker script\nwith SLURM using the command srun. The worker script is a modification\nof the \nbasic script\n described above.\n\n\nIn our example this will then start sixteen jobs on the sixteen cores\nyou requested. Once a job has finished, it will take an unprocessed job\nand place it on the idle core for processing. This will continue until\nall jobs are processed. The ordering of the jobs can not be relied on.\n\n\nFor our example the entire setup assumes the submission directory has\n200 sub-directories, named job_0, job_1, job_2, \u2026, job_199. Each of\nthe directories contains the input data and the program executable to be\nrun.\n\n\nKeep the number of jobs-steps at a reasonable level. Recent testing by\nthe Lunarc support team has shown that, when including a sleep statement\ninside the do loop the setup can be used to processes 800 jobs.\n\n\nThe master script\n\n\nThe master script describes the resources required and registers, once\nrunning the worker tasks with SLURM. In most cases modifying the number\nof cores needed, the total job time and the number of jobs to be\nprocessed should be all that is required.\n\n\n#!/bin/sh\n\n\n# requesting the number of cores needed\n\n\n#SBATCH -N 1\n\n\n#SBATCH --tasks-per-node=16\n\n\n#SBATCH --exclusive\n\n\n#\n\n\n# job time, change for what your job farm requires\n\n\n#SBATCH -t 20:00:00\n\n\n#\n\n\n# job name and output file names\n\n\n#SBATCH -J jobFarm\n\n\n#SBATCH -o res_jobFarm_%j.out\n\n\n#SBATCH -e res_jobFarm_%j.out\n\n\ncat \\$0\n\n\n# set the number of jobs - change for your requirements\n\n\nexport NB_of_jobs=200\n\n\n# Loop over the job number\n\n\nfor ((i=0; i\\\n\\$NB_of_jobs; i++))\n\n\ndo\n\n\nsrun -Q --exclusive -n 1 -N 1 \\\n\n\nworkScript.sh \\$i \n> worker_\\${SLURM_JOB_ID}_\\${i} \n\n\nsleep 1\n\n\n\n\ndone\n\n\n\n\n# keep the wait statement, it is important!\n\n\nwait\n\n\nThe script assumes that the job is described in a script file\n\u201cworkScript.sh\u201d, which takes a single number identifying the job\ndirectory to be accessed as a command line argument. Please note the\n\u201csleep 1\u201d command inside the do loop. In our testing this greatly\nenhances the stability by submitting the actual jobs over a longer\nperiod of time. With this statement included the script was able to\nsuccessfully handle up to about 800 outstanding jobs on 16 and 32 cores.\nFor reasons of job reliability, we therefore recommend not to process\nmore than 800 jobs in a single script. However it is possible to process\nsignificantly larger job numbers than 800 by carefully tuning sleep-time\nand core count in relation to the average job-time.\n\n\nRemarks:\n When using srun inside a batch script many srun-options act\ndifferently compared to using srun within a different environment.\nConsult the man-page of srun for documentation and contact the Lunarc\nhelp desk if your require further consultancy.\n\n\nThe worker script\n\n\nThis outlines the worker script. Compared to the script describing a\n\nsingle serial job\n, a few modifications are\nrequired:\n\n\n\n\n\n\nTo avoid access conflicts between the individual jobs, each job\n    \n creates a job private sub-directory on the node local disk.\n\n\n\n\n\n\nThe input file(s) are expected in the sub_directories job_0,\n    \n job_1, job_2, \u2026 of the submission directory. The result file(s)\n    \n will also be placed in these directories.\n\n\n\n\n\n\nThe example assumes a single input file and single result file. If\n    \n you have multiple input and/or result files modifications are\n    \n needed, as are modifications for that actual names of your file\n\n\n\n\n\n\nThe present set up allows for different executables for each\n    \n job-stop. The script assumes to find an executable named\n    \n \u201cprocessor\u201d in the same location as the input file(s). If you all\n    \n job steps use the same executable the scripts can be simplified.\n\n\n\n\n\n\nOnce a job-step has finished and the result file has been copied\n    \n back, the job private sub-directory on the node local disk is\n    \n removed to prevent the disc from overflow.\n\n\n\n\n\n\nIf you are using the above master script, the script should be named\n\u201cworkScript.sh\u201d.\n\n\n#!/bin/sh\n\n\n# document this script to stdout (assumes redirection from caller)\n\n\ncat \\$0\n\n\n# receive my worker number\n\n\nexport WRK_NB=\\$1\n\n\n# create worker-private subdirectory in \\$SNIC_TMP\n\n\nexport WRK_DIR=\\$SNIC_TMP/WRK_\\${WRK_NB}\n\n\nmkdir \\$WRK_DIR\n\n\n# create a variable to address the \"job directory\"\n\n\nexport JOB_DIR=\\$SLURM_SUBMIT_DIR/job_\\${WRK_NB}\n\n\n# now copy the input data and program from there\n\n\ncd \\$JOB_DIR\n\n\ncp -p input.dat processor \\$WRK_DIR\n\n\n# change to the execution directory\n\n\ncd \\$WRK_DIR\n\n\n# run the program\n\n\n./processor\n\n\n# rescue the results back to job directory\n\n\ncp -p result.dat \\${JOB_DIR}\n\n\n# clean up the local disk and remove the worker-private directory\n\n\ncd \\$SNIC_TMP\n\n\nrm -rf WRK_\\${WRK_NB}\n\n\nMonitoring the progress of your multi-job submission\n\n\nUsing the -s option of sbatch you can monitor the progression of the\nindividual job-steps of your multi-job submission. Please keep in mind,\nthat the step number SLURM assigns to your job and the one you assign\ntypically differs from the loop index used in the master script.\n\n\nThe below is an output from squeue when running a script processing 500\njobs on 32 cores. The jobid of the job is 8070. The output shows the\njob-steps the script is presently processing\n\n\n[fred@alarik MultiSerialTest]\\$ squeue -j 8070 -s\n\n\n\n\nSTEPID NAME PARTITION USER TIME NODELIST\n\n\n8070.130 small_ex snic fred 2:09 an074\n\n\n8070.133 small_ex snic fred 2:02 an073\n\n\n8070.135 small_ex snic fred 1:55 an074\n\n\n8070.136 small_ex snic fred 1:41 an073\n\n\n8070.139 small_ex snic fred 1:41 an073\n\n\n8070.140 small_ex snic fred 1:41 an073\n\n\n8070.143 small_ex snic fred 1:41 an073\n\n\n8070.144 small_ex snic fred 1:41 an074\n\n\n8070.147 small_ex snic fred 1:41 an074\n\n\n8070.148 small_ex snic fred 1:41 an074\n\n\n8070.151 small_ex snic fred 1:41 an074\n\n\n8070.155 small_ex snic fred 1:38 an074\n\n\n8070.156 small_ex snic fred 1:35 an074\n\n\n8070.157 small_ex snic fred 1:34 an073\n\n\n8070.158 small_ex snic fred 1:34 an073\n\n\n8070.159 small_ex snic fred 1:34 an073\n\n\n8070.161 small_ex snic fred 1:34 an073\n\n\n8070.164 small_ex snic fred 1:33 an074\n\n\n8070.165 small_ex snic fred 1:33 an074\n\n\n8070.168 small_ex snic fred 1:32 an073\n\n\n8070.170 small_ex snic fred 1:26 an073\n\n\n8070.171 small_ex snic fred 1:12 an073\n\n\n8070.172 small_ex snic fred 1:12 an073\n\n\n8070.175 small_ex snic fred 1:11 an074\n\n\n8070.176 small_ex snic fred 1:11 an074\n\n\n8070.179 small_ex snic fred 1:11 an074\n\n\n8070.184 small_ex snic fred 1:04 an074\n\n\n8070.185 small_ex snic fred 0:42 an073\n\n\n8070.190 small_ex snic fred 0:35 an073\n\n\n8070.193 small_ex snic fred 0:35 an074\n\n\n8070.194 small_ex snic fred 0:13 an073\n\n\n8070.195 small_ex snic fred 0:13 an074\n\n\n\n\nMPI job using 16 tasks per node\n\n\nMost MPI jobs achieve best cost efficiency when deploying 16 tasks per\nnode, that is one task per core. Benchmarking by the Lunarc team showed\nthat these jobs typically require binding to achieve good performance.\nThe binding offered by the OpenMPI library works satisfactory.\n\n\nThe resource request is very easy in this case. Ask for a number of\ncores equivalent to the number of tasks you want to run. We recommend\nusing the --exclusive option to avoid getting unrelated jobs placed on\nthe last node in case the number of cores requested doesn\u2019t divide by\nthe number of cores per node. The following is an example submission\nscript to run the MPI application simula_mpi with 64 tasks on 4 nodes.\nNotice you do not need to specify the node count.\n\n\n#!/bin/sh\n\n\n# requesting the number of cores needed on exclusive nodes\n\n\n#SBATCH -N 4\n\n\n#SBATCH --tasks-per-node=16\n\n\n#SBATCH --exclusive\n\n\n#\n\n\n# job time, change for what your job requires\n\n\n#SBATCH -t 0:30:0\n\n\n#\n\n\n# job name\n\n\n#SBATCH -J simula_n64\n\n\n#\n\n\n# filenames stdout and stderr - customise, include %j\n\n\n#SBATCH -o simula_n64_%j.out\n\n\n#SBATCH -e simula_n64_%j.out\n\n\n# write this script to stdout-file - useful for scripting errors\n\n\ncat \\$0\n\n\n# Example assumes we need the intel runtime and OpenMPI library\n\n\n# customise for the libraries your executable needs\n\n\nmodule add intel/13.0\n\n\nmodule add openmpi/1.6.2/intel/13.0\n\n\n# Copying the executable onto the local disks of the nodes\n\n\nsrun -n \\$SLURM_NNODES -N \\$SLURM_NNODES cp -p simula_mpi \\$SNIC_TMP\n\n\n# Copy the input file onto the headnode - if your MPI program\n\n\n# reads from all tasks, you need to do the above srun construct\n\n\n# again\n\n\ncp -p input.dat \\$SNIC_TMP\n\n\n# change to local disk and start the mpi executable\n\n\ncd \\$SNIC_TMP\n\n\nmpirun -bind-to-core simula_mpi\n\n\n# Copy result files back - example assumes only task zero writes\n\n\n# if in your application result files are written on all nodes\n\n\n# you need to initiate a copy on each node via srun\n\n\ncp -p result.dat \\$SLURM_SUBMIT_DIR\n\n\nThis script assumes you are using up to 2000 MB of memory per task. If\nyou need more, adding the two lines\n\n\n#SBATCH -C mem64GB\n\n\n#SBATCH --mem-per-cpu=4000\n\n\nto the script will allow for using up to 4000 MB. Since fewer nodes are\nequipped with 64 GB of memory, you have to allow for longer queueing\ntimes until resource become available.\n\n\nModifications required for file I/O on all nodes\n\n\nAs discussed in the comments of the sample script, the script assumes\nthat only MPI-task 0 on the head node reads the input file and writes to\nthe output file. If for your MPI application every MPI task reads the\ninput file(s), replace the line\n\n\ncp -p input.dat \\$SNIC_TMP\n\n\nwith\n\n\nsrun -n \\$SLURM_NNODES -N \\$SLURM_NNODES cp -p input.dat \\$SNIC_TMP\n\n\nand the file gets copied onto the local disk of each node. Matters are\nslightly more complex, if your output is written from all tasks. We\nassume the output files can be wild-carded as result_*.dat. Copying\nthese files back to the submission directory can be achieved creating a\nscript, which is placed on all nodes and subsequently executed on all\nnodes. The following addition to the submission script will create the\nscript and place it on all your nodes\n\n\ncat \\\n\\\nEOF > copyfile.sh\n\n\n#!/bin/sh\n\n\ncp -p result*.dat \\$SLURM_SUBMIT_DIR\n\n\nEOF\n\n\nchmod u+x copyfile.sh\n\n\nsrun -n \\$SLURM_NNODES -N \\$SLURM_NNODES cp copyfile.sh \\$SNIC_TMP\n\n\nThis needs inserting into the script before the \u201ccd \\$SNIC_TMP\u201d\nstatement. Once this is in place you can copy your result files by\nreplacing the line\n\n\ncp -p result.dat \\$SLURM_SUBMIT_DIR\n\n\nwith the line\n\n\nsrun -n \\$SLURM_NNODES -N \\$SLURM_NNODES copyfile.sh\n\n\nMPI jobs using fewer than 16 tasks per node\n\n\nIf you want to use fewer than 16 task per nodes to e.g. give more\nresources to the individual task, you can use the -N and --task-per-node\noptions of sbatch. We recommend not to use the -n option in this case.\nThis example is for 4 nodes with 8 tasks each, a total of 32 tasks. In\nour experience, in this case and when using --exclusive it is typically\nadvantageous to not use binding. Though we encourage experimenting with\nyour own application.\n\n\n#!/bin/sh\n\n\n# requesting the number of nodes and cores needed, exclusive nodes\n\n\n#SBATCH -N 4\n\n\n#SBATCH --tasks-per-node=8\n\n\n#SBATCH --mem-per-cpu=8000\n\n\n#SBATCH -C mem64GB\n\n\n#SBATCH --exclusive\n\n\n#\n\n\n# job time, change for what your job requires\n\n\n#SBATCH -t 0:30:0\n\n\n#\n\n\n# job name\n\n\n#SBATCH -J simula_n64\n\n\n#\n\n\n# filenames stdout and stderr - customise, include %j\n\n\n#SBATCH -o simula_n64_%j.out\n\n\n#SBATCH -e simula_n64_%j.out\n\n\n# write this script to stdout-file - useful for scripting errors\n\n\ncat \\$0\n\n\n# Example assumes we need the intel runtime and OpenMPI library\n\n\n# customise for the libraries your executable needs\n\n\nmodule add intel/13.0\n\n\nmodule add openmpi/1.6.2/intel/13.0\n\n\n# Copying the executable onto the local disks of the nodes\n\n\nsrun -n \\$SLURM_NNODES -N \\$SLURM_NNODES cp -p simula_mpi \\$SNIC_TMP\n\n\n# Copy the input file onto the headnode - if your MPI program\n\n\n# reads from all tasks, you need to do the above srun construct\n\n\n# again\n\n\ncp -p input.dat \\$SNIC_TMP\n\n\n# change to local disk and start the mpi executable\n\n\ncd \\$SNIC_TMP\n\n\nmpirun simula_mpi\n\n\n# Copy result files back - example assumes only task zero writes\n\n\n# if in your application result files are written on all nodes\n\n\n# you need to initiate a copy on each node via srun\n\n\ncp -p result.dat \\$SLURM_SUBMIT_DIR\n\n\nOpenMP jobs using shared memory\n\n\nTo run a shared memory code using OpenMP on Alarik, you specify the\nnumber of cores you require using --tasks-per-node option of sbatch. In\nthis case you have to request placement on a single node with the \u201c-N 1\u201d\noption. In this example we call the executable \u201cprocessor_omp\u201d to\nemphasis that this need to be compiled with OpenMP support. Unless you\nare doing something special, you are not required to specify the\nenvironment variable OMP_NUM_THREADS. The example script uses the\ntechniques described for the \nbasic run script\n to\nengage the node local disk.\n\n\n#!/bin/bash\n\n\n#\n\n\n# Specify the number of threads - request all on 1 node\n\n\n#SBATCH -N 1\n\n\n#SBATCH --tasks-per-node=16\n\n\n#\n\n\n# job time, change for what your job requires\n\n\n#SBATCH -t 00:10:00\n\n\n#\n\n\n# job name\n\n\n#SBATCH -J data_process\n\n\n#\n\n\n# filenames stdout and stderr - customise, include %j\n\n\n#SBATCH -o process_omp_%j.out\n\n\n#SBATCH -e process_omp_%j.err\n\n\n# write this script to stdout-file - useful for scripting errors\n\n\ncat \\$0\n\n\n# copy the input data and program to node local disk\n\n\n# customise for your input file(s) and program name\n\n\ncp -p input.dat processor_omp \\$SNIC_TMP\n\n\n# change to the execution directory\n\n\ncd \\$SNIC_TMP\n\n\n# run the program\n\n\n# customise for your program name and add arguments if required\n\n\n./processor_omp\n\n\n# rescue the results to the submission directory\n\n\n# customise for your result file(s)\n\n\ncp -p result.dat \\$SLURM_SUBMIT_DIR\n\n\nThis script allows to use 2000 MB of main memory per requested core. If\nyou need more memory, this can be requested by:\n\n\n#SBATCH -C mem64GB\n\n\n#SBATCH --mem-per-cpu=4000\n\n\nThis will increase you memory request to 4000 MB per requested core.\n\n\nThread binding for OpenMP codes\n\n\nThe Alarik nodes deploy a cache-coherent non-uniform-memory access\narchitecture (cc-numa). Many scientific simulation codes gain\nsignificant performance benefits on a cc-numa architecture when the user\nbinds the threads to physical cores of the hardware. This inhibits\nthread migration and improves memory locality. Unfortunately invoking\nthread binding is not standartised. Depending on the OpenMP runtime\nlibrary the user needs to modify different environment variables to bind\nhis threads.\n\n\nThread binding with the GNU compilers\n\n\nBy default the GNU compiler suite (gcc/gfortran) does not bind threads\nto cores. To engage thread binding, you need to set the environment\nvariable GOMP_CPU_AFFINITY and provide this with a binding list. When\nsetting\n\n\nexport GOMP_CPU_AFFINITY=\u201d0-15\u201d\n\n\nin your submission script, prior to starting your OpenMP application,\nthis will bind the threads to the 16 cores in the node. The above will\nbind thread 0 to core 0, thread 1 to core 1 and so on.\n\n\nMore advanced remark:\n If you want to utilise only 8 cores from a\nnode and asking for exclusive node access (#SBATCH --exclusive), it\nmight be a good idea to place threads on every second core only. This\nwill give you more memory bandwidth and make sure you are utilising all\nFPUs of the Interlagos architecture. This can be achieved by setting:\n\n\nexport GOMP_CPU_AFFINITY=\u201d0-14:2\u201d\n\n\nor\n\n\nexport GOMP_CPU_AFFINITY=\u201d0 2 4 6 8 10 12 14\u201d\n\n\nIt depend on details of your application, whether or not this helps\nperformance. Also note, when asking for a exclusive access to a note,\nyou will be charged for the full node, whether or not you use all cores.\n\n\nImportant pitfall:\n If you set GOMP_CPU_AFFINITY=0 this will bind\nall threads to core 0. You will see extremely poor performance in this\ncase.\n\n\nThread binding with the open64 compiler\n\n\nOpenMP code compiled with the \nopen64\n compiler will use thread\nbinding on Alarik. In standard use cases this will actually boost\nperformance. However in special situation, e.g. when using fewer threads\nthan the size of your partition, you might see a performance boost by\nnot using thread binding. To do so you need to set the environment\nvariable \u201cO64_OMP_SET_AFFINITY=false\u201d\n\n\nThread binding with the Intel compiler\n\n\nVersions 12.1 and 13.0 of the \nIntel\n compiler do not support thread\nbinding when used on the AMD processors deployed on Alarik. Starting\nfrom version 13.1 the Intel compile does support thread binding on the\nAMD processors deployed on Alarik. Obviously all versions of the Intel\ncompiler support thread binding on the Intel processors deployed on\nErik.\n\n\nFor version 13.1 of the Intel compiler thread is controlled by setting\nthe environment variable KMP_AFFINITY. The value\n\n\nexport KMP_AFFINITY=granularity=fine,compact\n\n\nmight be a good starting point for your experimentation.\n\n\nHybrid-jobs using threads within an MPI framework\n\n\nA cluster with multicore nodes such as Alarik is a natural environment\nto execute parallel codes deploying both MPI and OpenMP threads. When\nrunning such applications the optimal number of MPI-tasks and OpenMP\nthreads to place on a node can depend highly on the details of the\napplication. In particular for application which make many references to\nmain memory and the programmer has not implemented a proper \u201cfirst touch\ndata allocation\u201d it is typically \nbest to have 2 or 4 threads\n per MPI\ntask on an Alarik node. Together with a proper binding of your MPI tasks\nto the \u201cnuma-islands\u201d, this will ensure memory locality for your code.\nFor the below syntax you have to use \nversion 1.8.3 or newer\n of the\nOpenMPI library.\n\n\nIn the following we give a simple example script to run a MPI-OpenMP\nhybrid named simul_hyb on 2 nodes using 8 tasks and 4 threads per task.\nThe tasks and their threads will be bound to the \nnuma-islands\n,\nminimising cc-numa effects.\n\n\n#!/bin/sh\n\n\n# requesting number of nodes (-N option)\n\n\n# number of mpi-tasks per node\n\n\n# and number of threads per task exclusive nodes\n\n\n#SBATCH -N 2\n\n\n#SBATCH --tasks-per-node=4\n\n\n#SBATCH --cpus-per-task=4\n\n\n#SBATCH --exclusive\n\n\n# time required\n\n\n#SBATCH -t 01:00:00\n\n\n#SBATCH -J hybrid_run\n\n\n# filenames stdout and stderr - customise, include %j\n\n\n#SBATCH -o simula_N2t4c4_%j.out\n\n\n#SBATCH -e simula_N2t4c4_%j.out\n\n\ncat \\$0\n\n\n# Example assumes we need the intel runtime and OpenMPI library\n\n\n# customise for the libraries your executable needs\n\n\nmodule add intel/15.0\n\n\nmodule add openmpi/1.8.3/intel/15.0\n\n\n# Copying the executable onto the local disks of the nodes\n\n\nsrun -n \\$SLURM_NNODES -N \\$SLURM_NNODES cp -p simul_hyb \\$SNIC_TMP\n\n\n# Copy the input file onto the headnode - if your MPI program\n\n\n# reads from all tasks, you need to do the above srun construct\n\n\n# again\n\n\ncp -p input.dat \\$SNIC_TMP\n\n\ncd \\$SNIC_TMP\n\n\n# setting number of OpenMP threads and ask for thread binding\n\n\nexport OMP_NUM_THREADS=\\$SLURM_CPUS_PER_TASK\n\n\nexport OMP_PROC_BIND=true\n\n\n\n\nmpiexec --map-by\nppr:\\$SLURM_NTASKS_PER_NODE:node:PE=\\$SLURM_CPUS_PER_TASK \\\nsimul_hyb\n\n\n\n\n# Copy result files back - example assumes only task zero writes\n\n\n# if in your application result files are written on all nodes\n\n\n# you need to initiate a copy on each node via srun\n\n\ncp -p result.dat \\$SLURM_SUBMIT_DIR\n\n\nThe example assumes that MPI task 0 is the only task reading and writing\ninput files. If your application reads and writes data on all nodes, you\nneed to study the \nmodifications\n described in the\nMPI section.\n\n\nAs discussed, the above binds the tasks and their threads to the\nnuma-islands of the Alarik architecture. Alariks numa-islands have four\ncores, therefore the script is best used with 2 or four threads per MPI\ntask. This results in one or two MPI tasks per numa islands.\n\n\nThings to try for MPI-OpenMP hybrids with 16 threads per task\n\n\nWhile using more than 4 threads per MPI task on the Alarik system can\nresult in reduced performance due to cc-numa effects, there are\nsituations when using 16 threads per task can be required (e.g. special\nalgorithms or extreme memory requirements per MPI task).\n\n\nWhen running 16 threads per MPI task, that is a single MPI task per\nAlarik node, you might want to experiment with starting your job without\nspecifying binding on mpiexec, that is remove the -bind-to-core, but\nutilise the \nOpenMP thread binding\n techniques\ndescribed in the OpenMP sample section.\n\n\nInteractive access to compute nodes\n\n\nSometimes it is desirable to have an interactive login to the compute\nnodes of the cluster. Extensive code testing is a typical use case.\n\n\nStarting an interactive session\n\n\nTo start an interactive session you need to use the \u201cinteractive\u201d\ncommand. This will request the required resources from the resource pool\nfor you and start the interactive session once the resources are\navailable.\n\n\nUse the following command to start an interactive session asking for 32\ncores lasting 60 minutes\n\n\ninteractive -n 32 -t 60\n\n\nOn Alarik and Eric this will be allocated on multiple nodes, since the\nnodes have only 16 cores available. The interactive session will last\nuntil either the requested time, 60 minutes in the above example, has\nexpired or you manually exit the interactive shell. Your account gets\ncharged with the wall time duration of your interactive session,\nindependent of the amount of computation you do. In the above example,\nif your session lasts until it expires after 60 min, you get charged for\n32 cpu hours. If you terminate your session after 1/2 hour, you would\nget charged 16 cpu hours.\n\n\nThe interactive command supports most command line options of the sbatch\ncommand. Please refer to the man pages of sbatch.\n\n\nModules and environment variables\n\n\nLoaded modules and environment are not always exported properly to your\ninteractive session. Once placed in the interactive session, we\nrecommend users to reload \nall\n the modules they require. That is\ndespite the \u201cmodules list\u201d command claiming they are still loaded.\n\n\nYou also need to check whether environment variables still have the\nrequired values. If the software you are using has a set-up script, you\nmight need to re-run that script.\n\n\nKnown issues with the interactive command\n\n\nNone at the time of writing.", 
            "title": "Batch system"
        }, 
        {
            "location": "/batch_system/#contents", 
            "text": "Contents  SLURM - the batch system on Alarik and\nErik  Job submission  First example for a job\nsubmission  The job script and sbatch  The three parts of a job script  Resource statements for all\njobs  Walltime  Job naming  Specifying a project for users with multiple\nprojects  Specifying memory requirements  Controlling job output  Notification  Job dependencies  Test queue  Extra fat nodes on Alarik  Fat, extra fat and MIC nodes on\nErik  Resource statements for\nmultiprocessor  Terminology around nodes, processors, cores,\ntasks  Outline: Resource requests for multiprocessor\njobs  Exclusive node access  Specifying the number of nodes required for the\njob  Specifying the number of tasks per\nnode  Specifying the number of threads for a shared-memory\njob  Resource statements for hybrid programs using distributed and shared\nmemory  Specifying the number of cores to be required by the\njob  Program execution environment  Job execution environment  Compiler modules  SLURM variables  SNIC variables  Using the node local disks to improve I/O\nperformance  Launching MPI jobs in OpenMPI  Submitting, monitoring and manipulating jobs in\nSLURM  Submitting with sbatch  Starting executables within SLURM with\nsrun  Monitoring with squeue  Terminating jobs with scancel  Example job scripts  Job scripts using the node local disk  Basic run script  Version for codes requiring more memory than 2000\nMB  Running multiple serial jobs within a single job\nsubmission  The master script  The worker script  Monitoring the progress of your multi-job\nsubmission  MPI job using 16 tasks per node  Modifications required for file I/O on all nodes  MPI jobs using fewer than 16 tasks per\nnode  OpenMP jobs using shared memory  Thread binding for OpenMP codes  Thread binding with the GNU\ncompilers  Thread binding with the open64\ncompiler  Thread binding with the Intel\ncompiler  Hybrid-jobs using threads within an MPI\nframework  Things to try for MPI-OpenMP hybrids with 16 threads per\ntask  Interactive access to compute\nnodes  Starting an interactive session  Modules and environment\nvariables  Known issues with the interactive\ncommand", 
            "title": "Contents"
        }, 
        {
            "location": "/batch_system/#slurm-the-batch-system-on-alarik-and-erik", 
            "text": "On a modern HPC system efficient management of the compute resources is\nabsolutely crucial for the system to perform. Alarik and Erik are the\nfirst Lunarc systems to deploy SLURM ( S imple  L inux  U tility\nfor  R esource  M anagement) as resource manager. For your program\nto be executed you have to describe to SLURM the resources required by\nyour program, the name of your program and the command line arguments\nyour program may require. SLURM also allows monitoring and manipulation\nof the progress of your programs execution.  This document contains two key parts. The  first\npart  describes in-depth the job submission system\nand its options. The  second part  gives example\nscripts for the most common use cases. They hopefully serve as a good\nstarting point when creating submission scripts fitting your needs and\nrequirements.", 
            "title": "SLURM - the batch system on Alarik and Erik"
        }, 
        {
            "location": "/batch_system/#job-submission", 
            "text": "", 
            "title": "Job submission"
        }, 
        {
            "location": "/batch_system/#first-example-for-a-job-submission", 
            "text": "The job script and sbatch  You register your program with SLURM for execution using the  sbatch \ncommand. This is done easiest by using a  job description file . The job\ndescription file is also know as a  job script .  A very simple job script, looks as follows:   #!/bin/sh  #SBATCH -t 00:05:00  echo \"hello\"   Write this into a file. In the following we assume the file is named\necho_script.sh, but in principle any name will do. You can now send the\nscript for execution using the sbatch command. This will execute the\n\u201cprogram\u201d echo on the backend.  sbatch echo_script.sh  This should deliver a screen output similar to  [fred@alarik Serial]\\$ sbatch echo_script.sh  Submitted batch job 7185  Where 7185 is the job number assigned by SLURM. Once your job has\nexecuted you will find a file slurm-7185.out in your directory which\ncontains the output and error messages from your program.  The three parts of a job script  The example echo_script.sh shows the three parts every job script\nrequires    Shell specification    Resource statement    Body containing a UNIX script    In our example each part consists of a single line. The first line of\nour example contains the shell specification, in most cases the sh-shell\nas used here is just fine. The second line starting with #SBATCH\nspecifies the resources needed. In our case it asks for 10 minutes of\ncomputer time. If the jobs hasn\u2019t finished after that time, SLURM will\nterminate it. Job scripts typically contain more than one of these\nstatements, specifying e.g. more than one processor or more memory. The\nmost commonly used resource statements at Lunarc will be explained\nbelow. The resource statements are followed by a list of programs and\nUNIX commands to be executed on the system. This is actually a normal\nUNIX script and everything you can do in a UNIX script can be done here\nas well. In our example the script consists out of the UNIX echo\ncommand.", 
            "title": "First example for a job submission"
        }, 
        {
            "location": "/batch_system/#resource-statements-for-all-jobs", 
            "text": "We now describe a number of statements which are most commonly used to\nspecify resource requirements for all kind of jobs. Refer to \u201cman\nsbatch\u201d for more information.  Walltime  The walltime attribute specifies the time requested for completing the\njob. The time is  not  cpu-time but the total time, as measured by a\nnormal clock. In the previous example the time requested was 0 hours 5\nminutes and 0 seconds. Walltime is specified in seconds or using the\nfollowing notation:   Hours:Minutes:Seconds   If your calculation hasn\u2019t finished once the specified time has elapsed,\nSLURM will terminate your job. It is therefore  good practise  to\nspecify a bit more time than you anticipate your job to take. This makes\nsure that you still get your results, even the jobs is slowed by some\ninterference, e.g. waiting for a write to a shared file system to\nfinish. However don\u2019t specify excessive extra time. Due to scheduling\nconstraints, jobs asking for less time will typically spend less time in\nthe queue, waiting for their execution. This also provides safety\nagainst depletion of your allocation. If, e.g., your job hangs, SLURM\nwill terminate your job and the project will be charged less time if the\nwalltime margin is not excessive.  To specify your walltime requirements write a statement like  #SBATCH -t 00:10:00  into your job script.  The maximum walltime for any job on Alarik is 168h, which is the same as\n7 days. On Erik the maximum walltime for any job is 48h.  Job naming  All jobs are given both a job identifier and a name, for easier\nidentification in the batch-system. The default name given to a job is\nthe file name of the submit script, which can make it difficult to\nidentify your job, if you use a standard name for your submit scripts.\nYou can give your job a name from inside the script by using the -J\noption:  #SBATCH -J parameterTest  This will name your job \u201cparameterTest\u201d.  Specifying a project for users with multiple projects  Most users are members of only one project. These users do not need to\nspecify a project in their in their submission script. The Lunarc set-up\nwill automatically use that project for accounting.  A few users are members of more than project. In this case the system\nwould not know which project to charge for the run, so you need to\nspecify the project using the -A option:  #SBATCH -A snic2015-x-xxx  Replace the \u201csnic2015-x-xxx\u201d with the string naming your project. You\ncan inquire the correct string using the projinfo command or the SUPR\nsystem.  Specifying memory requirements  Alarik has 32 GB of memory installed on the small memory nodes and 64 GB\nof memory on the large memory nodes. The default memory request per core\non the system is 2000 MB (a sixteenth of 32GB). If more then 2000 MB per\ncore is needed it has to be requested explictly with the --mem-per-cpu  option of sbatch. In this case you also have to\nrequest allocation on a large memory node using the  -C mem64GB \noption of sbatch. The following show an example how to request 4000 MB\nor main memory per compute core used:  #SBATCH -C mem64GB  #SBATCH --mem-per-cpu=4000  When requesting more than 2000 MB of memory, your jobs may spend a\nlonger time in the queue, waiting for execution, since it needs to wait\nfor run-slot(s) on the large memory nodes to become available. When\nrequesting more then 4000 MB per processing core, your jobs will be\ncharged at a higher rate. In this case some processing cores have to\nremain idle since you are using more than your fair share of memory.  Erik has 64 Gb of memory on the standard nodes. Each node has two CPUs\nwith eight cores each. The default memory request per core is therefore\n4000 MB of memory. As in the case of Alarik, if more than 4000MB of\nmemory per core is needed it has to be described as above.  Controlling job output  By default, the output which your job writes to stdout and stderr is\nwritten to a file named  slurm_%j.out  The %j in the file name will be replaced by the jobnumber SLURM assigns\nto your job. This ensures that the output file from your job is unique\nand different jobs do not interfere with each other's output file.  In many cases the default file name is not convenient. You might want to\nhave a file name which is more descriptive of the job that is actually\nrunning - you might even want to include important meta-data, such as\nphysical parameters, into the output filename(s). This can be achieved\nby using the -o and -e options of sbatch. The -o option specifies the\nfile containing the stdout and the -e option the file containing the\nstderr. It is good practise to include the %j string into the filenames.\nThat will prevent jobs from overwriting each other's output files. The\nfollowing gives an example:  #SBATCH -o calcflow_m1_%j.out  #SBATCH -e calcflow_m1_%j.err  You can give the same filename for both options to get stdout and stderr\nwritten to the same file.  Notification  SLURM on the systems can send you email if the status of your job\nchanges as it progresses through the job queue. To use this feature you\nneed to specify the email address using the --mail-user option and\nspecify the event you want to get notified about using the --mail-type\noption. The following  #SBATCH --mail-user=fred@institute.se  #SBATCH --mail-type=END  Will send an email to the address fred@institute.se once the job has\nended. Valid type values, selecting the event you can get notified\nabout, are BEGIN, END, FAIL, REQUEUE, and ALL (any state change).  Job dependencies  To describe job dependencies, use the -d option of sbatch. This is\nparticularly useful for job dependencies, in workflows.  To illustrate this consider the following example. You require a serial\njob to create a mesh for your simulation. Once this has finished, you\nwant to start a parallel job, which uses the mesh. You first submit the\nmesh creation job using sbatch  [fred@alarik Simcode]\\$ sbatch run_mesh.sh  Submitted batch job 8042  As discussed, sbatch returns you a jobid, 8042 in this example. You use\nthis to declare your dependency when submitting the simulation job to\nthe queue  [fred@alarik Simcode]\\$ sbatch -d afterok:8042 run_sim.sh  Submitted batch job 8043  When using squeue to monitor job 8043, this should now be in status\npending (PD) with the reason of dependency. Another common use case for\nthis functionality is a simulation requiring many days of computer times\nbeing split into a number of submissions.  Test queue  To run short tests, it is possible to request extra high priority on\nAlarik with the help of  #SBATCH --qos=test  For one such job, the maximum walltime is 1 h and the maximum number of\nnodes is two and a user is only allowed to run two such jobs\nsimultaneously. A system of floating reservations is used to free two\nnodes every second hour between 8.00 and 20.00 to reduce the queue time\nfor test jobs. The way it works also means that the shorter the test\njob, the more likely it is to start sooner rather than later. It is not\nallowed to use qos=test for series of production runs.  On Erik there is one two-GPU node reserved for tests in a partition of\nits own, which is specified with  #SBATCH -p test  Like on Alarik, the maximum walltime is 1 h.  Extra fat nodes on Alarik  Alarik has four nodes with 48 cores and 128 GB memory. To access them,\nthe partition extra has to be specified  #SBATCH -p extra  Furthermore, the amount of memory per requested core also needs to be\ngiven.  #SBATCH --mem-per-cpu=\\ memory in MB>  Otherwise, the default value of 2 000 MB will be set at and if more is\nused, slurm can kill the job. For example, to run on the 48 cores of a\nsingle node and use a total of 128 000 MB of memory (for more on\nmultiprocessor statements, see the next section):  #SBATCH -N 1  #SBATCH --tasks-per-node=48  #SBATCH --mem-per-cpu=3000  #SBATCH -p extra  Fat, extra fat and MIC nodes on Erik  Erik has 7 nodes with 4 GPUs (and 96 GB of memory). To access them, the\npartition fat has to be specified  #SBATCH -p fat  One node is equipped with 8 GPUs (and 96 GB of memory), which is in\npartition extra  #SBATCH -p extra  There is also one node with two Xeon Phi (MIC) cards in the partition\nmic  #SBATCH -p mic  There is also one node with two Nvidia K80 cards in the partition new  #SBATCH -p new  If no -p option is specified, normal nodes with two Nvidia K20 cards\nwill be allocated to the job.", 
            "title": "Resource statements for all jobs"
        }, 
        {
            "location": "/batch_system/#resource-statements-for-multiprocessor", 
            "text": "In HPC it is very common to have many processing elements working on a\njob. The extra processing power can be utilised to process large\nproblems beyond the capabilities of a single processing element. It can\nalso be used to swiftly perform a number of calculations within a single\njob submission.  Terminology around nodes, processors, cores, tasks  There is a a lot of structure within modern HPC equipment. For the\npurposes of this user guide we will stick to the following terminology:   Term      Explanation                                                                                                                                                                                                                                                               Number on Alarik            Number on Erik   Node        A physical computer                                                                                                                                                                                                                                                          Standard :                  Standard :                                                                                                                                                                                                                                                                                        200                           16\n\n                                                                                                                                                                                                                                                                                      **Extra**:                    **Fat**:\n\n                                                                                                                                                                                                                                                                                      4                             7\n\n                                                                                                                                                                                                                                                                                                                    **Extra**:\n\n                                                                                                                                                                                                                                                                                                                    1\n\n                                                                                                                                                                                                                                                                                                                    **Mic:**\n\n                                                                                                                                                                                                                                                                                                                    1\n\n                                                                                                                                                                                                                                                                                                                    **New:**\n\n                                                                                                                                                                                                                                                                                                                    1  Processor   This denotes a the multi-core processor, housing many processing elements                                                                                                                                                                                                    Standard :                 2 per node                                                                                                                                                                                                                                                                                        2 per node\n\n                                                                                                                                                                                                                                                                                      **Extra**:\n\n                                                                                                                                                                                                                                                                                      4 per node  GPU         This denotes a nvidia co-processor                                                                                                                                                                                                                                          0                              Standard :                                                                                                                                                                                                                                                                                                                      2 per node\n\n                                                                                                                                                                                                                                                                                                                    **Fat**:\n\n                                                                                                                                                                                                                                                                                                                    4 per node\n\n                                                                                                                                                                                                                                                                                                                    **Extra**:\n\n                                                                                                                                                                                                                                                                                                                    8 per node\n\n                                                                                                                                                                                                                                                                                                                    **Mic:**\n\n                                                                                                                                                                                                                                                                                                                    0 per node\n\n                                                                                                                                                                                                                                                                                                                    **New:**\n\n                                                                                                                                                                                                                                                                                                                    2 cards per node\n\n                                                                                                                                                                                                                                                                                                                    2 logical per card\n\n                                                                                                                                                                                                                                                                                                                    4 logical per node  Socket      This is the \u201cplug\u201d the processor gets plugged into. Used as a synonym for the processor                                                                                                                                                                                      Standard :                 2 per node                                                                                                                                                                                                                                                                                        2 per node\n\n                                                                                                                                                                                                                                                                                      **Extra**:\n\n                                                                                                                                                                                                                                                                                      4 per node  Core        Individual processing element                                                                                                                                                                                                                                                Standard :                 16 per node                                                                                                                                                                                                                                                                                        16 per node                   8 per processor\n\n                                                                                                                                                                                                                                                                                      8 per processor\n\n                                                                                                                                                                                                                                                                                      **Extra**:\n\n                                                                                                                                                                                                                                                                                      48 per node\n\n                                                                                                                                                                                                                                                                                      12 per processor  Task        This is a software concept. It denotes a process, which is an instance of a running program. It has its own data and instruction stream(s). It can fork multiple threads to increase the computational speed. Serial programs and pure MPI programs do not spawn threads.   User controls in job script   User controls in job script  Thread      This is also a software concept. A thread is a stream of instructions executed on the hardware. It is part of a task and shares resources such as data with other threads within the same task.                                                                             User controls in job script   User controls in job script   Outline: Resource requests for multiprocessor jobs  When running multi processor jobs on the Lunarc clusters, one should\nspecify:    The number of nodes required by the jobs    The number of computational tasks per node    The number of threads spawned by each task    For a pure MPI job or when processing a large number of serial jobs in a\nso called task farm, one will typically only specify the items 1 and 2,\nwhile for a threaded job, using e.g. OpenMP or Java, one will typically\nonly specify items 1 and 3.  It is typically not advisable to have the product of items 2 and 3\nexceeding the number of cores per node, which is 16 for standard Alarik\nand Erik compute nodes. On the Alarik extra compute nodes this number is\n48. In most cases users requesting multiple nodes will want the product\nto equal the number of cores per node. The syntax how to control nodes,\ntasks per node and threads per task is explaned below.  Exclusive node access  For parallel codes using MPI or OpenMP it is typically best to keep\ninterference on the nodes at a minimum, that is to have exclusive access\nto the nodes you are using. This also applies to specialist and\nexperimental work, which would interfere very badly with other user\u2019s\ncodes on the nodes. Adding   #SBATCH --exclusive   to your job script will ensure that SLURM will allocate dedicated nodes\nto your job. Obviously your project gets charged for the full costs of\nthe nodes you are using, that is in case of Alarik and Erik 16 cores per\nnode.  Specifying the number of nodes required for the job  In SLURM one requests the number of nodes for a job with the  -N \noption. The following statement requests four nodes for your job:  #SBATCH -N 4  Important:  without using either the --tasks-per-node or\nthe --cpus-per-task options of sbatch, this will reserve a single core\nper node, so four in total, which is most likely not what you want.  Specifying the number of tasks per node  Use the --tasks-per-node of sbatch to specify the number of tasks you\nrequire per node. Most multinode job will set this equal to the number\nof cores availble per node. The following example asks for 16 task per\nnode:  #SBATCH --tasks-per-node=16  This should be used together with the -N option, specifying the number\nof nodes to be used. The default value for the number of tasks per node\nis 1. For example to specify the requirements for an MPI job with 64\ntasks or multiprocessor job using 64 processors to process a larger\nnumber of serial jobs one would specify  #SBATCH -N 4  #SBATCH --tasks-per-node=16  When using fewer than 16 tasks per node and you want to prevent other\nuser\u2019s jobs sharing your node, you need to consider using\nthe --exclusive option. If --exclusive is not specified, SLURM might\nplace other tasks onto your node.  Specifying the number of threads for a shared-memory job  If you want to run shared-memory applications using threads, e.g. OpenMP\nparallised code or Java applications, you need to specify the number of\nthreads you require per task. This can be done with the --tasks-per-node\noption of sbatch.  For a standard shared-memory program, which doesn\u2019t also use distributed\nmemory programming models such as MPI, one is restricted to a single\nnode. On that node, one can request as many threads as there are cores\non the node. On the standard Alarik compute nodes one can efficiently\nuse up to 16 threads. Use the following resource statement:  #SBATCH -N 1  #SBATCH --tasks-per-node=16  If your program is only efficient at a lower thread count, you may want\nto use e.g.:  #SBATCH -N 1  #SBATCH --tasks-per-node=4  if you only want to use four threads. The Alarik extra nodes with 48\ncores allow for very wide shared-memory jobs:  #SBATCH -N 1  #SBATCH --tasks-per-node=48  #SBATCH --mem-per-cpu=3000  #SBATCH -p extra  Resource statements for hybrid programs using distributed and shared memory  So-called hybrid programs, using both distributed and shared-memory\ntechniques have recently become popular. For example: for a program\nusing 32 MPI tasks, each task spawning 2 OpenMP threads one would\nrequire 4 nodes and place eight tasks on each node. The number of\nthreads per task is given by --cpus-per-task. The resource statement\nwould look as follows:  #SBATCH -N 4  #SBATCH --tasks-per-node=8  #SBATCH --cpus-per-task=2  Specifying the number of cores to be required by the job  In special cases, such as using very unusal numbers of tasks, the  -n \noption of sbatch to specify the number of cores might become useful.\nWhen running a pure MPI program this option corresponds to the  number\nof tasks  required for your program. The following statement in a job\nscript would reserve 63 cores for your job  #SBATCH -N 4  #SBATCH --tasks-per-node=16  #SBATCH -n 63  Please consider using the --exclusive option of sbatch to avoid SLURM\nspreading your job on more nodes than necessary and placing other user\u2019s\njobs on nodes utilising fewer than 16 cores for your job. Other user\u2019s\njobs could via shared node resources (memory bus, cache, FPU, \u2026)\ninterfere with your job and introduce undue operational noise. Such\nnoise is something parallel program execution can be extremely sensitive\nto.", 
            "title": "Resource statements for multiprocessor"
        }, 
        {
            "location": "/batch_system/#program-execution-environment", 
            "text": "Job execution environment  When submitting your job to SLURM using sbatch, your entire environment\nincluding the currently loaded modules gets copied. This behaviour is\ndifferent from earlier Lunarc machines, including Platon. On Alarik,\nwhen hitting sbatch:   Make sure that the loaded modules and any environment variable you\n      may have set will not be in conflict with the environment expected\n      by the job script   Compiler modules  On Alarik we automatically load a modern version of the GCC compiler,\nwhich supports the deployed AMD Opteron processors. At the time of\nwriting this is version 4.6.2 of GCC. If you prefer using a different\ncompiler, you can add the desired module, e.g., version 12.1 of the\nIntel compiler  module add intel/12.1  If different modules have files with the same names in the search path,\nthose of the module added last will be picked. Generally this is not a\nproblem, but the compiler wrappers in the openmpi modules have the same\nnames and it safest to only have one loaded at a time.  On Erik the same compilers as on Alarik are present. Note that the\nprocessors on Erik are of the Intel Xeon type and thus utilize the mkl\nas supplied.  SLURM variables  To come  SNIC variables  The SNIC meta-centres have agreed on a set of environment variables\nwhich should improve the portability of (parts of) job-scripts between\nSNIC sites. On Alarik the following variables are set by the system:   Environment variable     Explanation                                                                                 Value on Alarik     Value on Erik   SNIC_SITE                 Identifying the SNIC site you are using                                                       lunarc                lunarc  SNIC_RESOURCE             Identifying the compute resource you are using                                                alarik                erik  SNIC_BACKUP               User directory which is:                                                                      /home/\\ user>        /home/\\ user>                             Regularly backed up against accidental deletion\n\n                           Typically extremely limited space\n\n                           Use for e.g. precious source code  SNIC_NOBACKUP             User directory which is:                                                                      /lunarc               /lunarc                             Accessible on all Lunarc systems                                                            /nobackup             /nobackup\n\n                           Outliving individual systems                                                                /users/\\ user\\        /users/\\ user\\ \n\n                           For storing larger amounts of data\n\n                           Not backed up against accidental deletion\n\n                           Protected against disk failure (RAID configuration)\n\n                           On Alarik: the primary root directory for job management (job scripts, input/output data)  SNIC_TMP                  Directory for best performance during a job                                                    jobid dependent       jobid dependent                           At Lunarc:\n\n                           Local disk on nodes\n\n                           Storing temporary data during job execution\n\n                           High bandwidth\n\n                           Automatically deleted\n\n                           Transfer data with long-term value to SNIC\\_NOBACKUP before job has finished", 
            "title": "Program execution environment"
        }, 
        {
            "location": "/batch_system/#using-the-node-local-disks-to-improve-io-performance", 
            "text": "On Alarik and Erik, all nodes have a local disk. This disk offers\nsuperior bandwidth when compared to accessing your home space or the\n/lunarc/nobackup centre storage. In particular when files are read or\nwritten repeatedly during execution it is advisable to copy the input\ndata onto the local disk prior to job execution and copy the result\nfiles back to the submission directory once your program has finished.\nDuring its execution, your program would then read and write to local\ndisk.  In case of Alarik and Erik, the submission directory typically resides\non the /lunarc/nobackup centre storage. All data left on the node local\ndisks  will be deleted  when your job has finished. You need to copy\neverything of interest to a more permanent storage space such as\n/lunarc/nobackup. If a job is terminated prematurely, for example, if it\nexceeds the requested walltime, the files on the local disk (in\n\\$SNIC_TMP) will be lost.  Files that would still be useful can be listed in a special file \\$SNIC_TMP/slurm_save_files . Filenames are assumed to be relative\nto \\$SNIC_TMP and should be separated by spaces or listed on separate\nlines. Wildcards are allowed. These files will be copied from the local\ndisk where \\$SNIC_TMP/slurm_save_files exists to the submission\ndirectory regardless whether the job ends as planned or is deleted,\nunless there is a problem with the disk or node itself. Note that the\nslurm_save_files feature is unique to Lunarc.  For the required UNIX scripting you should use the following environment\nvariables. Example scripts using this technique are provided in the\nexample section of this document. Contact the help desk if you have\nspecific requirements and require consultation.   Variable           Addressed Volume   SNIC_TMP            node local disk                     copy your input data here and start your program from here  TMPDIR               node local disk                     Many applications use this environment variable to locate a disk volume for temporary scratch space. If your application follows that convention nothing needs to be done.  SLURM_SUBMIT_DIR   submission directory                     where you ran sbatch", 
            "title": "Using the node local disks to improve I/O performance"
        }, 
        {
            "location": "/batch_system/#launching-mpi-jobs-in-openmpi", 
            "text": "To execute message passing parallel jobs these should be built against\none of the MPI libraries provided by the support team as a module. To\nexecute an MPI job, your job script should do the following    Load MPI module relevant for the compiler you are using    Start the program with mpirun    On Alarik the correct binding is crucial to achieve good\n      performance. When using 16 task per node, we recommend using\n      the -bind-to-core option of mpirun. When using fewer than 16 tasks\n      we recommend experimenting whether not using binding helps or\n      hinders performance.", 
            "title": "Launching MPI jobs in OpenMPI"
        }, 
        {
            "location": "/batch_system/#submitting-monitoring-and-manipulating-jobs-in-slurm", 
            "text": "Submitting with sbatch  One uses the command sbatch to submit a job script to the batch system\nfor execution. SLURM will reply with the jobid number. The job will then\nbe held in the queue until the requested resources become available. A\ntypical use case looks as follows:   [fred@alarik MPItest]\\$ sbatch runjob.sh  Submitted batch job 7197   User fred submitted the script runjob.sh to the job queue and got the\njobid 7197 assigned to it.  Starting executables within SLURM with srun  The command srun allows to start executables in a way managed by SLURM.\nThis is particularly effective if you want to process a large number of\njobs within a single submission to the batch system. A use case of srun\nto start many serial jobs in a single multicore submission scipt is\ndiscussed in the  example section .  Monitoring with squeue  The command squeue will show you the current state of the job queue. The\nstandard output, created by calling squeue without any options looks as\nfollows:  JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)  7303 snic hybrid_n fred PD 0:00 32 (Priority)  7302 snic hybrid_n fred PD 0:00 32 (Priority)  7301 snic hybrid_n fred PD 0:00 32 (Resources)  7304 snic preproce karl PD 0:00 6 (Priority)  7300 snic hybrid_n fred R 0:24 32 an[001-032]  7305 snic preproce karl R 0:37 6 an[081-086]  7306 snic hybrid_n fred R 0:37 6 an[081-086]  7307 snic testsimu sven R 0:07 1 an081  The first column gives the jobid, the third the job names, followed by\nthe userid. The column labeled \u201cST\u201d gives the job state. The most\nimportant states are:   Symbol     Meaning    R          running   PD           pending, awaiting resources  CG           completing   The state column is followed by the time used by the job and number of\nnodes utilised by the job. For running jobs the last column gives the\nnames of the nodes utilised or if the job is waiting a reason why it is\nnot executing.  The squeue command is highly configurable. Useful options include -u\nmyid, which lists all jobs of the user myid and also the --start option.\nThe latter gives the current estimate of when SLURM expects the job to\nstart. Note, that this can shift in either direction, depending on e.g.\njobs finishing earlier than specified or jobs with higher priority\ngetting added to the job queue.  The command jobinfo is a script that sorts the output of squeue into\nrunning and waiting jobs. It also shows additional information, such as\nhow long running jobs have left and in some cases when waiting jobs are\nexpected to start.  Terminating jobs with scancel  It is frequently required to remove jobs from the queue. This might be\nthat you discover a problem in your job specification or intermediate\nresults of running job indicating that something went wrong. Use scancel\nto remove a job from the job queue. To do so you need the jobid, which\nis best queried with the squeue command. To remove the job with the\njobid 7103 from the job queue type  scancel 7103", 
            "title": "Submitting, monitoring and manipulating jobs in SLURM"
        }, 
        {
            "location": "/batch_system/#example-job-scripts", 
            "text": "In this section we provide sample scripts for typical use cases.", 
            "title": "Example job scripts"
        }, 
        {
            "location": "/batch_system/#job-scripts-using-the-node-local-disk", 
            "text": "Basic run script  As discussed the node local disk provides better I/O-bandwidth than the\nother file systems available on Alarik. The following script assumes the\nprogram processor reads the file  input.dat  and produces a file result.dat .  This example executes a single serial program and is suitable for the\noccasional serial job. If you need to process a large number of serial\njobs, we request you bundle them into a single submission. Refer to the\nsection \u201c R unning multiple serial jobs within a\nsingle job submission \u201d for a scripting example.  The script copies the input data and the program executable from the\nsubmission directory to the node local disk, executes the program on the\nnode local disk and copies the result file back to the submission\ndirectory for safe keeping. The individual steps are highlighted by\ncomments starting with a \u201c#\u201d. These comment lines can be kept in the\nfile.  This is the Lunarc standard example and represents  recommended\npractise  for a basic serial job. You need to customise the file to\nsuit your specific needs. The script is suitable for jobs consuming no\nmore than 2000 MB of main memory.  #!/bin/bash  #  # job time, change for what your job requires  #SBATCH -t 00:10:00  #  # job name  #SBATCH -J data_process  #  # filenames stdout and stderr - customise, include %j  #SBATCH -o process_%j.out  #SBATCH -e process_%j.err  # write this script to stdout-file - useful for scripting errors  cat \\$0  # copy the input data and program to node local disk  # customise for your input file(s) and program name  cp -p input.dat processor \\$SNIC_TMP  # change to the execution directory  cd \\$SNIC_TMP  # run the program  # customise for your program name and add arguments if required  ./processor  # rescue the results to the submission directory  # customise for your result file(s)  cp -p result.dat \\$SLURM_SUBMIT_DIR  We recommend to be selective about the files you copy between the\nsubmission directory and the local node disk. If you have multiple input\nand result files you need to modify the copy statements accordingly. The\nabove example assumes your program has been compiled with the GCC\ncompiler loaded by default. If it has been compiled with a different\ncompiler you need to load the compiler module by adding a line similar\nto  module add intel/12.1  If you are running on Erik, it is necessary to add the support for the\nGPU with the line  module add cuda  prior to the line ./processor. You need to consult with the person who\nbuild the executable for you. Lunarc provided modules typically complain\nif the wrong compiler is loaded and are hence self-documenting.  Version for codes requiring more memory than 2000 MB  If your program requires more memory than 2000 MB, use the following\nscript. This example is set up to use 4000 MB. If you need even more you\ncan request this, but your runs will be charged to your project at a\nhigher rate, since other cores have to remain idle. The comments on the\nprevious example also apply here  #!/bin/bash  #  # job time, change for what your job requires  #SBATCH -t 00:10:00  #  # job name  #SBATCH -J data_process  #  # filenames stdout and stderr - customise, include %j  #SBATCH -o process_%j.out  #SBATCH -e process_%j.err  #  # requesting a large memory node and 4000 MB or main memory  #SBATCH -C mem64GB  #SBATCH --mem-per-cpu=4000  # write this script to stdout-file - useful for scripting errors  cat \\$0  # copy the input data and program to node local disk  # customise for your input file(s) and program name  cp -p input.dat processor \\$SNIC_TMP  # change to the execution directory  cd \\$SNIC_TMP  # run the program  # customise for your program name and add arguments if required  ./processor  # rescue the results to the submission directory  # customise for your result file(s)  cp -p result.dat \\$SLURM_SUBMIT_DIR  Since fewer nodes are equipped with 64 GB of memory, you have to allow\nfor longer queueing times until resource become available.", 
            "title": "Job scripts using the node local disk"
        }, 
        {
            "location": "/batch_system/#running-multiple-serial-jobs-within-a-single-job-submission", 
            "text": "When you need to run many serial jobs, similar to the ones  described\nabove , these should be bundled together and\nsubmitted to the job queue in a small number of submissions or even a\nsingle submission. With SLURM is perfectly reasonable to run several\nhundred individual jobs in a single submission. To speed up the\nprocessing of your jobs, you can ask for the cores from a number of\nnodes. The concept is known as a  task-farm . The individual job are\nknown as  job-steps .  The following is an example processing 200 such jobs using 16 cores from\na single node. The scripting use two scripts, the master script and the\nworker script. The Master script requests the resources (number of\ncores, job time, ...) and then registers 200 copies of the worker script\nwith SLURM using the command srun. The worker script is a modification\nof the  basic script  described above.  In our example this will then start sixteen jobs on the sixteen cores\nyou requested. Once a job has finished, it will take an unprocessed job\nand place it on the idle core for processing. This will continue until\nall jobs are processed. The ordering of the jobs can not be relied on.  For our example the entire setup assumes the submission directory has\n200 sub-directories, named job_0, job_1, job_2, \u2026, job_199. Each of\nthe directories contains the input data and the program executable to be\nrun.  Keep the number of jobs-steps at a reasonable level. Recent testing by\nthe Lunarc support team has shown that, when including a sleep statement\ninside the do loop the setup can be used to processes 800 jobs.  The master script  The master script describes the resources required and registers, once\nrunning the worker tasks with SLURM. In most cases modifying the number\nof cores needed, the total job time and the number of jobs to be\nprocessed should be all that is required.  #!/bin/sh  # requesting the number of cores needed  #SBATCH -N 1  #SBATCH --tasks-per-node=16  #SBATCH --exclusive  #  # job time, change for what your job farm requires  #SBATCH -t 20:00:00  #  # job name and output file names  #SBATCH -J jobFarm  #SBATCH -o res_jobFarm_%j.out  #SBATCH -e res_jobFarm_%j.out  cat \\$0  # set the number of jobs - change for your requirements  export NB_of_jobs=200  # Loop over the job number  for ((i=0; i\\ \\$NB_of_jobs; i++))  do  srun -Q --exclusive -n 1 -N 1 \\  workScript.sh \\$i  > worker_\\${SLURM_JOB_ID}_\\${i}   sleep 1   done   # keep the wait statement, it is important!  wait  The script assumes that the job is described in a script file\n\u201cworkScript.sh\u201d, which takes a single number identifying the job\ndirectory to be accessed as a command line argument. Please note the\n\u201csleep 1\u201d command inside the do loop. In our testing this greatly\nenhances the stability by submitting the actual jobs over a longer\nperiod of time. With this statement included the script was able to\nsuccessfully handle up to about 800 outstanding jobs on 16 and 32 cores.\nFor reasons of job reliability, we therefore recommend not to process\nmore than 800 jobs in a single script. However it is possible to process\nsignificantly larger job numbers than 800 by carefully tuning sleep-time\nand core count in relation to the average job-time.  Remarks:  When using srun inside a batch script many srun-options act\ndifferently compared to using srun within a different environment.\nConsult the man-page of srun for documentation and contact the Lunarc\nhelp desk if your require further consultancy.  The worker script  This outlines the worker script. Compared to the script describing a single serial job , a few modifications are\nrequired:    To avoid access conflicts between the individual jobs, each job\n      creates a job private sub-directory on the node local disk.    The input file(s) are expected in the sub_directories job_0,\n      job_1, job_2, \u2026 of the submission directory. The result file(s)\n      will also be placed in these directories.    The example assumes a single input file and single result file. If\n      you have multiple input and/or result files modifications are\n      needed, as are modifications for that actual names of your file    The present set up allows for different executables for each\n      job-stop. The script assumes to find an executable named\n      \u201cprocessor\u201d in the same location as the input file(s). If you all\n      job steps use the same executable the scripts can be simplified.    Once a job-step has finished and the result file has been copied\n      back, the job private sub-directory on the node local disk is\n      removed to prevent the disc from overflow.    If you are using the above master script, the script should be named\n\u201cworkScript.sh\u201d.  #!/bin/sh  # document this script to stdout (assumes redirection from caller)  cat \\$0  # receive my worker number  export WRK_NB=\\$1  # create worker-private subdirectory in \\$SNIC_TMP  export WRK_DIR=\\$SNIC_TMP/WRK_\\${WRK_NB}  mkdir \\$WRK_DIR  # create a variable to address the \"job directory\"  export JOB_DIR=\\$SLURM_SUBMIT_DIR/job_\\${WRK_NB}  # now copy the input data and program from there  cd \\$JOB_DIR  cp -p input.dat processor \\$WRK_DIR  # change to the execution directory  cd \\$WRK_DIR  # run the program  ./processor  # rescue the results back to job directory  cp -p result.dat \\${JOB_DIR}  # clean up the local disk and remove the worker-private directory  cd \\$SNIC_TMP  rm -rf WRK_\\${WRK_NB}  Monitoring the progress of your multi-job submission  Using the -s option of sbatch you can monitor the progression of the\nindividual job-steps of your multi-job submission. Please keep in mind,\nthat the step number SLURM assigns to your job and the one you assign\ntypically differs from the loop index used in the master script.  The below is an output from squeue when running a script processing 500\njobs on 32 cores. The jobid of the job is 8070. The output shows the\njob-steps the script is presently processing  [fred@alarik MultiSerialTest]\\$ squeue -j 8070 -s   STEPID NAME PARTITION USER TIME NODELIST  8070.130 small_ex snic fred 2:09 an074  8070.133 small_ex snic fred 2:02 an073  8070.135 small_ex snic fred 1:55 an074  8070.136 small_ex snic fred 1:41 an073  8070.139 small_ex snic fred 1:41 an073  8070.140 small_ex snic fred 1:41 an073  8070.143 small_ex snic fred 1:41 an073  8070.144 small_ex snic fred 1:41 an074  8070.147 small_ex snic fred 1:41 an074  8070.148 small_ex snic fred 1:41 an074  8070.151 small_ex snic fred 1:41 an074  8070.155 small_ex snic fred 1:38 an074  8070.156 small_ex snic fred 1:35 an074  8070.157 small_ex snic fred 1:34 an073  8070.158 small_ex snic fred 1:34 an073  8070.159 small_ex snic fred 1:34 an073  8070.161 small_ex snic fred 1:34 an073  8070.164 small_ex snic fred 1:33 an074  8070.165 small_ex snic fred 1:33 an074  8070.168 small_ex snic fred 1:32 an073  8070.170 small_ex snic fred 1:26 an073  8070.171 small_ex snic fred 1:12 an073  8070.172 small_ex snic fred 1:12 an073  8070.175 small_ex snic fred 1:11 an074  8070.176 small_ex snic fred 1:11 an074  8070.179 small_ex snic fred 1:11 an074  8070.184 small_ex snic fred 1:04 an074  8070.185 small_ex snic fred 0:42 an073  8070.190 small_ex snic fred 0:35 an073  8070.193 small_ex snic fred 0:35 an074  8070.194 small_ex snic fred 0:13 an073  8070.195 small_ex snic fred 0:13 an074", 
            "title": "Running multiple serial jobs within a single job submission"
        }, 
        {
            "location": "/batch_system/#mpi-job-using-16-tasks-per-node", 
            "text": "Most MPI jobs achieve best cost efficiency when deploying 16 tasks per\nnode, that is one task per core. Benchmarking by the Lunarc team showed\nthat these jobs typically require binding to achieve good performance.\nThe binding offered by the OpenMPI library works satisfactory.  The resource request is very easy in this case. Ask for a number of\ncores equivalent to the number of tasks you want to run. We recommend\nusing the --exclusive option to avoid getting unrelated jobs placed on\nthe last node in case the number of cores requested doesn\u2019t divide by\nthe number of cores per node. The following is an example submission\nscript to run the MPI application simula_mpi with 64 tasks on 4 nodes.\nNotice you do not need to specify the node count.  #!/bin/sh  # requesting the number of cores needed on exclusive nodes  #SBATCH -N 4  #SBATCH --tasks-per-node=16  #SBATCH --exclusive  #  # job time, change for what your job requires  #SBATCH -t 0:30:0  #  # job name  #SBATCH -J simula_n64  #  # filenames stdout and stderr - customise, include %j  #SBATCH -o simula_n64_%j.out  #SBATCH -e simula_n64_%j.out  # write this script to stdout-file - useful for scripting errors  cat \\$0  # Example assumes we need the intel runtime and OpenMPI library  # customise for the libraries your executable needs  module add intel/13.0  module add openmpi/1.6.2/intel/13.0  # Copying the executable onto the local disks of the nodes  srun -n \\$SLURM_NNODES -N \\$SLURM_NNODES cp -p simula_mpi \\$SNIC_TMP  # Copy the input file onto the headnode - if your MPI program  # reads from all tasks, you need to do the above srun construct  # again  cp -p input.dat \\$SNIC_TMP  # change to local disk and start the mpi executable  cd \\$SNIC_TMP  mpirun -bind-to-core simula_mpi  # Copy result files back - example assumes only task zero writes  # if in your application result files are written on all nodes  # you need to initiate a copy on each node via srun  cp -p result.dat \\$SLURM_SUBMIT_DIR  This script assumes you are using up to 2000 MB of memory per task. If\nyou need more, adding the two lines  #SBATCH -C mem64GB  #SBATCH --mem-per-cpu=4000  to the script will allow for using up to 4000 MB. Since fewer nodes are\nequipped with 64 GB of memory, you have to allow for longer queueing\ntimes until resource become available.  Modifications required for file I/O on all nodes  As discussed in the comments of the sample script, the script assumes\nthat only MPI-task 0 on the head node reads the input file and writes to\nthe output file. If for your MPI application every MPI task reads the\ninput file(s), replace the line  cp -p input.dat \\$SNIC_TMP  with  srun -n \\$SLURM_NNODES -N \\$SLURM_NNODES cp -p input.dat \\$SNIC_TMP  and the file gets copied onto the local disk of each node. Matters are\nslightly more complex, if your output is written from all tasks. We\nassume the output files can be wild-carded as result_*.dat. Copying\nthese files back to the submission directory can be achieved creating a\nscript, which is placed on all nodes and subsequently executed on all\nnodes. The following addition to the submission script will create the\nscript and place it on all your nodes  cat \\ \\ EOF > copyfile.sh  #!/bin/sh  cp -p result*.dat \\$SLURM_SUBMIT_DIR  EOF  chmod u+x copyfile.sh  srun -n \\$SLURM_NNODES -N \\$SLURM_NNODES cp copyfile.sh \\$SNIC_TMP  This needs inserting into the script before the \u201ccd \\$SNIC_TMP\u201d\nstatement. Once this is in place you can copy your result files by\nreplacing the line  cp -p result.dat \\$SLURM_SUBMIT_DIR  with the line  srun -n \\$SLURM_NNODES -N \\$SLURM_NNODES copyfile.sh", 
            "title": "MPI job using 16 tasks per node"
        }, 
        {
            "location": "/batch_system/#mpi-jobs-using-fewer-than-16-tasks-per-node", 
            "text": "If you want to use fewer than 16 task per nodes to e.g. give more\nresources to the individual task, you can use the -N and --task-per-node\noptions of sbatch. We recommend not to use the -n option in this case.\nThis example is for 4 nodes with 8 tasks each, a total of 32 tasks. In\nour experience, in this case and when using --exclusive it is typically\nadvantageous to not use binding. Though we encourage experimenting with\nyour own application.  #!/bin/sh  # requesting the number of nodes and cores needed, exclusive nodes  #SBATCH -N 4  #SBATCH --tasks-per-node=8  #SBATCH --mem-per-cpu=8000  #SBATCH -C mem64GB  #SBATCH --exclusive  #  # job time, change for what your job requires  #SBATCH -t 0:30:0  #  # job name  #SBATCH -J simula_n64  #  # filenames stdout and stderr - customise, include %j  #SBATCH -o simula_n64_%j.out  #SBATCH -e simula_n64_%j.out  # write this script to stdout-file - useful for scripting errors  cat \\$0  # Example assumes we need the intel runtime and OpenMPI library  # customise for the libraries your executable needs  module add intel/13.0  module add openmpi/1.6.2/intel/13.0  # Copying the executable onto the local disks of the nodes  srun -n \\$SLURM_NNODES -N \\$SLURM_NNODES cp -p simula_mpi \\$SNIC_TMP  # Copy the input file onto the headnode - if your MPI program  # reads from all tasks, you need to do the above srun construct  # again  cp -p input.dat \\$SNIC_TMP  # change to local disk and start the mpi executable  cd \\$SNIC_TMP  mpirun simula_mpi  # Copy result files back - example assumes only task zero writes  # if in your application result files are written on all nodes  # you need to initiate a copy on each node via srun  cp -p result.dat \\$SLURM_SUBMIT_DIR", 
            "title": "MPI jobs using fewer than 16 tasks per node"
        }, 
        {
            "location": "/batch_system/#openmp-jobs-using-shared-memory", 
            "text": "To run a shared memory code using OpenMP on Alarik, you specify the\nnumber of cores you require using --tasks-per-node option of sbatch. In\nthis case you have to request placement on a single node with the \u201c-N 1\u201d\noption. In this example we call the executable \u201cprocessor_omp\u201d to\nemphasis that this need to be compiled with OpenMP support. Unless you\nare doing something special, you are not required to specify the\nenvironment variable OMP_NUM_THREADS. The example script uses the\ntechniques described for the  basic run script  to\nengage the node local disk.  #!/bin/bash  #  # Specify the number of threads - request all on 1 node  #SBATCH -N 1  #SBATCH --tasks-per-node=16  #  # job time, change for what your job requires  #SBATCH -t 00:10:00  #  # job name  #SBATCH -J data_process  #  # filenames stdout and stderr - customise, include %j  #SBATCH -o process_omp_%j.out  #SBATCH -e process_omp_%j.err  # write this script to stdout-file - useful for scripting errors  cat \\$0  # copy the input data and program to node local disk  # customise for your input file(s) and program name  cp -p input.dat processor_omp \\$SNIC_TMP  # change to the execution directory  cd \\$SNIC_TMP  # run the program  # customise for your program name and add arguments if required  ./processor_omp  # rescue the results to the submission directory  # customise for your result file(s)  cp -p result.dat \\$SLURM_SUBMIT_DIR  This script allows to use 2000 MB of main memory per requested core. If\nyou need more memory, this can be requested by:  #SBATCH -C mem64GB  #SBATCH --mem-per-cpu=4000  This will increase you memory request to 4000 MB per requested core.  Thread binding for OpenMP codes  The Alarik nodes deploy a cache-coherent non-uniform-memory access\narchitecture (cc-numa). Many scientific simulation codes gain\nsignificant performance benefits on a cc-numa architecture when the user\nbinds the threads to physical cores of the hardware. This inhibits\nthread migration and improves memory locality. Unfortunately invoking\nthread binding is not standartised. Depending on the OpenMP runtime\nlibrary the user needs to modify different environment variables to bind\nhis threads.  Thread binding with the GNU compilers  By default the GNU compiler suite (gcc/gfortran) does not bind threads\nto cores. To engage thread binding, you need to set the environment\nvariable GOMP_CPU_AFFINITY and provide this with a binding list. When\nsetting  export GOMP_CPU_AFFINITY=\u201d0-15\u201d  in your submission script, prior to starting your OpenMP application,\nthis will bind the threads to the 16 cores in the node. The above will\nbind thread 0 to core 0, thread 1 to core 1 and so on.  More advanced remark:  If you want to utilise only 8 cores from a\nnode and asking for exclusive node access (#SBATCH --exclusive), it\nmight be a good idea to place threads on every second core only. This\nwill give you more memory bandwidth and make sure you are utilising all\nFPUs of the Interlagos architecture. This can be achieved by setting:  export GOMP_CPU_AFFINITY=\u201d0-14:2\u201d  or  export GOMP_CPU_AFFINITY=\u201d0 2 4 6 8 10 12 14\u201d  It depend on details of your application, whether or not this helps\nperformance. Also note, when asking for a exclusive access to a note,\nyou will be charged for the full node, whether or not you use all cores.  Important pitfall:  If you set GOMP_CPU_AFFINITY=0 this will bind\nall threads to core 0. You will see extremely poor performance in this\ncase.  Thread binding with the open64 compiler  OpenMP code compiled with the  open64  compiler will use thread\nbinding on Alarik. In standard use cases this will actually boost\nperformance. However in special situation, e.g. when using fewer threads\nthan the size of your partition, you might see a performance boost by\nnot using thread binding. To do so you need to set the environment\nvariable \u201cO64_OMP_SET_AFFINITY=false\u201d  Thread binding with the Intel compiler  Versions 12.1 and 13.0 of the  Intel  compiler do not support thread\nbinding when used on the AMD processors deployed on Alarik. Starting\nfrom version 13.1 the Intel compile does support thread binding on the\nAMD processors deployed on Alarik. Obviously all versions of the Intel\ncompiler support thread binding on the Intel processors deployed on\nErik.  For version 13.1 of the Intel compiler thread is controlled by setting\nthe environment variable KMP_AFFINITY. The value  export KMP_AFFINITY=granularity=fine,compact  might be a good starting point for your experimentation.", 
            "title": "OpenMP jobs using shared memory"
        }, 
        {
            "location": "/batch_system/#hybrid-jobs-using-threads-within-an-mpi-framework", 
            "text": "A cluster with multicore nodes such as Alarik is a natural environment\nto execute parallel codes deploying both MPI and OpenMP threads. When\nrunning such applications the optimal number of MPI-tasks and OpenMP\nthreads to place on a node can depend highly on the details of the\napplication. In particular for application which make many references to\nmain memory and the programmer has not implemented a proper \u201cfirst touch\ndata allocation\u201d it is typically  best to have 2 or 4 threads  per MPI\ntask on an Alarik node. Together with a proper binding of your MPI tasks\nto the \u201cnuma-islands\u201d, this will ensure memory locality for your code.\nFor the below syntax you have to use  version 1.8.3 or newer  of the\nOpenMPI library.  In the following we give a simple example script to run a MPI-OpenMP\nhybrid named simul_hyb on 2 nodes using 8 tasks and 4 threads per task.\nThe tasks and their threads will be bound to the  numa-islands ,\nminimising cc-numa effects.  #!/bin/sh  # requesting number of nodes (-N option)  # number of mpi-tasks per node  # and number of threads per task exclusive nodes  #SBATCH -N 2  #SBATCH --tasks-per-node=4  #SBATCH --cpus-per-task=4  #SBATCH --exclusive  # time required  #SBATCH -t 01:00:00  #SBATCH -J hybrid_run  # filenames stdout and stderr - customise, include %j  #SBATCH -o simula_N2t4c4_%j.out  #SBATCH -e simula_N2t4c4_%j.out  cat \\$0  # Example assumes we need the intel runtime and OpenMPI library  # customise for the libraries your executable needs  module add intel/15.0  module add openmpi/1.8.3/intel/15.0  # Copying the executable onto the local disks of the nodes  srun -n \\$SLURM_NNODES -N \\$SLURM_NNODES cp -p simul_hyb \\$SNIC_TMP  # Copy the input file onto the headnode - if your MPI program  # reads from all tasks, you need to do the above srun construct  # again  cp -p input.dat \\$SNIC_TMP  cd \\$SNIC_TMP  # setting number of OpenMP threads and ask for thread binding  export OMP_NUM_THREADS=\\$SLURM_CPUS_PER_TASK  export OMP_PROC_BIND=true   mpiexec --map-by\nppr:\\$SLURM_NTASKS_PER_NODE:node:PE=\\$SLURM_CPUS_PER_TASK \\\nsimul_hyb   # Copy result files back - example assumes only task zero writes  # if in your application result files are written on all nodes  # you need to initiate a copy on each node via srun  cp -p result.dat \\$SLURM_SUBMIT_DIR  The example assumes that MPI task 0 is the only task reading and writing\ninput files. If your application reads and writes data on all nodes, you\nneed to study the  modifications  described in the\nMPI section.  As discussed, the above binds the tasks and their threads to the\nnuma-islands of the Alarik architecture. Alariks numa-islands have four\ncores, therefore the script is best used with 2 or four threads per MPI\ntask. This results in one or two MPI tasks per numa islands.  Things to try for MPI-OpenMP hybrids with 16 threads per task  While using more than 4 threads per MPI task on the Alarik system can\nresult in reduced performance due to cc-numa effects, there are\nsituations when using 16 threads per task can be required (e.g. special\nalgorithms or extreme memory requirements per MPI task).  When running 16 threads per MPI task, that is a single MPI task per\nAlarik node, you might want to experiment with starting your job without\nspecifying binding on mpiexec, that is remove the -bind-to-core, but\nutilise the  OpenMP thread binding  techniques\ndescribed in the OpenMP sample section.", 
            "title": "Hybrid-jobs using threads within an MPI framework"
        }, 
        {
            "location": "/batch_system/#interactive-access-to-compute-nodes", 
            "text": "Sometimes it is desirable to have an interactive login to the compute\nnodes of the cluster. Extensive code testing is a typical use case.", 
            "title": "Interactive access to compute nodes"
        }, 
        {
            "location": "/batch_system/#starting-an-interactive-session", 
            "text": "To start an interactive session you need to use the \u201cinteractive\u201d\ncommand. This will request the required resources from the resource pool\nfor you and start the interactive session once the resources are\navailable.  Use the following command to start an interactive session asking for 32\ncores lasting 60 minutes  interactive -n 32 -t 60  On Alarik and Eric this will be allocated on multiple nodes, since the\nnodes have only 16 cores available. The interactive session will last\nuntil either the requested time, 60 minutes in the above example, has\nexpired or you manually exit the interactive shell. Your account gets\ncharged with the wall time duration of your interactive session,\nindependent of the amount of computation you do. In the above example,\nif your session lasts until it expires after 60 min, you get charged for\n32 cpu hours. If you terminate your session after 1/2 hour, you would\nget charged 16 cpu hours.  The interactive command supports most command line options of the sbatch\ncommand. Please refer to the man pages of sbatch.", 
            "title": "Starting an interactive session"
        }, 
        {
            "location": "/batch_system/#modules-and-environment-variables", 
            "text": "Loaded modules and environment are not always exported properly to your\ninteractive session. Once placed in the interactive session, we\nrecommend users to reload  all  the modules they require. That is\ndespite the \u201cmodules list\u201d command claiming they are still loaded.  You also need to check whether environment variables still have the\nrequired values. If the software you are using has a set-up script, you\nmight need to re-run that script.", 
            "title": "Modules and environment variables"
        }, 
        {
            "location": "/batch_system/#known-issues-with-the-interactive-command", 
            "text": "None at the time of writing.", 
            "title": "Known issues with the interactive command"
        }, 
        {
            "location": "/aurora_modules/", 
            "text": "Using supported software on Lunarc's Aurora service\n\n\nAuthor: Joachim Hein (Lunarc)\n\n\nHierachical module naming scheme\n\n\nWith the start of the Aurora service Lunarc is using an hierachical module naming scheme.  Hierachical modules ensure that the correct shared libraries are available when running an application, while keeping screen output of standard module commands such as \nmodule avail\n managable.\n\n\nHierachical naming scheme concept\n\n\nWhen loging into the system, you only get access to those modules that do not require any special dynamic libraries.  After \nloading a compiler module\n you obtaing access to those packages that have been build with that specific compiler and depend on its shared libraries.  For many compilers this will include one or more matching MPI libraries.  After loading an MPI library additional software packages, depending on this pair (compiler \n MPI library), will become available.  Users should take note that in many cases loading an MPI library is required for software that doesn't really depend on it.\n\n\nUsing Modules\n\n\nThe module system on Aurora is utilising the Lua based \nLmod\n software.\n\n\nLoading packages\n\n\nThe command\n\n\nmodule avail\n\n\n\n\nshows the modules that can currently be accessed.  The output will look similar to\n\n\n--------------------------------- /sw/Modules/modulefiles/Core ---------------------------------\n   matlab/8.5\n\n-------------------------------- /sw/easybuild/modules/all/Core --------------------------------\n   Bison/3.0.4                    gompi/2015b\n   EasyBuild/2.5.0                gompi/2016a                        (D)\n   EasyBuild/2.6.0         (D)    icc/2015.3.187-GNU-4.9.3-2.25\n   GCC/4.9.3-binutils-2.25        icc/2016.1.150-GCC-4.9.3-2.25      (D)\n   GCC/4.9.3-2.25                 iccifort/2015.3.187-GNU-4.9.3-2.25\n   GCC/5.3.0               (D)    iccifort/2016.1.150-GCC-4.9.3-2.25 (D)\n   GCCcore/4.9.3                  ifort/2015.3.187-GNU-4.9.3-2.25\n   GNU/4.9.3-2.25                 ifort/2016.1.150-GCC-4.9.3-2.25    (D)\n   Java/1.8.0_72                  iimpi/7.3.5-GNU-4.9.3-2.25\n   M4/1.4.17                      iimpi/2016.01-GCC-4.9.3-2.25       (D)\n   binutils/2.25                  intel/2015b\n   flex/2.5.39                    intel/2016a\n   foss/2015b                     intel/2016.01                      (D)\n   foss/2016a              (D)    zlib/1.2.8\n\n-------------------------------- /sw/lmod/lmod/modulefiles/Core --------------------------------\n   lmod/6.0.24    settarg/6.0.24\n\n\n\n\nIn this example you can see modules and versions located in 3 directories.  Any of these modules can be accessed directly.  To obtain access to the software inside e.g. the toolchain module \nfoss/2016a\n one loads the module by issueing\n\n\nmodule load foss/2016a\n\n\n\n\nMany modules will load a number of modules, which they depend on.\nSince in the above output from \nmodule avail\n version 2016a is marked\nas the default version, the command\n\n\nmodule load foss\n\n\n\n\nwould have the same effect unless the default changes, which it may\ndo, if\n\n\n\n\nYou load a module\n\n\nThe lunarc team installs another version of the software\n\n\n\n\nSo if you require a specific version, the Lunarc team strongly\nrecommends to not rely on defaults, but explicitly specify the version\nyou are after.\n\n\nTo see what modules you have currently loaded use\n\n\nmodule list\n\n\n\n\nIn a hierachical module naming scheme the command \nmodule avail\n is\nnot as useful as it is in a flat module naming scheme which Lunarc\ndeployed on earlier services.   In many situations \nmodule avail\n\nresulted in the desired action, one has to use the \nmodule spider\n\ncommand which is descripted in the text below.\n\n\nPurging the loaded modules\n\n\nMany modules will load a number extra of modules, which they depend on.\nWhen unloading a  module, these dependencies will typically not be unloaded.  For\nthat reason we currently recommend using\n\n\nmodule purge\n\n\n\n\nwhen loaded modules are no longer needed.   You would then start\nloading the modules required for the next task you need to accomplish\nfrom scratch.\n\n\nSeaching for all software packages\n\n\nIn practical use, the command \nmodule spider\n is key to search for packages in an Lmod based hierachical module naming scheme.  To get an overview on the software installed on Aurora, simply type\n\n\nmodule spider\n\n\n\n\nat the command prompt.  This will create an output similar to:\n\n\n---------------------------------------------------------------------\nThe following is a list of the modules currently available:\n---------------------------------------------------------------------\n  Autoconf: Autoconf/2.69\n\n  Automake: Automake/1.15\n\n  Autotools: Autotools/20150215\n\n  Bison: Bison/3.0.4\n\n  Boost: Boost/1.58.0-Python-2.7.9\n\n  CMake: CMake/3.2.2, CMake/3.3.2\n\n  Cube: Cube/4.3\n\n  EasyBuild: EasyBuild/2.5.0, EasyBuild/2.6.0\n\n  FFTW: FFTW/3.3.4\n\n  GCC: GCC/4.9.3-binutils-2.25, GCC/4.9.3-2.25, GCC/5.3.0\n\n  GCCcore: GCCcore/4.9.3\n\n  GLib: GLib/2.42.1, GLib/2.46.0\n\n  GNU: GNU/4.9.3-2.25\n\n  GROMACS: GROMACS/5.0.4-mt, GROMACS/5.0.5-hybrid\n\n  ...\n\n\n\n\nThis is a full list of the packages and versions available on the service.  \n\n\nSearching for a specific package\n\n\nIf you are looking for a specific package and have an idea on what its name might be, you can give this as an argument to \nmodule spider\n.   This argument is case insensitive.  \n\n\nExample: Accessing a Gromacs version\n\n\nFor example, when looking to run Gromacs:\n\n\nmodule spider gromacs\n\n\n\n\nYou obtain output simlar to:\n\n\n---------------------------------------------------------------------------------\n  GROMACS:\n---------------------------------------------------------------------------------\n     Versions:\n        GROMACS/5.0.4-mt\n        GROMACS/5.0.5-hybrid\n\n---------------------------------------------------------------------------------\n  To find detailed information about GROMACS please enter the full name.\n  For example:\n\n     $ module spider GROMACS/5.0.5-hybrid\n---------------------------------------------------------------------------------\n\n\n\n\nThis tells you that the multi threaded version 5.0.4 and the hybrid version 5.0.5 are installed.  If you want to use the version 5.0.5 issue the command:\n\n\nmodule spider GROMACS/5.0.5-hybrid\n\n\n\n\nYou get the folling output\n\n\n---------------------------------------------------------------------------------\n  GROMACS: GROMACS/5.0.5-hybrid\n---------------------------------------------------------------------------------\n\n    This module can only be loaded through the following modules:\n\n      icc/2016.1.150-GCC-4.9.3-2.25  impi/5.1.2.150\n      ifort/2016.1.150-GCC-4.9.3-2.25  impi/5.1.2.150\n\n... \n\n\n\n\nThis lists the modules you have to load before accessing Gromacs.  In this case you have two options, we choose the first option.  We load\n\n\nmodule load icc/2016.1.150-GCC-4.9.3-2.25\nmodule load impi/5.1.2.150\n\n\n\n\nAfter which we can load the gromacs installation:\n\n\nmodule load GROMACS/5.0.5-hybrid\n\n\n\n\nLoading this module will load a number of additional module require for Gromacs to work.\n\n\nExample accessing R\n\n\nThis is another example on how to access a specific software package.  This time we want to run the statistical software package R.\n\n\nmodule spider R\n\n\n\n\nOne gets:\n\n\n------------------------------------------------------------------\n  R:\n------------------------------------------------------------------\n     Versions:\n        R/3.2.1-bare\n        R/3.2.1\n        R/3.2.3\n\n     Other possible modules matches:\n        GCCcore  GROMACS  SuiteSparse  cURL  fixesproto  fontsproto  ...\n\n------------------------------------------------------------------\n  To find other possible module matches do:\n      module -r spider '.*R.*'\n\n------------------------------------------------------------------\n  To find detailed information about R please enter the full name.\n  For example:\n\n     $ module spider R/3.2.1-bare\n------------------------------------------------------------------\n\n\n\n\nIf we are interested in version 3.2.3, we do a\n\n\nmodule spider R/3.2.3\n\n\n\n\nnext and get the following info:\n\n\n------------------------------------------------------------------\n  R: R/3.2.3\n------------------------------------------------------------------\n\n     Other possible modules matches:\n        GCCcore, GROMACS, SuiteSparse, cURL, fixesproto, ...\n\n    This module can only be loaded through the following modules:\n\n      GCC/4.9.3-binutils-2.25  OpenMPI/1.8.8\n\n...\n\n\n\n\nThe output states the two modules that need loading to get access to this R version.  We issue\n\n\nmodule load GCC/4.9.3-binutils-2.25\nmodule load OpenMPI/1.8.8\nmodule load R/3.2.3\n\n\n\n\nand have acces to R.\n\n\nLmod cache\n\n\nTo improve the performance of the \nmodule spider\n command, lmod caches\nthe entire module structure of the system.  This cache is currently\nconfigured to be updated once in 24 hours.\n\n\nThis can have the effect that you see an outdated version of the\nmodule tree, when using commands such as \nmodule avail\nor \nmodule\nspider\n.   The cache file is stored in the directory\n\n$HOME/.lmod.d/.cache/\n remove the cache file and lmod will recreate\nit for you.\n\n\nToolchains\n\n\nA signficant portion of the Aurora software is build \nEasyBuild\n software framework.  This frame work provides so called \nToolchains\n which are utilised to build software.  Lunarc recommends using these toolchains even when building software outside the EasyBuild framework.\n\n\nLunarc actively maintains the following toolchains\n+ \nfoss\n: BLACS, FFTW, GCC, OpenBLAS, OpenMPI, ScaLAPACK\n+ \ngompi\n: GCC, OpenMPI\n+ \nintel\n: icc, ifort, MKL, Intel MPI\n+ \niimpi\n: icc, ifort, Intel MPI\n\n\nIf you require additional toolchains, contact \nLunarc support\n to discuss your requirements.", 
            "title": "Aurora Software"
        }, 
        {
            "location": "/aurora_modules/#using-supported-software-on-lunarcs-aurora-service", 
            "text": "Author: Joachim Hein (Lunarc)", 
            "title": "Using supported software on Lunarc's Aurora service"
        }, 
        {
            "location": "/aurora_modules/#hierachical-module-naming-scheme", 
            "text": "With the start of the Aurora service Lunarc is using an hierachical module naming scheme.  Hierachical modules ensure that the correct shared libraries are available when running an application, while keeping screen output of standard module commands such as  module avail  managable.", 
            "title": "Hierachical module naming scheme"
        }, 
        {
            "location": "/aurora_modules/#hierachical-naming-scheme-concept", 
            "text": "When loging into the system, you only get access to those modules that do not require any special dynamic libraries.  After  loading a compiler module  you obtaing access to those packages that have been build with that specific compiler and depend on its shared libraries.  For many compilers this will include one or more matching MPI libraries.  After loading an MPI library additional software packages, depending on this pair (compiler   MPI library), will become available.  Users should take note that in many cases loading an MPI library is required for software that doesn't really depend on it.", 
            "title": "Hierachical naming scheme concept"
        }, 
        {
            "location": "/aurora_modules/#using-modules", 
            "text": "The module system on Aurora is utilising the Lua based  Lmod  software.", 
            "title": "Using Modules"
        }, 
        {
            "location": "/aurora_modules/#loading-packages", 
            "text": "The command  module avail  shows the modules that can currently be accessed.  The output will look similar to  --------------------------------- /sw/Modules/modulefiles/Core ---------------------------------\n   matlab/8.5\n\n-------------------------------- /sw/easybuild/modules/all/Core --------------------------------\n   Bison/3.0.4                    gompi/2015b\n   EasyBuild/2.5.0                gompi/2016a                        (D)\n   EasyBuild/2.6.0         (D)    icc/2015.3.187-GNU-4.9.3-2.25\n   GCC/4.9.3-binutils-2.25        icc/2016.1.150-GCC-4.9.3-2.25      (D)\n   GCC/4.9.3-2.25                 iccifort/2015.3.187-GNU-4.9.3-2.25\n   GCC/5.3.0               (D)    iccifort/2016.1.150-GCC-4.9.3-2.25 (D)\n   GCCcore/4.9.3                  ifort/2015.3.187-GNU-4.9.3-2.25\n   GNU/4.9.3-2.25                 ifort/2016.1.150-GCC-4.9.3-2.25    (D)\n   Java/1.8.0_72                  iimpi/7.3.5-GNU-4.9.3-2.25\n   M4/1.4.17                      iimpi/2016.01-GCC-4.9.3-2.25       (D)\n   binutils/2.25                  intel/2015b\n   flex/2.5.39                    intel/2016a\n   foss/2015b                     intel/2016.01                      (D)\n   foss/2016a              (D)    zlib/1.2.8\n\n-------------------------------- /sw/lmod/lmod/modulefiles/Core --------------------------------\n   lmod/6.0.24    settarg/6.0.24  In this example you can see modules and versions located in 3 directories.  Any of these modules can be accessed directly.  To obtain access to the software inside e.g. the toolchain module  foss/2016a  one loads the module by issueing  module load foss/2016a  Many modules will load a number of modules, which they depend on.\nSince in the above output from  module avail  version 2016a is marked\nas the default version, the command  module load foss  would have the same effect unless the default changes, which it may\ndo, if   You load a module  The lunarc team installs another version of the software   So if you require a specific version, the Lunarc team strongly\nrecommends to not rely on defaults, but explicitly specify the version\nyou are after.  To see what modules you have currently loaded use  module list  In a hierachical module naming scheme the command  module avail  is\nnot as useful as it is in a flat module naming scheme which Lunarc\ndeployed on earlier services.   In many situations  module avail \nresulted in the desired action, one has to use the  module spider \ncommand which is descripted in the text below.", 
            "title": "Loading packages"
        }, 
        {
            "location": "/aurora_modules/#purging-the-loaded-modules", 
            "text": "Many modules will load a number extra of modules, which they depend on.\nWhen unloading a  module, these dependencies will typically not be unloaded.  For\nthat reason we currently recommend using  module purge  when loaded modules are no longer needed.   You would then start\nloading the modules required for the next task you need to accomplish\nfrom scratch.", 
            "title": "Purging the loaded modules"
        }, 
        {
            "location": "/aurora_modules/#seaching-for-all-software-packages", 
            "text": "In practical use, the command  module spider  is key to search for packages in an Lmod based hierachical module naming scheme.  To get an overview on the software installed on Aurora, simply type  module spider  at the command prompt.  This will create an output similar to:  ---------------------------------------------------------------------\nThe following is a list of the modules currently available:\n---------------------------------------------------------------------\n  Autoconf: Autoconf/2.69\n\n  Automake: Automake/1.15\n\n  Autotools: Autotools/20150215\n\n  Bison: Bison/3.0.4\n\n  Boost: Boost/1.58.0-Python-2.7.9\n\n  CMake: CMake/3.2.2, CMake/3.3.2\n\n  Cube: Cube/4.3\n\n  EasyBuild: EasyBuild/2.5.0, EasyBuild/2.6.0\n\n  FFTW: FFTW/3.3.4\n\n  GCC: GCC/4.9.3-binutils-2.25, GCC/4.9.3-2.25, GCC/5.3.0\n\n  GCCcore: GCCcore/4.9.3\n\n  GLib: GLib/2.42.1, GLib/2.46.0\n\n  GNU: GNU/4.9.3-2.25\n\n  GROMACS: GROMACS/5.0.4-mt, GROMACS/5.0.5-hybrid\n\n  ...  This is a full list of the packages and versions available on the service.", 
            "title": "Seaching for all software packages"
        }, 
        {
            "location": "/aurora_modules/#searching-for-a-specific-package", 
            "text": "If you are looking for a specific package and have an idea on what its name might be, you can give this as an argument to  module spider .   This argument is case insensitive.    Example: Accessing a Gromacs version  For example, when looking to run Gromacs:  module spider gromacs  You obtain output simlar to:  ---------------------------------------------------------------------------------\n  GROMACS:\n---------------------------------------------------------------------------------\n     Versions:\n        GROMACS/5.0.4-mt\n        GROMACS/5.0.5-hybrid\n\n---------------------------------------------------------------------------------\n  To find detailed information about GROMACS please enter the full name.\n  For example:\n\n     $ module spider GROMACS/5.0.5-hybrid\n---------------------------------------------------------------------------------  This tells you that the multi threaded version 5.0.4 and the hybrid version 5.0.5 are installed.  If you want to use the version 5.0.5 issue the command:  module spider GROMACS/5.0.5-hybrid  You get the folling output  ---------------------------------------------------------------------------------\n  GROMACS: GROMACS/5.0.5-hybrid\n---------------------------------------------------------------------------------\n\n    This module can only be loaded through the following modules:\n\n      icc/2016.1.150-GCC-4.9.3-2.25  impi/5.1.2.150\n      ifort/2016.1.150-GCC-4.9.3-2.25  impi/5.1.2.150\n\n...   This lists the modules you have to load before accessing Gromacs.  In this case you have two options, we choose the first option.  We load  module load icc/2016.1.150-GCC-4.9.3-2.25\nmodule load impi/5.1.2.150  After which we can load the gromacs installation:  module load GROMACS/5.0.5-hybrid  Loading this module will load a number of additional module require for Gromacs to work.  Example accessing R  This is another example on how to access a specific software package.  This time we want to run the statistical software package R.  module spider R  One gets:  ------------------------------------------------------------------\n  R:\n------------------------------------------------------------------\n     Versions:\n        R/3.2.1-bare\n        R/3.2.1\n        R/3.2.3\n\n     Other possible modules matches:\n        GCCcore  GROMACS  SuiteSparse  cURL  fixesproto  fontsproto  ...\n\n------------------------------------------------------------------\n  To find other possible module matches do:\n      module -r spider '.*R.*'\n\n------------------------------------------------------------------\n  To find detailed information about R please enter the full name.\n  For example:\n\n     $ module spider R/3.2.1-bare\n------------------------------------------------------------------  If we are interested in version 3.2.3, we do a  module spider R/3.2.3  next and get the following info:  ------------------------------------------------------------------\n  R: R/3.2.3\n------------------------------------------------------------------\n\n     Other possible modules matches:\n        GCCcore, GROMACS, SuiteSparse, cURL, fixesproto, ...\n\n    This module can only be loaded through the following modules:\n\n      GCC/4.9.3-binutils-2.25  OpenMPI/1.8.8\n\n...  The output states the two modules that need loading to get access to this R version.  We issue  module load GCC/4.9.3-binutils-2.25\nmodule load OpenMPI/1.8.8\nmodule load R/3.2.3  and have acces to R.", 
            "title": "Searching for a specific package"
        }, 
        {
            "location": "/aurora_modules/#lmod-cache", 
            "text": "To improve the performance of the  module spider  command, lmod caches\nthe entire module structure of the system.  This cache is currently\nconfigured to be updated once in 24 hours.  This can have the effect that you see an outdated version of the\nmodule tree, when using commands such as  module avail or  module\nspider .   The cache file is stored in the directory $HOME/.lmod.d/.cache/  remove the cache file and lmod will recreate\nit for you.", 
            "title": "Lmod cache"
        }, 
        {
            "location": "/aurora_modules/#toolchains", 
            "text": "A signficant portion of the Aurora software is build  EasyBuild  software framework.  This frame work provides so called  Toolchains  which are utilised to build software.  Lunarc recommends using these toolchains even when building software outside the EasyBuild framework.  Lunarc actively maintains the following toolchains\n+  foss : BLACS, FFTW, GCC, OpenBLAS, OpenMPI, ScaLAPACK\n+  gompi : GCC, OpenMPI\n+  intel : icc, ifort, MKL, Intel MPI\n+  iimpi : icc, ifort, Intel MPI  If you require additional toolchains, contact  Lunarc support  to discuss your requirements.", 
            "title": "Toolchains"
        }
    ]
}
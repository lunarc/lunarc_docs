<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Batch system - Lunarc Documentation</title>
  

  <link rel="shortcut icon" href="../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "Batch system";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script>
  <script src="../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Lunarc Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../login_howto/">How to login</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../quick_reference/">Quick reference</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">Batch system</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#slurm-the-batch-system-on-alarik-and-erik">SLURM - the batch system on Alarik and Erik</a></li>
                
            
                <li class="toctree-l3"><a href="#job-submission">Job submission</a></li>
                
                    <li><a class="toctree-l4" href="#first-example-for-a-job-submission">First example for a job submission</a></li>
                
                    <li><a class="toctree-l4" href="#resource-statements-for-all-jobs">Resource statements for all jobs</a></li>
                
                    <li><a class="toctree-l4" href="#resource-statements-for-multiprocessor">Resource statements for multiprocessor</a></li>
                
                    <li><a class="toctree-l4" href="#program-execution-environment">Program execution environment</a></li>
                
                    <li><a class="toctree-l4" href="#using-the-node-local-disks-to-improve-io-performance">Using the node local disks to improve I/O performance</a></li>
                
                    <li><a class="toctree-l4" href="#launching-mpi-jobs-in-openmpi">Launching MPI jobs in OpenMPI</a></li>
                
                    <li><a class="toctree-l4" href="#submitting-monitoring-and-manipulating-jobs-in-slurm">Submitting, monitoring and manipulating jobs in SLURM</a></li>
                
            
                <li class="toctree-l3"><a href="#example-job-scripts">Example job scripts</a></li>
                
                    <li><a class="toctree-l4" href="#job-scripts-using-the-node-local-disk">Job scripts using the node local disk</a></li>
                
                    <li><a class="toctree-l4" href="#running-multiple-serial-jobs-within-a-single-job-submission">Running multiple serial jobs within a single job submission</a></li>
                
                    <li><a class="toctree-l4" href="#mpi-job-using-16-tasks-per-node">MPI job using 16 tasks per node</a></li>
                
                    <li><a class="toctree-l4" href="#mpi-jobs-using-fewer-than-16-tasks-per-node">MPI jobs using fewer than 16 tasks per node</a></li>
                
                    <li><a class="toctree-l4" href="#openmp-jobs-using-shared-memory">OpenMP jobs using shared memory</a></li>
                
                    <li><a class="toctree-l4" href="#hybrid-jobs-using-threads-within-an-mpi-framework">Hybrid-jobs using threads within an MPI framework</a></li>
                
            
                <li class="toctree-l3"><a href="#interactive-access-to-compute-nodes">Interactive access to compute nodes</a></li>
                
                    <li><a class="toctree-l4" href="#starting-an-interactive-session">Starting an interactive session</a></li>
                
                    <li><a class="toctree-l4" href="#modules-and-environment-variables">Modules and environment variables</a></li>
                
                    <li><a class="toctree-l4" href="#known-issues-with-the-interactive-command">Known issues with the interactive command</a></li>
                
            
            </ul>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../aurora_modules/">Aurora Software</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../linux_basics/">Linux Basics</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Lunarc Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Batch system</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p><strong>Using the job submission system on Alarik and Erik</strong></p>
<p>Joachim Hein, Jonas Lindemann, Anders Sjöström, Magnus Ullner</p>
<p><em>Document under active development - Check back frequently!</em></p>
<p>A more in-depth guide to the job submission system on Alarik and Erik.</p>
<h1 id="slurm-the-batch-system-on-alarik-and-erik">SLURM - the batch system on Alarik and Erik</h1>
<p>On a modern HPC system efficient management of the compute resources is
absolutely crucial for the system to perform. Alarik and Erik are the
first Lunarc systems to deploy SLURM (<strong>S</strong>imple <strong>L</strong>inux <strong>U</strong>tility
for <strong>R</strong>esource <strong>M</strong>anagement) as resource manager. For your program
to be executed you have to describe to SLURM the resources required by
your program, the name of your program and the command line arguments
your program may require. SLURM also allows monitoring and manipulation
of the progress of your programs execution.</p>
<p>This document contains two key parts. The <a href="#id.asft9ff8nuhu"><em>first
part</em></a> describes in-depth the job submission system
and its options. The <a href="#id.6w4ezxvvga34"><em>second part</em></a> gives example
scripts for the most common use cases. They hopefully serve as a good
starting point when creating submission scripts fitting your needs and
requirements.</p>
<h1 id="job-submission">Job submission</h1>
<h2 id="first-example-for-a-job-submission">First example for a job submission</h2>
<h3 id="the-job-script-and-sbatch">The job script and sbatch</h3>
<p>You register your program with SLURM for execution using the <strong>sbatch</strong>
command. This is done easiest by using a <em>job description file</em>. The job
description file is also know as a <em>job script</em>.</p>
<p>A very simple job script, looks as follows:</p>
<pre><code>#!/bin/sh
#SBATCH -t 00:05:00

echo "hello"
</code></pre>
<p>Write this into a file. In the following we assume the file is named
echo_script.sh, but in principle any name will do. You can now send the
script for execution using the sbatch command. This will execute the
“program” echo on the backend.</p>
<pre><code>sbatch echo_script.sh
</code></pre>
<p>This should deliver a screen output similar to</p>
<pre><code>[fred@alarik Serial]$ sbatch echo_script.sh
Submitted batch job 7185
</code></pre>
<p>Where 7185 is the job number assigned by SLURM. Once your job has
executed you will find a file slurm-7185.out in your directory which
contains the output and error messages from your program.</p>
<h3 id="the-three-parts-of-a-job-script">The three parts of a job script</h3>
<p>The example echo_script.sh shows the three parts every job script
requires</p>
<ol>
<li>Shell specification</li>
<li>Resource statement</li>
<li>Body containing a UNIX script</li>
</ol>
<p>In our example each part consists of a single line. The first line of
our example contains the shell specification, in most cases the sh-shell
as used here is just fine. The second line starting with #SBATCH
specifies the resources needed. In our case it asks for 10 minutes of
computer time. If the jobs hasn’t finished after that time, SLURM will
terminate it. Job scripts typically contain more than one of these
statements, specifying e.g. more than one processor or more memory. The
most commonly used resource statements at Lunarc will be explained
below. The resource statements are followed by a list of programs and
UNIX commands to be executed on the system. This is actually a normal
UNIX script and everything you can do in a UNIX script can be done here
as well. In our example the script consists out of the UNIX echo
command.</p>
<h2 id="resource-statements-for-all-jobs">Resource statements for all jobs</h2>
<p>We now describe a number of statements which are most commonly used to
specify resource requirements for all kind of jobs. Refer to “man
sbatch” for more information.</p>
<h3 id="walltime">Walltime</h3>
<p>The walltime attribute specifies the time requested for completing the
job. The time is <em>not</em> cpu-time but the total time, as measured by a
normal clock. In the previous example the time requested was 0 hours 5
minutes and 0 seconds. Walltime is specified in seconds or using the
following notation:</p>
<pre><code>Hours:Minutes:Seconds
</code></pre>
<p>If your calculation hasn’t finished once the specified time has elapsed,
SLURM will terminate your job. It is therefore <strong>good practise</strong> to
specify a bit more time than you anticipate your job to take. This makes
sure that you still get your results, even the jobs is slowed by some
interference, e.g. waiting for a write to a shared file system to
finish. However don’t specify excessive extra time. Due to scheduling
constraints, jobs asking for less time will typically spend less time in
the queue, waiting for their execution. This also provides safety
against depletion of your allocation. If, e.g., your job hangs, SLURM
will terminate your job and the project will be charged less time if the
walltime margin is not excessive.</p>
<p>To specify your walltime requirements write a statement like</p>
<pre><code>#SBATCH -t 00:10:00
</code></pre>
<p>into your job script.</p>
<p>The maximum walltime for any job on Alarik is 168h, which is the same as
7 days. On Erik the maximum walltime for any job is 48h.</p>
<h3 id="job-naming">Job naming</h3>
<p>All jobs are given both a job identifier and a name, for easier
identification in the batch-system. The default name given to a job is
the file name of the submit script, which can make it difficult to
identify your job, if you use a standard name for your submit scripts.
You can give your job a name from inside the script by using the -J
option:</p>
<pre><code>#SBATCH -J parameterTest
</code></pre>
<p>This will name your job “parameterTest”.</p>
<h3 id="specifying-a-project-for-users-with-multiple-projects">Specifying a project for users with multiple projects</h3>
<p>Most users are members of only one project. These users do not need to
specify a project in their in their submission script. The Lunarc set-up
will automatically use that project for accounting.</p>
<p>A few users are members of more than project. In this case the system
would not know which project to charge for the run, so you need to
specify the project using the -A option:</p>
<pre><code>#SBATCH -A snic2015-x-xxx
</code></pre>
<p>Replace the “snic2015-x-xxx” with the string naming your project. You
can inquire the correct string using the projinfo command or the SUPR
system.</p>
<h3 id="specifying-memory-requirements">Specifying memory requirements</h3>
<p>Alarik has 32 GB of memory installed on the small memory nodes and 64 GB
of memory on the large memory nodes. The default memory request per core
on the system is 2000 MB (a sixteenth of 32GB). If more then 2000 MB per
core is needed it has to be requested explictly with the
<strong>--mem-per-cpu</strong> option of sbatch. In this case you also have to
request allocation on a large memory node using the <strong>-C mem64GB</strong>
option of sbatch. The following show an example how to request 4000 MB
or main memory per compute core used:</p>
<pre><code>#SBATCH -C mem64GB
#SBATCH --mem-per-cpu=4000
</code></pre>
<p>When requesting more than 2000 MB of memory, your jobs may spend a
longer time in the queue, waiting for execution, since it needs to wait
for run-slot(s) on the large memory nodes to become available. When
requesting more then 4000 MB per processing core, your jobs will be
charged at a higher rate. In this case some processing cores have to
remain idle since you are using more than your fair share of memory.</p>
<p>Erik has 64 Gb of memory on the standard nodes. Each node has two CPUs
with eight cores each. The default memory request per core is therefore
4000 MB of memory. As in the case of Alarik, if more than 4000MB of
memory per core is needed it has to be described as above.</p>
<h3 id="controlling-job-output">Controlling job output</h3>
<p>By default, the output which your job writes to stdout and stderr is
written to a file named</p>
<pre><code>slurm_%j.out
</code></pre>
<p>The %j in the file name will be replaced by the jobnumber SLURM assigns
to your job. This ensures that the output file from your job is unique
and different jobs do not interfere with each other's output file.</p>
<p>In many cases the default file name is not convenient. You might want to
have a file name which is more descriptive of the job that is actually
running - you might even want to include important meta-data, such as
physical parameters, into the output filename(s). This can be achieved
by using the -o and -e options of sbatch. The -o option specifies the
file containing the stdout and the -e option the file containing the
stderr. It is good practise to include the %j string into the filenames.
That will prevent jobs from overwriting each other's output files. The
following gives an example:</p>
<pre><code>#SBATCH -o calcflow_m1_%j.out
#SBATCH -e calcflow_m1_%j.err
</code></pre>
<p>You can give the same filename for both options to get stdout and stderr
written to the same file.</p>
<h3 id="notification">Notification</h3>
<p>SLURM on the systems can send you email if the status of your job
changes as it progresses through the job queue. To use this feature you
need to specify the email address using the --mail-user option and
specify the event you want to get notified about using the --mail-type
option. The following</p>
<pre><code>#SBATCH --mail-user=fred@institute.se
#SBATCH --mail-type=END
</code></pre>
<p>Will send an email to the address fred@institute.se once the job has
ended. Valid type values, selecting the event you can get notified
about, are BEGIN, END, FAIL, REQUEUE, and ALL (any state change).</p>
<h3 id="job-dependencies">Job dependencies</h3>
<p>To describe job dependencies, use the -d option of sbatch. This is
particularly useful for job dependencies, in workflows.</p>
<p>To illustrate this consider the following example. You require a serial
job to create a mesh for your simulation. Once this has finished, you
want to start a parallel job, which uses the mesh. You first submit the
mesh creation job using sbatch</p>
<pre><code>[fred@alarik Simcode]$ sbatch run_mesh.sh
Submitted batch job 8042
</code></pre>
<p>As discussed, sbatch returns you a jobid, 8042 in this example. You use
this to declare your dependency when submitting the simulation job to
the queue</p>
<pre><code>[fred@alarik Simcode]$ sbatch -d afterok:8042 run_sim.sh
Submitted batch job 8043
</code></pre>
<p>When using squeue to monitor job 8043, this should now be in status
pending (PD) with the reason of dependency. Another common use case for
this functionality is a simulation requiring many days of computer times
being split into a number of submissions.</p>
<h3 id="test-queue">Test queue</h3>
<p>To run short tests, it is possible to request extra high priority on
Alarik with the help of</p>
<pre><code>#SBATCH --qos=test
</code></pre>
<p>For one such job, the maximum walltime is 1 h and the maximum number of
nodes is two and a user is only allowed to run two such jobs
simultaneously. A system of floating reservations is used to free two
nodes every second hour between 8.00 and 20.00 to reduce the queue time
for test jobs. The way it works also means that the shorter the test
job, the more likely it is to start sooner rather than later. It is not
allowed to use qos=test for series of production runs.</p>
<p>On Erik there is one two-GPU node reserved for tests in a partition of
its own, which is specified with</p>
<pre><code>#SBATCH -p test
</code></pre>
<p>Like on Alarik, the maximum walltime is 1 h.</p>
<h3 id="extra-fat-nodes-on-alarik">Extra fat nodes on Alarik</h3>
<p>Alarik has four nodes with 48 cores and 128 GB memory. To access them,
the partition extra has to be specified</p>
<pre><code>#SBATCH -p extra
</code></pre>
<p>Furthermore, the amount of memory per requested core also needs to be
given.</p>
<pre><code>#SBATCH --mem-per-cpu=&lt;memory in MB&gt;
</code></pre>
<p>Otherwise, the default value of 2 000 MB will be set at and if more is
used, slurm can kill the job. For example, to run on the 48 cores of a
single node and use a total of 128 000 MB of memory (for more on
multiprocessor statements, see the next section):</p>
<pre><code>#SBATCH -N 1
#SBATCH --tasks-per-node=48
#SBATCH --mem-per-cpu=3000
#SBATCH -p extra
</code></pre>
<h3 id="fat-extra-fat-and-mic-nodes-on-erik">Fat, extra fat and MIC nodes on Erik</h3>
<p>Erik has 7 nodes with 4 GPUs (and 96 GB of memory). To access them, the
partition fat has to be specified</p>
<pre><code>#SBATCH -p fat
</code></pre>
<p>One node is equipped with 8 GPUs (and 96 GB of memory), which is in
partition extra</p>
<pre><code>#SBATCH -p extra
</code></pre>
<p>There is also one node with two Xeon Phi (MIC) cards in the partition
mic</p>
<pre><code>#SBATCH -p mic
</code></pre>
<p>There is also one node with two Nvidia K80 cards in the partition new</p>
<pre><code>#SBATCH -p new
</code></pre>
<p>If no -p option is specified, normal nodes with two Nvidia K20 cards
will be allocated to the job.</p>
<h2 id="resource-statements-for-multiprocessor">Resource statements for multiprocessor</h2>
<p>In HPC it is very common to have many processing elements working on a
job. The extra processing power can be utilised to process large
problems beyond the capabilities of a single processing element. It can
also be used to swiftly perform a number of calculations within a single
job submission.</p>
<h3 id="terminology-around-nodes-processors-cores-tasks">Terminology around nodes, processors, cores, tasks</h3>
<p>There is a a lot of structure within modern HPC equipment. For the
purposes of this user guide we will stick to the following terminology:</p>
<table>
<thead>
<tr>
<th>Term</th>
<th>Explanation</th>
<th>Number on Alarik</th>
<th>Number  on Erik</th>
</tr>
</thead>
<tbody>
<tr>
<td>Node</td>
<td>A physical computer</td>
<td>Standard:</td>
<td>Standard:</td>
</tr>
<tr>
<td></td>
<td></td>
<td>200</td>
<td>16</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Extra:</td>
<td>Fat:</td>
</tr>
<tr>
<td></td>
<td></td>
<td>4</td>
<td>7</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Extra:</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>1</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Mic:</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>1</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>New:</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>1</td>
</tr>
<tr>
<td>Processor</td>
<td>This denotes a the multi-core processor, housing many processing elements</td>
<td>Standard:</td>
<td>2 per node</td>
</tr>
<tr>
<td></td>
<td></td>
<td>2 per node</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Extra:</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>4 per node</td>
<td></td>
</tr>
<tr>
<td>GPU</td>
<td>This denotes a nvidia co-processor</td>
<td>0</td>
<td>Standard:</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>2 per node</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Fat:</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>4 per node</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Extra:</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>8 per node</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Mic:</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>0 per node</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>New:</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>2 cards per node</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>2 logical per card</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>4 logical per node</td>
</tr>
<tr>
<td>Socket</td>
<td>This is the “plug” the processor gets plugged into.  Used as a synonym for the processor</td>
<td>Standard:</td>
<td>2 per node</td>
</tr>
<tr>
<td></td>
<td></td>
<td>2 per node</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Extra:</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>4 per node</td>
<td></td>
</tr>
<tr>
<td>Core</td>
<td>Individual processing element</td>
<td>Standard:</td>
<td>16 per node</td>
</tr>
<tr>
<td></td>
<td></td>
<td>16 per node</td>
<td>8 per processor</td>
</tr>
<tr>
<td></td>
<td></td>
<td>8 per processor</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Extra:</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>48 per node</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>12 per processor</td>
<td></td>
</tr>
<tr>
<td>Task</td>
<td>This is a software concept.  It denotes a process, which is an instance of a running program.  It has its own data and instruction stream(s).  It can fork multiple threads to increase the computational speed.  Serial programs and pure MPI programs do not spawn threads.</td>
<td>User controls in job script</td>
<td>User controls in job script</td>
</tr>
<tr>
<td>Thread</td>
<td>This is also a software concept.  A thread is a stream of instructions executed on the hardware.  It is part of a task and shares resources such as  data with other threads within the same task.</td>
<td>User controls in job script</td>
<td>User controls in job script</td>
</tr>
</tbody>
</table>
<h3 id="outline-resource-requests-for-multiprocessor-jobs">Outline: Resource requests for multiprocessor jobs</h3>
<p>When running multi processor jobs on the Lunarc clusters, one should
specify:</p>
<ol>
<li>The number of nodes required by the jobs</li>
<li>The number of computational tasks per node</li>
<li>The number of threads spawned by each task</li>
</ol>
<p>For a pure MPI job or when processing a large number of serial jobs in a
so called task farm, one will typically only specify the items 1 and 2,
while for a threaded job, using e.g. OpenMP or Java, one will typically
only specify items 1 and 3.</p>
<p>It is typically not advisable to have the product of items 2 and 3
exceeding the number of cores per node, which is 16 for standard Alarik
and Erik compute nodes. On the Alarik extra compute nodes this number is
48. In most cases users requesting multiple nodes will want the product
to equal the number of cores per node. The syntax how to control nodes,
tasks per node and threads per task is explaned below.</p>
<h3 id="exclusive-node-access">Exclusive node access</h3>
<p>For parallel codes using MPI or OpenMP it is typically best to keep
interference on the nodes at a minimum, that is to have exclusive access
to the nodes you are using. This also applies to specialist and
experimental work, which would interfere very badly with other user’s
codes on the nodes. Adding</p>
<pre><code>#SBATCH --exclusive
</code></pre>
<p>to your job script will ensure that SLURM will allocate dedicated nodes
to your job. Obviously your project gets charged for the full costs of
the nodes you are using, that is in case of Alarik and Erik 16 cores per
node.</p>
<h3 id="specifying-the-number-of-nodes-required-for-the-job">Specifying the number of nodes required for the job</h3>
<p>In SLURM one requests the number of nodes for a job with the <strong>-N</strong>
option. The following statement requests four nodes for your job:</p>
<pre><code>#SBATCH -N 4
</code></pre>
<p><strong>Important:</strong> without using either the --tasks-per-node or
the --cpus-per-task options of sbatch, this will reserve a single core
per node, so four in total, which is most likely not what you want.</p>
<h3 id="specifying-the-number-of-tasks-per-node">Specifying the number of tasks per node</h3>
<p>Use the --tasks-per-node of sbatch to specify the number of tasks you
require per node. Most multinode job will set this equal to the number
of cores availble per node. The following example asks for 16 task per
node:</p>
<pre><code>#SBATCH --tasks-per-node=16
</code></pre>
<p>This should be used together with the -N option, specifying the number
of nodes to be used. The default value for the number of tasks per node
is 1. For example to specify the requirements for an MPI job with 64
tasks or multiprocessor job using 64 processors to process a larger
number of serial jobs one would specify</p>
<pre><code>#SBATCH -N 4
#SBATCH --tasks-per-node=16
</code></pre>
<p>When using fewer than 16 tasks per node and you want to prevent other
user’s jobs sharing your node, you need to consider using
the --exclusive option. If --exclusive is not specified, SLURM might
place other tasks onto your node.</p>
<h3 id="specifying-the-number-of-threads-for-a-shared-memory-job">Specifying the number of threads for a shared-memory job</h3>
<p>If you want to run shared-memory applications using threads, e.g. OpenMP
parallised code or Java applications, you need to specify the number of
threads you require per task. This can be done with the --tasks-per-node
option of sbatch.</p>
<p>For a standard shared-memory program, which doesn’t also use distributed
memory programming models such as MPI, one is restricted to a single
node. On that node, one can request as many threads as there are cores
on the node. On the standard Alarik compute nodes one can efficiently
use up to 16 threads. Use the following resource statement:</p>
<pre><code>#SBATCH -N 1
#SBATCH --tasks-per-node=16
</code></pre>
<p>If your program is only efficient at a lower thread count, you may want
to use e.g.:</p>
<pre><code>#SBATCH -N 1
#SBATCH --tasks-per-node=4
</code></pre>
<p>if you only want to use four threads. The Alarik extra nodes with 48
cores allow for very wide shared-memory jobs:</p>
<pre><code>#SBATCH -N 1
#SBATCH --tasks-per-node=48
#SBATCH --mem-per-cpu=3000
#SBATCH -p extra
</code></pre>
<h3 id="resource-statements-for-hybrid-programs-using-distributed-and-shared-memory">Resource statements for hybrid programs using distributed and shared memory</h3>
<p>So-called hybrid programs, using both distributed and shared-memory
techniques have recently become popular. For example: for a program
using 32 MPI tasks, each task spawning 2 OpenMP threads one would
require 4 nodes and place eight tasks on each node. The number of
threads per task is given by --cpus-per-task. The resource statement
would look as follows:</p>
<pre><code>#SBATCH -N 4
#SBATCH --tasks-per-node=8
#SBATCH --cpus-per-task=2
</code></pre>
<h3 id="specifying-the-number-of-cores-to-be-required-by-the-job">Specifying the number of cores to be required by the job</h3>
<p>In special cases, such as using very unusal numbers of tasks, the <strong>-n</strong>
option of sbatch to specify the number of cores might become useful.
When running a pure MPI program this option corresponds to the <strong>number
of tasks</strong> required for your program. The following statement in a job
script would reserve 63 cores for your job</p>
<pre><code>#SBATCH -N 4
#SBATCH --tasks-per-node=16
#SBATCH -n 63
</code></pre>
<p>Please consider using the --exclusive option of sbatch to avoid SLURM
spreading your job on more nodes than necessary and placing other user’s
jobs on nodes utilising fewer than 16 cores for your job. Other user’s
jobs could via shared node resources (memory bus, cache, FPU, …)
interfere with your job and introduce undue operational noise. Such
noise is something parallel program execution can be extremely sensitive
to.</p>
<h2 id="program-execution-environment">Program execution environment</h2>
<h3 id="job-execution-environment">Job execution environment</h3>
<p>When submitting your job to SLURM using sbatch, your entire environment
including the currently loaded modules gets copied. This behaviour is
different from earlier Lunarc machines, including Platon. On Alarik,
when hitting sbatch:</p>
<ul>
<li>Make sure that the loaded modules and any environment variable you may have set will not be in conflict with the environment expected by the job script</li>
</ul>
<h3 id="compiler-modules">Compiler modules</h3>
<p>On Alarik we automatically load a modern version of the GCC compiler,
which supports the deployed AMD Opteron processors. At the time of
writing this is version 4.6.2 of GCC. If you prefer using a different
compiler, you can add the desired module, e.g., version 12.1 of the
Intel compiler</p>
<pre><code>module add intel/12.1
</code></pre>
<p>If different modules have files with the same names in the search path,
those of the module added last will be picked. Generally this is not a
problem, but the compiler wrappers in the openmpi modules have the same
names and it safest to only have one loaded at a time.</p>
<p>On Erik the same compilers as on Alarik are present. Note that the
processors on Erik are of the Intel Xeon type and thus utilize the mkl
as supplied.</p>
<h3 id="slurm-variables">SLURM variables</h3>
<p><em>To come</em></p>
<h3 id="snic-variables">SNIC variables</h3>
<p>The SNIC meta-centres have agreed on a set of environment variables
which should improve the portability of (parts of) job-scripts between
SNIC sites. On Alarik the following variables are set by the system:</p>
<hr />
<p><strong>Environment variable</strong>   <strong>Explanation</strong>                                                                               <strong>Value on Alarik</strong>   <strong>Value on Erik</strong></p>
<hr />
<p>SNIC_SITE                 Identifying the SNIC site you are using                                                       lunarc                lunarc</p>
<p>SNIC_RESOURCE             Identifying the compute resource you are using                                                alarik                erik</p>
<p>SNIC_BACKUP               User directory which is:                                                                      /home/<user>        /home/<user></p>
<pre><code>                         &gt; Regularly backed up against accidental deletion

                         &gt; Typically extremely limited space

                         &gt; Use for e.g. precious source code
</code></pre>
<p>SNIC_NOBACKUP             User directory which is:                                                                      /lunarc               /lunarc</p>
<pre><code>                         &gt; Accessible on all Lunarc systems                                                            /nobackup             /nobackup

                         &gt; Outliving individual systems                                                                /users/&lt;user&gt;       /users/&lt;user&gt;

                         &gt; For storing larger amounts of data

                         &gt; Not backed up against accidental deletion

                         &gt; Protected against disk failure (RAID configuration)

                         &gt; On Alarik: the primary root directory for job management (job scripts, input/output data)
</code></pre>
<p>SNIC_TMP                  Directory for best performance during a job                                                   <em>jobid dependent</em>     <em>jobid dependent</em></p>
<pre><code>                         At Lunarc:

                         &gt; Local disk on nodes

                         &gt; Storing temporary data during job execution

                         &gt; High bandwidth

                         &gt; Automatically deleted

                         &gt; Transfer data with long-term value to SNIC_NOBACKUP before job has finished
</code></pre>
<hr />
<h2 id="using-the-node-local-disks-to-improve-io-performance">Using the node local disks to improve I/O performance</h2>
<p>On Alarik and Erik, all nodes have a local disk. This disk offers
superior bandwidth when compared to accessing your home space or the
/lunarc/nobackup centre storage. In particular when files are read or
written repeatedly during execution it is advisable to copy the input
data onto the local disk prior to job execution and copy the result
files back to the submission directory once your program has finished.
During its execution, your program would then read and write to local
disk.</p>
<p>In case of Alarik and Erik, the submission directory typically resides
on the /lunarc/nobackup centre storage. All data left on the node local
disks <strong>will be deleted</strong> when your job has finished. You need to copy
everything of interest to a more permanent storage space such as
/lunarc/nobackup. If a job is terminated prematurely, for example, if it
exceeds the requested walltime, the files on the local disk (in
$SNIC_TMP) will be lost.</p>
<p>Files that would still be useful can be listed in a special file
<strong>$SNIC_TMP/slurm_save_files</strong>. Filenames are assumed to be relative
to $SNIC_TMP and should be separated by spaces or listed on separate
lines. Wildcards are allowed. These files will be copied from the local
disk where $SNIC_TMP/slurm_save_files exists to the submission
directory regardless whether the job ends as planned or is deleted,
unless there is a problem with the disk or node itself. Note that the
slurm_save_files feature is unique to Lunarc.</p>
<p>For the required UNIX scripting you should use the following environment
variables. Example scripts using this technique are provided in the
example section of this document. Contact the help desk if you have
specific requirements and require consultation.</p>
<hr />
<p><strong>Variable</strong>         <strong>Addressed Volume</strong></p>
<hr />
<p>SNIC_TMP            node local disk</p>
<pre><code>                   copy your input data here and start your program from here
</code></pre>
<p>TMPDIR               node local disk</p>
<pre><code>                   Many applications use this environment variable to locate a disk volume for temporary scratch space. If your application follows that convention nothing needs to be done.
</code></pre>
<p>SLURM_SUBMIT_DIR   submission directory</p>
<pre><code>                   where you ran sbatch
</code></pre>
<hr />
<h2 id="launching-mpi-jobs-in-openmpi">Launching MPI jobs in OpenMPI</h2>
<p>To execute message passing parallel jobs these should be built against
one of the MPI libraries provided by the support team as a module. To
execute an MPI job, your job script should do the following</p>
<ul>
<li>Load MPI module relevant for the compiler you are using</li>
<li>Start the program with mpirun</li>
<li>On Alarik the correct binding is crucial to achieve good
    &gt; performance. When using 16 task per node, we recommend using
    &gt; the -bind-to-core option of mpirun. When using fewer than 16 tasks
    &gt; we recommend experimenting whether not using binding helps or
    &gt; hinders performance.</li>
</ul>
<h2 id="submitting-monitoring-and-manipulating-jobs-in-slurm">Submitting, monitoring and manipulating jobs in SLURM</h2>
<h3 id="submitting-with-sbatch">Submitting with sbatch</h3>
<p>One uses the command sbatch to submit a job script to the batch system
for execution. SLURM will reply with the jobid number. The job will then
be held in the queue until the requested resources become available. A
typical use case looks as follows:</p>
<pre><code>[fred@alarik MPItest]$ sbatch runjob.sh
Submitted batch job 7197
</code></pre>
<p>User fred submitted the script runjob.sh to the job queue and got the
jobid 7197 assigned to it.</p>
<h3 id="starting-executables-within-slurm-with-srun">Starting executables within SLURM with srun</h3>
<p>The command srun allows to start executables in a way managed by SLURM.
This is particularly effective if you want to process a large number of
jobs within a single submission to the batch system. A use case of srun
to start many serial jobs in a single multicore submission scipt is
discussed in the <a href="#id.bdphbddpef0"><em>example section</em></a>.</p>
<h3 id="monitoring-with-squeue">Monitoring with squeue</h3>
<p>The command squeue will show you the current state of the job queue. The
standard output, created by calling squeue without any options looks as
follows:</p>
<pre><code>JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)
7303 snic hybrid_n fred PD 0:00 32 (Priority)
7302 snic hybrid_n fred PD 0:00 32 (Priority)
7301 snic hybrid_n fred PD 0:00 32 (Resources)
7304 snic preproce karl PD 0:00 6 (Priority)
7300 snic hybrid_n fred R 0:24 32 an[001-032]
7305 snic preproce karl R 0:37 6 an[081-086]
7306 snic hybrid_n fred R 0:37 6 an[081-086]
7307 snic testsimu sven R 0:07 1 an081
</code></pre>
<p>The first column gives the jobid, the third the job names, followed by
the userid. The column labeled “ST” gives the job state. The most
important states are:</p>
<hr />
<p><strong>Symbol</strong>   <strong>Meaning</strong></p>
<hr />
<blockquote>
<p>R          running</p>
</blockquote>
<p>PD           pending, awaiting resources</p>
<p>CG           completing</p>
<hr />
<p>The state column is followed by the time used by the job and number of
nodes utilised by the job. For running jobs the last column gives the
names of the nodes utilised or if the job is waiting a reason why it is
not executing.</p>
<p>The squeue command is highly configurable. Useful options include -u
myid, which lists all jobs of the user myid and also the --start option.
The latter gives the current estimate of when SLURM expects the job to
start. Note, that this can shift in either direction, depending on e.g.
jobs finishing earlier than specified or jobs with higher priority
getting added to the job queue.</p>
<p>The command jobinfo is a script that sorts the output of squeue into
running and waiting jobs. It also shows additional information, such as
how long running jobs have left and in some cases when waiting jobs are
expected to start.</p>
<h3 id="terminating-jobs-with-scancel">Terminating jobs with scancel</h3>
<p>It is frequently required to remove jobs from the queue. This might be
that you discover a problem in your job specification or intermediate
results of running job indicating that something went wrong. Use scancel
to remove a job from the job queue. To do so you need the jobid, which
is best queried with the squeue command. To remove the job with the
jobid 7103 from the job queue type</p>
<p>scancel 7103</p>
<h1 id="example-job-scripts">Example job scripts</h1>
<p>In this section we provide sample scripts for typical use cases.</p>
<h2 id="job-scripts-using-the-node-local-disk">Job scripts using the node local disk</h2>
<h3 id="basic-run-script">Basic run script</h3>
<p>As discussed the node local disk provides better I/O-bandwidth than the
other file systems available on Alarik. The following script assumes the
program processor reads the file <strong>input.dat</strong> and produces a file
<strong>result.dat</strong>.</p>
<p>This example executes a single serial program and is suitable for the
occasional serial job. If you need to process a large number of serial
jobs, we request you bundle them into a single submission. Refer to the
section “<a href="#id.bdphbddpef0"><em>R</em></a><a href="#id.bdphbddpef0"><em>unning multiple serial jobs within a
single job submission</em></a>” for a scripting example.</p>
<p>The script copies the input data and the program executable from the
submission directory to the node local disk, executes the program on the
node local disk and copies the result file back to the submission
directory for safe keeping. The individual steps are highlighted by
comments starting with a “#”. These comment lines can be kept in the
file.</p>
<p>This is the Lunarc standard example and represents <strong>recommended
practise</strong> for a basic serial job. You need to customise the file to
suit your specific needs. The script is suitable for jobs consuming no
more than 2000 MB of main memory.</p>
<pre><code class="bash">#!/bin/bash
#
# job time, change for what your job requires
#SBATCH -t 00:10:00
#
# job name
#SBATCH -J data_process
#
# filenames stdout and stderr - customise, include %j
#SBATCH -o process_%j.out
#SBATCH -e process_%j.err

# write this script to stdout-file - useful for scripting errors
cat $0

# copy the input data and program to node local disk
# customise for your input file(s) and program name
cp -p input.dat processor $SNIC_TMP

# change to the execution directory
cd $SNIC_TMP

# run the program
# customise for your program name and add arguments if required
./processor

# rescue the results to the submission directory
# customise for your result file(s)
cp -p result.dat $SLURM_SUBMIT_DIR
</code></pre>

<p>We recommend to be selective about the files you copy between the
submission directory and the local node disk. If you have multiple input
and result files you need to modify the copy statements accordingly. The
above example assumes your program has been compiled with the GCC
compiler loaded by default. If it has been compiled with a different
compiler you need to load the compiler module by adding a line similar
to</p>
<pre><code>module add intel/12.1
</code></pre>
<p>If you are running on Erik, it is necessary to add the support for the
GPU with the line</p>
<pre><code>module add cuda
</code></pre>
<p>prior to the line ./processor. You need to consult with the person who
build the executable for you. Lunarc provided modules typically complain
if the wrong compiler is loaded and are hence self-documenting.</p>
<h3 id="version-for-codes-requiring-more-memory-than-2000-mb">Version for codes requiring more memory than 2000 MB</h3>
<p>If your program requires more memory than 2000 MB, use the following
script. This example is set up to use 4000 MB. If you need even more you
can request this, but your runs will be charged to your project at a
higher rate, since other cores have to remain idle. The comments on the
previous example also apply here</p>
<pre><code class="bash">#!/bin/bash
#
# job time, change for what your job requires
#SBATCH -t 00:10:00
#
# job name
#SBATCH -J data_process
#
# filenames stdout and stderr - customise, include %j
#SBATCH -o process_%j.out
#SBATCH -e process_%j.err
#
# requesting a large memory node and 4000 MB or main memory
#SBATCH -C mem64GB
#SBATCH --mem-per-cpu=4000
# write this script to stdout-file - useful for scripting errors
cat $0

# copy the input data and program to node local disk
# customise for your input file(s) and program name
cp -p input.dat processor $SNIC_TMP

# change to the execution directory
cd $SNIC_TMP

# run the program
# customise for your program name and add arguments if required
./processor

# rescue the results to the submission directory
# customise for your result file(s)
cp -p result.dat $SLURM_SUBMIT_DIR
</code></pre>

<p>Since fewer nodes are equipped with 64 GB of memory, you have to allow
for longer queueing times until resource become available.</p>
<h2 id="running-multiple-serial-jobs-within-a-single-job-submission">Running multiple serial jobs within a single job submission</h2>
<p>When you need to run many serial jobs, similar to the ones <a href="#id.81n49hdgiv29"><em>described
above</em></a>, these should be bundled together and
submitted to the job queue in a small number of submissions or even a
single submission. With SLURM is perfectly reasonable to run several
hundred individual jobs in a single submission. To speed up the
processing of your jobs, you can ask for the cores from a number of
nodes. The concept is known as a <strong>task-farm</strong>. The individual job are
known as <strong>job-steps</strong>.</p>
<p>The following is an example processing 200 such jobs using 16 cores from
a single node. The scripting use two scripts, the master script and the
worker script. The Master script requests the resources (number of
cores, job time, ...) and then registers 200 copies of the worker script
with SLURM using the command srun. The worker script is a modification
of the <a href="#id.oyajyndi4e55"><em>basic script</em></a> described above.</p>
<p>In our example this will then start sixteen jobs on the sixteen cores
you requested. Once a job has finished, it will take an unprocessed job
and place it on the idle core for processing. This will continue until
all jobs are processed. The ordering of the jobs can not be relied on.</p>
<p>For our example the entire setup assumes the submission directory has
200 sub-directories, named job_0, job_1, job_2, …, job_199. Each of
the directories contains the input data and the program executable to be
run.</p>
<p>Keep the number of jobs-steps at a reasonable level. Recent testing by
the Lunarc support team has shown that, when including a sleep statement
inside the do loop the setup can be used to processes 800 jobs.</p>
<h3 id="the-master-script">The master script</h3>
<p>The master script describes the resources required and registers, once
running the worker tasks with SLURM. In most cases modifying the number
of cores needed, the total job time and the number of jobs to be
processed should be all that is required.</p>
<pre><code class="bash">#!/bin/sh
# requesting the number of cores needed
#SBATCH -N 1
#SBATCH --tasks-per-node=16
#SBATCH --exclusive
#
# job time, change for what your job farm requires
#SBATCH -t 20:00:00
#
# job name and output file names
#SBATCH -J jobFarm
#SBATCH -o res_jobFarm_%j.out
#SBATCH -e res_jobFarm_%j.out
cat $0

# set the number of jobs - change for your requirements
export NB_of_jobs=200

# Loop over the job number

for ((i=0; i&lt;$NB_of_jobs; i++))
do
    srun -Q --exclusive -n 1 -N 1 
        workScript.sh $i &amp;&gt; worker_${SLURM_JOB_ID}_${i} &amp;
    sleep 1
done

# keep the wait statement, it is important!

wait
</code></pre>

<p>The script assumes that the job is described in a script file
“workScript.sh”, which takes a single number identifying the job
directory to be accessed as a command line argument. Please note the
“sleep 1” command inside the do loop. In our testing this greatly
enhances the stability by submitting the actual jobs over a longer
period of time. With this statement included the script was able to
successfully handle up to about 800 outstanding jobs on 16 and 32 cores.
For reasons of job reliability, we therefore recommend not to process
more than 800 jobs in a single script. However it is possible to process
significantly larger job numbers than 800 by carefully tuning sleep-time
and core count in relation to the average job-time.</p>
<p><strong>Remarks:</strong> When using srun inside a batch script many srun-options act
differently compared to using srun within a different environment.
Consult the man-page of srun for documentation and contact the Lunarc
help desk if your require further consultancy.</p>
<h3 id="the-worker-script">The worker script</h3>
<p>This outlines the worker script. Compared to the script describing a
<a href="#id.oyajyndi4e55"><em>single serial job</em></a>, a few modifications are
required:</p>
<ul>
<li>
<p>To avoid access conflicts between the individual jobs, each job
    &gt; creates a job private sub-directory on the node local disk.</p>
</li>
<li>
<p>The input file(s) are expected in the sub_directories job_0,
    &gt; job_1, job_2, … of the submission directory. The result file(s)
    &gt; will also be placed in these directories.</p>
</li>
<li>
<p>The example assumes a single input file and single result file. If
    &gt; you have multiple input and/or result files modifications are
    &gt; needed, as are modifications for that actual names of your file</p>
</li>
<li>
<p>The present set up allows for different executables for each
    &gt; job-stop. The script assumes to find an executable named
    &gt; “processor” in the same location as the input file(s). If you all
    &gt; job steps use the same executable the scripts can be simplified.</p>
</li>
<li>
<p>Once a job-step has finished and the result file has been copied
    &gt; back, the job private sub-directory on the node local disk is
    &gt; removed to prevent the disc from overflow.</p>
</li>
</ul>
<p>If you are using the above master script, the script should be named
“workScript.sh”.</p>
<pre><code class="bash">#!/bin/sh
# document this script to stdout (assumes redirection from caller)
cat $0

# receive my worker number
export WRK_NB=$1

# create worker-private subdirectory in $SNIC_TMP
export WRK_DIR=$SNIC_TMP/WRK_${WRK_NB}
mkdir $WRK_DIR

# create a variable to address the &quot;job directory&quot;
export JOB_DIR=$SLURM_SUBMIT_DIR/job_${WRK_NB}

# now copy the input data and program from there

cd $JOB_DIR

cp -p input.dat processor $WRK_DIR

# change to the execution directory

cd $WRK_DIR

# run the program

./processor

# rescue the results back to job directory

cp -p result.dat ${JOB_DIR}

# clean up the local disk and remove the worker-private directory

cd $SNIC_TMP

rm -rf WRK_${WRK_NB}
</code></pre>

<h3 id="monitoring-the-progress-of-your-multi-job-submission">Monitoring the progress of your multi-job submission</h3>
<p>Using the -s option of sbatch you can monitor the progression of the
individual job-steps of your multi-job submission. Please keep in mind,
that the step number SLURM assigns to your job and the one you assign
typically differs from the loop index used in the master script.</p>
<p>The below is an output from squeue when running a script processing 500
jobs on 32 cores. The jobid of the job is 8070. The output shows the
job-steps the script is presently processing</p>
<pre><code>[fred@alarik MultiSerialTest]$ squeue -j 8070 -s
STEPID NAME PARTITION USER TIME NODELIST
8070.130 small_ex snic fred 2:09 an074
8070.133 small_ex snic fred 2:02 an073
8070.135 small_ex snic fred 1:55 an074
8070.136 small_ex snic fred 1:41 an073
8070.139 small_ex snic fred 1:41 an073
8070.140 small_ex snic fred 1:41 an073
8070.143 small_ex snic fred 1:41 an073
8070.144 small_ex snic fred 1:41 an074
8070.147 small_ex snic fred 1:41 an074
8070.148 small_ex snic fred 1:41 an074
8070.151 small_ex snic fred 1:41 an074
8070.155 small_ex snic fred 1:38 an074
8070.156 small_ex snic fred 1:35 an074
8070.157 small_ex snic fred 1:34 an073
8070.158 small_ex snic fred 1:34 an073
8070.159 small_ex snic fred 1:34 an073
8070.161 small_ex snic fred 1:34 an073
8070.164 small_ex snic fred 1:33 an074
8070.165 small_ex snic fred 1:33 an074
8070.168 small_ex snic fred 1:32 an073
8070.170 small_ex snic fred 1:26 an073
8070.171 small_ex snic fred 1:12 an073
8070.172 small_ex snic fred 1:12 an073
8070.175 small_ex snic fred 1:11 an074
8070.176 small_ex snic fred 1:11 an074
8070.179 small_ex snic fred 1:11 an074
8070.184 small_ex snic fred 1:04 an074
8070.185 small_ex snic fred 0:42 an073
8070.190 small_ex snic fred 0:35 an073
8070.193 small_ex snic fred 0:35 an074
8070.194 small_ex snic fred 0:13 an073
8070.195 small_ex snic fred 0:13 an074
</code></pre>
<h2 id="mpi-job-using-16-tasks-per-node">MPI job using 16 tasks per node</h2>
<p>Most MPI jobs achieve best cost efficiency when deploying 16 tasks per
node, that is one task per core. Benchmarking by the Lunarc team showed
that these jobs typically require binding to achieve good performance.
The binding offered by the OpenMPI library works satisfactory.</p>
<p>The resource request is very easy in this case. Ask for a number of
cores equivalent to the number of tasks you want to run. We recommend
using the --exclusive option to avoid getting unrelated jobs placed on
the last node in case the number of cores requested doesn’t divide by
the number of cores per node. The following is an example submission
script to run the MPI application simula_mpi with 64 tasks on 4 nodes.
Notice you do not need to specify the node count.</p>
<pre><code class="bash">#!/bin/sh
# requesting the number of cores needed on exclusive nodes
#SBATCH -N 4
#SBATCH --tasks-per-node=16
#SBATCH --exclusive
#
# job time, change for what your job requires
#SBATCH -t 0:30:0
#
# job name
#SBATCH -J simula_n64
#
# filenames stdout and stderr - customise, include %j
#SBATCH -o simula_n64_%j.out
#SBATCH -e simula_n64_%j.out

# write this script to stdout-file - useful for scripting errors
cat $0

# Example assumes we need the intel runtime and OpenMPI library
# customise for the libraries your executable needs
module add intel/13.0
module add openmpi/1.6.2/intel/13.0

# Copying the executable onto the local disks of the nodes
srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p simula_mpi $SNIC_TMP

# Copy the input file onto the headnode - if your MPI program
# reads from all tasks, you need to do the above srun construct
# again
cp -p input.dat $SNIC_TMP

# change to local disk and start the mpi executable
cd $SNIC_TMP
mpirun -bind-to-core simula_mpi

# Copy result files back - example assumes only task zero writes
# if in your application result files are written on all nodes
# you need to initiate a copy on each node via srun

cp -p result.dat $SLURM_SUBMIT_DIR
```bash

This script assumes you are using up to 2000 MB of memory per task. If
you need more, adding the two lines

    #SBATCH -C mem64GB
    #SBATCH --mem-per-cpu=4000

to the script will allow for using up to 4000 MB. Since fewer nodes are
equipped with 64 GB of memory, you have to allow for longer queueing
times until resource become available.

### Modifications required for file I/O on all nodes

As discussed in the comments of the sample script, the script assumes
that only MPI-task 0 on the head node reads the input file and writes to
the output file. If for your MPI application every MPI task reads the
input file(s), replace the line

    cp -p input.dat $SNIC_TMP

with

    srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p input.dat $SNIC_TMP

and the file gets copied onto the local disk of each node. Matters are
slightly more complex, if your output is written from all tasks. We
assume the output files can be wild-carded as result_*.dat. Copying
these files back to the submission directory can be achieved creating a
script, which is placed on all nodes and subsequently executed on all
nodes. The following addition to the submission script will create the
script and place it on all your nodes

```bash
cat &lt;&lt;EOF &gt; copyfile.sh
#!/bin/sh
cp -p result*.dat $SLURM_SUBMIT_DIR
EOF

chmod u+x copyfile.sh
srun -n $SLURM_NNODES -N $SLURM_NNODES cp copyfile.sh $SNIC_TMP
</code></pre>

<p>This needs inserting into the script before the “cd $SNIC_TMP”
statement. Once this is in place you can copy your result files by
replacing the line</p>
<pre><code>cp -p result.dat $SLURM_SUBMIT_DIR
</code></pre>
<p>with the line</p>
<pre><code>srun -n $SLURM_NNODES -N $SLURM_NNODES copyfile.sh
</code></pre>
<h2 id="mpi-jobs-using-fewer-than-16-tasks-per-node">MPI jobs using fewer than 16 tasks per node</h2>
<p>If you want to use fewer than 16 task per nodes to e.g. give more
resources to the individual task, you can use the -N and --task-per-node
options of sbatch. We recommend not to use the -n option in this case.
This example is for 4 nodes with 8 tasks each, a total of 32 tasks. In
our experience, in this case and when using --exclusive it is typically
advantageous to not use binding. Though we encourage experimenting with
your own application.</p>
<pre><code class="bash">#!/bin/sh
# requesting the number of nodes and cores needed, exclusive nodes
#SBATCH -N 4
#SBATCH --tasks-per-node=8
#SBATCH --mem-per-cpu=8000
#SBATCH -C mem64GB
#SBATCH --exclusive
#
# job time, change for what your job requires
#SBATCH -t 0:30:0
#
# job name
#SBATCH -J simula_n64
#
# filenames stdout and stderr - customise, include %j
#SBATCH -o simula_n64_%j.out
#SBATCH -e simula_n64_%j.out

# write this script to stdout-file - useful for scripting errors
cat $0

# Example assumes we need the intel runtime and OpenMPI library
# customise for the libraries your executable needs
module add intel/13.0
module add openmpi/1.6.2/intel/13.0

# Copying the executable onto the local disks of the nodes
srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p simula_mpi $SNIC_TMP

# Copy the input file onto the headnode - if your MPI program
# reads from all tasks, you need to do the above srun construct
# again
cp -p input.dat $SNIC_TMP

# change to local disk and start the mpi executable
cd $SNIC_TMP
mpirun simula_mpi

# Copy result files back - example assumes only task zero writes
# if in your application result files are written on all nodes
# you need to initiate a copy on each node via srun

cp -p result.dat $SLURM_SUBMIT_DIR
</code></pre>

<h2 id="openmp-jobs-using-shared-memory">OpenMP jobs using shared memory</h2>
<p>To run a shared memory code using OpenMP on Alarik, you specify the
number of cores you require using --tasks-per-node option of sbatch. In
this case you have to request placement on a single node with the “-N 1”
option. In this example we call the executable “processor_omp” to
emphasis that this need to be compiled with OpenMP support. Unless you
are doing something special, you are not required to specify the
environment variable OMP_NUM_THREADS. The example script uses the
techniques described for the <a href="#id.oyajyndi4e55"><em>basic run script</em></a> to
engage the node local disk.</p>
<pre><code class="bash">#!/bin/bash
#
# Specify the number of threads - request all on 1 node
#SBATCH -N 1
#SBATCH --tasks-per-node=16
#
# job time, change for what your job requires
#SBATCH -t 00:10:00
#
# job name
#SBATCH -J data_process
#
# filenames stdout and stderr - customise, include %j
#SBATCH -o process_omp_%j.out
#SBATCH -e process_omp_%j.err

# write this script to stdout-file - useful for scripting errors
cat $0

# copy the input data and program to node local disk
# customise for your input file(s) and program name
cp -p input.dat processor_omp $SNIC_TMP

# change to the execution directory
cd $SNIC_TMP

# run the program
# customise for your program name and add arguments if required
./processor_omp

# rescue the results to the submission directory
# customise for your result file(s)
cp -p result.dat $SLURM_SUBMIT_DIR
</code></pre>

<p>This script allows to use 2000 MB of main memory per requested core. If
you need more memory, this can be requested by:</p>
<pre><code>#SBATCH -C mem64GB
#SBATCH --mem-per-cpu=4000
</code></pre>
<p>This will increase you memory request to 4000 MB per requested core.</p>
<h3 id="thread-binding-for-openmp-codes">Thread binding for OpenMP codes</h3>
<p>The Alarik nodes deploy a cache-coherent non-uniform-memory access
architecture (cc-numa). Many scientific simulation codes gain
significant performance benefits on a cc-numa architecture when the user
binds the threads to physical cores of the hardware. This inhibits
thread migration and improves memory locality. Unfortunately invoking
thread binding is not standartised. Depending on the OpenMP runtime
library the user needs to modify different environment variables to bind
his threads.</p>
<h4 id="thread-binding-with-the-gnu-compilers">Thread binding with the GNU compilers</h4>
<p>By default the GNU compiler suite (gcc/gfortran) does not bind threads
to cores. To engage thread binding, you need to set the environment
variable GOMP_CPU_AFFINITY and provide this with a binding list. When
setting</p>
<pre><code>export GOMP_CPU_AFFINITY=”0-15”
</code></pre>
<p>in your submission script, prior to starting your OpenMP application,
this will bind the threads to the 16 cores in the node. The above will
bind thread 0 to core 0, thread 1 to core 1 and so on.</p>
<p><strong>More advanced remark:</strong> If you want to utilise only 8 cores from a
node and asking for exclusive node access (#SBATCH --exclusive), it
might be a good idea to place threads on every second core only. This
will give you more memory bandwidth and make sure you are utilising all
FPUs of the Interlagos architecture. This can be achieved by setting:</p>
<pre><code>export GOMP_CPU_AFFINITY=”0-14:2”
</code></pre>
<p>or</p>
<pre><code>export GOMP_CPU_AFFINITY=”0 2 4 6 8 10 12 14”
</code></pre>
<p>It depend on details of your application, whether or not this helps
performance. Also note, when asking for a exclusive access to a note,
you will be charged for the full node, whether or not you use all cores.</p>
<p><strong>Important pitfall:</strong> If you set GOMP_CPU_AFFINITY=0 this will bind
all threads to core 0. You will see extremely poor performance in this
case.</p>
<h4 id="thread-binding-with-the-open64-compiler">Thread binding with the open64 compiler</h4>
<p>OpenMP code compiled with the <strong>open64</strong> compiler will use thread
binding on Alarik. In standard use cases this will actually boost
performance. However in special situation, e.g. when using fewer threads
than the size of your partition, you might see a performance boost by
not using thread binding. To do so you need to set the environment
variable “O64_OMP_SET_AFFINITY=false”</p>
<h4 id="thread-binding-with-the-intel-compiler">Thread binding with the Intel compiler</h4>
<p>Versions 12.1 and 13.0 of the <strong>Intel</strong> compiler do not support thread
binding when used on the AMD processors deployed on Alarik. Starting
from version 13.1 the Intel compile does support thread binding on the
AMD processors deployed on Alarik. Obviously all versions of the Intel
compiler support thread binding on the Intel processors deployed on
Erik.</p>
<p>For version 13.1 of the Intel compiler thread is controlled by setting
the environment variable KMP_AFFINITY. The value</p>
<pre><code>export KMP_AFFINITY=granularity=fine,compact
</code></pre>
<p>might be a good starting point for your experimentation.</p>
<h2 id="hybrid-jobs-using-threads-within-an-mpi-framework">Hybrid-jobs using threads within an MPI framework</h2>
<p>A cluster with multicore nodes such as Alarik is a natural environment
to execute parallel codes deploying both MPI and OpenMP threads. When
running such applications the optimal number of MPI-tasks and OpenMP
threads to place on a node can depend highly on the details of the
application. In particular for application which make many references to
main memory and the programmer has not implemented a proper “first touch
data allocation” it is typically <strong>best to have 2 or 4 threads</strong> per MPI
task on an Alarik node. Together with a proper binding of your MPI tasks
to the “numa-islands”, this will ensure memory locality for your code.
For the below syntax you have to use <strong>version 1.8.3 or newer</strong> of the
OpenMPI library.</p>
<p>In the following we give a simple example script to run a MPI-OpenMP
hybrid named simul_hyb on 2 nodes using 8 tasks and 4 threads per task.
The tasks and their threads will be bound to the <em>numa-islands</em>,
minimising cc-numa effects.</p>
<pre><code class="bash">#!/bin/sh
# requesting number of nodes (-N option)
# number of mpi-tasks per node
# and number of threads per task exclusive nodes
#SBATCH -N 2
#SBATCH --tasks-per-node=4
#SBATCH --cpus-per-task=4
#SBATCH --exclusive
# time required
#SBATCH -t 01:00:00
#SBATCH -J hybrid_run
# filenames stdout and stderr - customise, include %j
#SBATCH -o simula_N2t4c4_%j.out
#SBATCH -e simula_N2t4c4_%j.out

cat $0

# Example assumes we need the intel runtime and OpenMPI library
# customise for the libraries your executable needs
module add intel/15.0
module add openmpi/1.8.3/intel/15.0

# Copying the executable onto the local disks of the nodes
srun -n $SLURM_NNODES -N $SLURM_NNODES cp -p simul_hyb $SNIC_TMP

# Copy the input file onto the headnode - if your MPI program
# reads from all tasks, you need to do the above srun construct
# again
cp -p input.dat $SNIC_TMP
cd $SNIC_TMP

# setting number of OpenMP threads and ask for thread binding
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export OMP_PROC_BIND=true

mpiexec --map-by ppr:$SLURM_NTASKS_PER_NODE:node:PE=$SLURM_CPUS_PER_TASK simul_hyb

# Copy result files back - example assumes only task zero writes
# if in your application result files are written on all nodes
# you need to initiate a copy on each node via srun
cp -p result.dat $SLURM_SUBMIT_DIR
</code></pre>

<p>The example assumes that MPI task 0 is the only task reading and writing
input files. If your application reads and writes data on all nodes, you
need to study the <a href="#id.hpgejkt8dzry"><em>modifications</em></a> described in the
MPI section.</p>
<p>As discussed, the above binds the tasks and their threads to the
numa-islands of the Alarik architecture. Alariks numa-islands have four
cores, therefore the script is best used with 2 or four threads per MPI
task. This results in one or two MPI tasks per numa islands.</p>
<h3 id="things-to-try-for-mpi-openmp-hybrids-with-16-threads-per-task">Things to try for MPI-OpenMP hybrids with 16 threads per task</h3>
<p>While using more than 4 threads per MPI task on the Alarik system can
result in reduced performance due to cc-numa effects, there are
situations when using 16 threads per task can be required (e.g. special
algorithms or extreme memory requirements per MPI task).</p>
<p>When running 16 threads per MPI task, that is a single MPI task per
Alarik node, you might want to experiment with starting your job without
specifying binding on mpiexec, that is remove the -bind-to-core, but
utilise the <a href="#id.zfbd8w955ujk"><em>OpenMP thread binding</em></a> techniques
described in the OpenMP sample section.</p>
<h1 id="interactive-access-to-compute-nodes">Interactive access to compute nodes</h1>
<p>Sometimes it is desirable to have an interactive login to the compute
nodes of the cluster. Extensive code testing is a typical use case.</p>
<h2 id="starting-an-interactive-session">Starting an interactive session</h2>
<p>To start an interactive session you need to use the “interactive”
command. This will request the required resources from the resource pool
for you and start the interactive session once the resources are
available.</p>
<p>Use the following command to start an interactive session asking for 32
cores lasting 60 minutes</p>
<pre><code>interactive -n 32 -t 60
</code></pre>
<p>On Alarik and Eric this will be allocated on multiple nodes, since the
nodes have only 16 cores available. The interactive session will last
until either the requested time, 60 minutes in the above example, has
expired or you manually exit the interactive shell. Your account gets
charged with the wall time duration of your interactive session,
independent of the amount of computation you do. In the above example,
if your session lasts until it expires after 60 min, you get charged for
32 cpu hours. If you terminate your session after 1/2 hour, you would
get charged 16 cpu hours.</p>
<p>The interactive command supports most command line options of the sbatch
command. Please refer to the man pages of sbatch.</p>
<h2 id="modules-and-environment-variables">Modules and environment variables</h2>
<p>Loaded modules and environment are not always exported properly to your
interactive session. Once placed in the interactive session, we
recommend users to reload <strong>all</strong> the modules they require. That is
despite the “modules list” command claiming they are still loaded.</p>
<p>You also need to check whether environment variables still have the
required values. If the software you are using has a set-up script, you
might need to re-run that script.</p>
<h2 id="known-issues-with-the-interactive-command">Known issues with the interactive command</h2>
<p>None at the time of writing.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../aurora_modules/" class="btn btn-neutral float-right" title="Aurora Software"/>Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../quick_reference/" class="btn btn-neutral" title="Quick reference"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../quick_reference/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../aurora_modules/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
